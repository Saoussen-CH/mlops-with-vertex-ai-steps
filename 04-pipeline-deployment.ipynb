{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109707f2",
   "metadata": {},
   "source": [
    "# 04 - Test and Deploy Training Pipeline to Vertex Pipelines\n",
    "\n",
    "The purpose of this notebook is to test, deploy, and run the `TFX` pipeline on `Vertex Pipelines`. The notebook covers the following tasks:\n",
    "1. Run the tests locally.\n",
    "2. Run the pipeline using `Vertex Pipelines`\n",
    "3. Execute the pipeline deployment `CI/CD` steps using `Cloud Build`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a51af1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133e7d80",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3bb184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 1.14.0\n",
      "KFP Version: 1.8.22\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kfp\n",
    "import tfx\n",
    "\n",
    "print(\"Tensorflow Version:\", tfx.__version__)\n",
    "print(\"KFP Version:\", kfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7bdded",
   "metadata": {},
   "source": [
    "### Setup Google Cloud project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b4b22be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://stellar-orb-408015/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'stellar-orb-408015' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n",
      "\n",
      "Project ID: stellar-orb-408015\n",
      "Region: us-central1\n",
      "Bucket name: stellar-orb-408015\n",
      "Service Account: 13110252891-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "PROJECT = 'stellar-orb-408015' # Change to your project id.\n",
    "REGION = 'us-central1' # Change to your region.\n",
    "BUCKET =  'stellar-orb-408015-bucket' # Change to your bucket name.\n",
    "SERVICE_ACCOUNT = \"pipelines-sa@stellar-orb-408015.iam.gserviceaccount.com\"\n",
    "\n",
    "if PROJECT == \"\" or PROJECT is None or PROJECT == \"stellar-orb-408015\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT = shell_output[0]\n",
    "    \n",
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"pipelines-sa@stellar-orb-408015.iam.gserviceaccount.com\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.account)' 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[0]\n",
    "    \n",
    "if BUCKET == \"\" or BUCKET is None or BUCKET == \"stellar-orb-408015-bucket\":\n",
    "    # Get your bucket name to GCP project id\n",
    "    BUCKET = PROJECT\n",
    "    # Try to create the bucket if it doesn't exists\n",
    "    ! gsutil mb -l $REGION gs://$BUCKET\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"Project ID:\", PROJECT)\n",
    "print(\"Region:\", REGION)\n",
    "print(\"Bucket name:\", BUCKET)\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660a9dc9",
   "metadata": {},
   "source": [
    "### Set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a75e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_LOCATION = 'US'\n",
    "BQ_DATASET_NAME = 'playground_us' # Change to your BQ dataset name.\n",
    "BQ_TABLE_NAME = 'chicago_taxitrips_final'\n",
    "\n",
    "VERSION = 'v01'\n",
    "DATASET_DISPLAY_NAME = 'chicago-taxi-tips'\n",
    "MODEL_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}-classifier-{VERSION}'\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-train-pipeline'\n",
    "\n",
    "CICD_IMAGE_NAME = 'cicd:latest'\n",
    "CICD_IMAGE_URI = f\"gcr.io/{PROJECT}/{CICD_IMAGE_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06be3555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'src/raw_schema/.ipynb_checkpoints/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r src/raw_schema/.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44678373",
   "metadata": {},
   "source": [
    "## 1. Run the CICD steps locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68204ed",
   "metadata": {},
   "source": [
    "### Set pipeline configurations for the local run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a1d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DATASET_DISPLAY_NAME\"] = DATASET_DISPLAY_NAME\n",
    "os.environ[\"MODEL_DISPLAY_NAME\"] =  MODEL_DISPLAY_NAME\n",
    "os.environ[\"PIPELINE_NAME\"] = PIPELINE_NAME\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"BQ_LOCATION\"] = BQ_LOCATION\n",
    "os.environ[\"BQ_DATASET_NAME\"] = BQ_DATASET_NAME\n",
    "os.environ[\"BQ_TABLE_NAME\"] = BQ_TABLE_NAME\n",
    "os.environ[\"GCS_LOCATION\"] = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/e2e_tests\"\n",
    "os.environ[\"TRAIN_LIMIT\"] = \"1000\"\n",
    "os.environ[\"TEST_LIMIT\"] = \"100\"\n",
    "os.environ[\"UPLOAD_MODEL\"] = \"0\"\n",
    "os.environ[\"ACCURACY_THRESHOLD\"] = \"0.1\"\n",
    "os.environ[\"BEAM_RUNNER\"] = \"DirectRunner\"\n",
    "os.environ[\"TRAINING_RUNNER\"] = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d353e3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 10:04:12.990459: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6Status12empty_stringB5cxx11Ev']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZNK10tensorflow4data11DatasetBase8FinalizeEPNS_15OpKernelContextESt8functionIFN3tsl8StatusOrISt10unique_ptrIS1_NS5_4core15RefCountDeleterEEEEvEE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n",
      "PROJECT: stellar-orb-408015\n",
      "REGION: us-central1\n",
      "GCS_LOCATION: gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests\n",
      "ARTIFACT_STORE_URI: gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts\n",
      "MODEL_REGISTRY_URI: gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/model_registry\n",
      "DATASET_DISPLAY_NAME: chicago-taxi-tips\n",
      "MODEL_DISPLAY_NAME: chicago-taxi-tips-classifier-v01\n",
      "PIPELINE_NAME: chicago-taxi-tips-classifier-v01-train-pipeline\n",
      "ML_USE_COLUMN: ml_use\n",
      "EXCLUDE_COLUMNS: trip_start_timestamp\n",
      "TRAIN_LIMIT: 1000\n",
      "TEST_LIMIT: 100\n",
      "SERVE_LIMIT: 0\n",
      "NUM_TRAIN_SPLITS: 4\n",
      "NUM_EVAL_SPLITS: 1\n",
      "ACCURACY_THRESHOLD: 0.1\n",
      "USE_KFP_SA: False\n",
      "TFX_IMAGE_URI: gcr.io/stellar-orb-408015/tfx-chicago-taxi-tips:latest\n",
      "BEAM_RUNNER: DirectRunner\n",
      "BEAM_DIRECT_PIPELINE_ARGS: ['--project=stellar-orb-408015', '--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp']\n",
      "BEAM_DATAFLOW_PIPELINE_ARGS: ['--project=stellar-orb-408015', '--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp', '--region=us-central1', '--runner=DirectRunner']\n",
      "TRAINING_RUNNER: local\n",
      "VERTEX_TRAINING_ARGS: {'project': 'stellar-orb-408015', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'gcr.io/stellar-orb-408015/tfx-chicago-taxi-tips:latest'}}]}\n",
      "VERTEX_TRAINING_CONFIG: {'ai_platform_training_enable_ucaip': True, 'ai_platform_training_ucaip_region': 'us-central1', 'ai_platform_training_args': {'project': 'stellar-orb-408015', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'gcr.io/stellar-orb-408015/tfx-chicago-taxi-tips:latest'}}]}, 'use_gpu': False}\n",
      "SERVING_RUNTIME: tf2-cpu.2-12\n",
      "SERVING_IMAGE_URI: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest\n",
      "BATCH_PREDICTION_BQ_DATASET_NAME: playground_us\n",
      "BATCH_PREDICTION_BQ_TABLE_NAME: chicago_taxitrips_prep\n",
      "BATCH_PREDICTION_BEAM_ARGS: {'runner': 'DirectRunner', 'temporary_dir': 'gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp', 'gcs_location': 'gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp', 'project': 'stellar-orb-408015', 'region': 'us-central1', 'setup_file': './setup.py'}\n",
      "BATCH_PREDICTION_JOB_RESOURCES: {'machine_type': 'n1-standard-2', 'starting_replica_count': 1, 'max_replica_count': 10}\n",
      "DATASTORE_PREDICTION_KIND: chicago-taxi-tips-classifier-v01-predictions\n",
      "ENABLE_CACHE: 0\n",
      "UPLOAD_MODEL: 0\n"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d51c12",
   "metadata": {},
   "source": [
    "### Run unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9be84a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.13, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/jupyter/mlops-with-vertex-ai_0\n",
      "plugins: anyio-4.2.0, typeguard-2.13.3\n",
      "collected 2 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "src/tests/datasource_utils_tests.py BigQuery Source: stellar-orb-408015.playground_us.chicago_taxitrips_final\n",
      "\u001b[32m.\u001b[0mBigQuery Source: stellar-orb-408015.playground_us.chicago_taxitrips_final\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../../opt/conda/lib/python3.10/site-packages/google/cloud/bigquery/_http.py:18\n",
      "  /opt/conda/lib/python3.10/site-packages/google/cloud/bigquery/_http.py:18: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "    import pkg_resources\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: 13 warnings\n",
      "  /opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: 11 warnings\n",
      "  /opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2350\n",
      "  /opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(parent)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m======================== \u001b[32m2 passed\u001b[0m, \u001b[33m\u001b[1m26 warnings\u001b[0m\u001b[33m in 5.19s\u001b[0m\u001b[33m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test src/tests/datasource_utils_tests.py -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4358f955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.13, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/jupyter/mlops-with-vertex-ai_0\n",
      "plugins: anyio-4.2.0, typeguard-2.13.3\n",
      "\u001b[1mcollecting ... \u001b[0m2024-01-26 10:05:56.364087: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "collected 2 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "src/tests/model_tests.py \u001b[32m.\u001b[0mModel: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " trip_month_xf (InputLayer)  [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " trip_day_xf (InputLayer)    [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " trip_day_of_week_xf (Input  [(None,)]                    0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " trip_hour_xf (InputLayer)   [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " trip_seconds_xf (InputLaye  [(None,)]                    0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " trip_miles_xf (InputLayer)  [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " payment_type_xf (InputLaye  [(None,)]                    0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " pickup_grid_xf (InputLayer  [(None,)]                    0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropoff_grid_xf (InputLaye  [(None,)]                    0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " euclidean_xf (InputLayer)   [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " loc_cross_xf (InputLayer)   [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " trip_month_xf_embedding (E  (None, 2)                    202       ['trip_month_xf[0][0]']       \n",
      " mbedding)                                                                                        \n",
      "                                                                                                  \n",
      " trip_day_xf_embedding (Emb  (None, 4)                    404       ['trip_day_xf[0][0]']         \n",
      " edding)                                                                                          \n",
      "                                                                                                  \n",
      " trip_day_of_week_xf_onehot  (None, 101)                  0         ['trip_day_of_week_xf[0][0]'] \n",
      "  (CategoryEncoding)                                                                              \n",
      "                                                                                                  \n",
      " trip_hour_xf_embedding (Em  (None, 3)                    303       ['trip_hour_xf[0][0]']        \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda  (None, 1)                    0         ['trip_seconds_xf[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_1 (TFOpLamb  (None, 1)                    0         ['trip_miles_xf[0][0]']       \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " payment_type_xf_onehot (Ca  (None, 101)                  0         ['payment_type_xf[0][0]']     \n",
      " tegoryEncoding)                                                                                  \n",
      "                                                                                                  \n",
      " pickup_grid_xf_embedding (  (None, 3)                    303       ['pickup_grid_xf[0][0]']      \n",
      " Embedding)                                                                                       \n",
      "                                                                                                  \n",
      " dropoff_grid_xf_embedding   (None, 3)                    303       ['dropoff_grid_xf[0][0]']     \n",
      " (Embedding)                                                                                      \n",
      "                                                                                                  \n",
      " tf.expand_dims_2 (TFOpLamb  (None, 1)                    0         ['euclidean_xf[0][0]']        \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " loc_cross_xf_embedding (Em  (None, 10)                   1010      ['loc_cross_xf[0][0]']        \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " combines_inputs (Concatena  (None, 230)                  0         ['trip_month_xf_embedding[0][0\n",
      " te)                                                                ]',                           \n",
      "                                                                     'trip_day_xf_embedding[0][0]'\n",
      "                                                                    , 'trip_day_of_week_xf_onehot[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'trip_hour_xf_embedding[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'tf.expand_dims[0][0]',      \n",
      "                                                                     'tf.expand_dims_1[0][0]',    \n",
      "                                                                     'payment_type_xf_onehot[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'pickup_grid_xf_embedding[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'dropoff_grid_xf_embedding[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'tf.expand_dims_2[0][0]',    \n",
      "                                                                     'loc_cross_xf_embedding[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " feedforward_network (Seque  (None, 32)                   16864     ['combines_inputs[0][0]']     \n",
      " ntial)                                                                                           \n",
      "                                                                                                  \n",
      " logits (Dense)              (None, 1)                    33        ['feedforward_network[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19422 (75.87 KB)\n",
      "Trainable params: 19422 (75.87 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 3.36s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test src/tests/model_tests.py -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa00fd5",
   "metadata": {},
   "source": [
    "### Run e2e pipeline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb9aad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.13, pytest-7.4.4, pluggy-1.3.0\n",
      "rootdir: /home/jupyter/mlops-with-vertex-ai_0\n",
      "plugins: anyio-4.2.0, typeguard-2.13.3\n",
      "\u001b[1mcollecting ... \u001b[0m2024-01-26 14:13:39.278283: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "collected 0 items / 1 error                                                    \u001b[0m\u001b[1m\n",
      "\n",
      "==================================== ERRORS ====================================\n",
      "\u001b[31m\u001b[1m___________ ERROR collecting src/tests/pipeline_deployment_tests.py ____________\u001b[0m\n",
      "\u001b[1m\u001b[31msrc/tests/pipeline_deployment_tests.py\u001b[0m:23: in <module>\n",
      "    \u001b[94mfrom\u001b[39;49;00m \u001b[04m\u001b[96msrc\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mtfx_pipelines\u001b[39;49;00m \u001b[94mimport\u001b[39;49;00m config\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31msrc/tfx_pipelines/config.py\u001b[0m:17: in <module>\n",
      "    \u001b[94mfrom\u001b[39;49;00m \u001b[04m\u001b[96mtfx\u001b[39;49;00m \u001b[94mimport\u001b[39;49;00m v1 \u001b[94mas\u001b[39;49;00m tfx\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/opt/conda/lib/python3.10/site-packages/tfx/v1/__init__.py\u001b[0m:20: in <module>\n",
      "    \u001b[94mfrom\u001b[39;49;00m \u001b[04m\u001b[96mtfx\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mv1\u001b[39;49;00m \u001b[94mimport\u001b[39;49;00m components\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/opt/conda/lib/python3.10/site-packages/tfx/v1/components/__init__.py\u001b[0m:19: in <module>\n",
      "    \u001b[94mfrom\u001b[39;49;00m \u001b[04m\u001b[96mtfx\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mcomponents\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mexample_diff\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mcomponent\u001b[39;49;00m \u001b[94mimport\u001b[39;49;00m ExampleDiff\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/opt/conda/lib/python3.10/site-packages/tfx/components/__init__.py\u001b[0m:25: in <module>\n",
      "    \u001b[94mfrom\u001b[39;49;00m \u001b[04m\u001b[96mtfx\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mcomponents\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mmodel_validator\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mcomponent\u001b[39;49;00m \u001b[94mimport\u001b[39;49;00m ModelValidator\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/opt/conda/lib/python3.10/site-packages/tfx/components/model_validator/component.py\u001b[0m:20: in <module>\n",
      "    \u001b[94mfrom\u001b[39;49;00m \u001b[04m\u001b[96mtfx\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mcomponents\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mmodel_validator\u001b[39;49;00m \u001b[94mimport\u001b[39;49;00m executor\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/opt/conda/lib/python3.10/site-packages/tfx/components/model_validator/executor.py\u001b[0m:32: in <module>\n",
      "    \u001b[94mclass\u001b[39;49;00m \u001b[04m\u001b[92mExecutor\u001b[39;49;00m(base_beam_executor.BaseBeamExecutor):\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/opt/conda/lib/python3.10/site-packages/tfx/components/model_validator/executor.py\u001b[0m:54: in Executor\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_pass_threshold\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, eval_result: tfma.EvalResult) -> \u001b[96mbool\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE   AttributeError: module 'tensorflow_model_analysis' has no attribute 'EvalResult'\u001b[0m\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../../opt/conda/lib/python3.10/site-packages/apache_beam/runners/portability/stager.py:63\n",
      "  /opt/conda/lib/python3.10/site-packages/apache_beam/runners/portability/stager.py:63: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "    import pkg_resources\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: 13 warnings\n",
      "  /opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: 11 warnings\n",
      "  /opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2350\n",
      "  /opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(parent)\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/hdfs/config.py:28\n",
      "  /opt/conda/lib/python3.10/site-packages/hdfs/config.py:28: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "    from imp import load_source\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98\n",
      "  /opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "  caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6Status12empty_stringB5cxx11Ev']\n",
      "    warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104\n",
      "  /opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "  caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZNK10tensorflow4data11DatasetBase8FinalizeEPNS_15OpKernelContextESt8functionIFN3tsl8StatusOrISt10unique_ptrIS1_NS5_4core15RefCountDeleterEEEEvEE']\n",
      "    warnings.warn(f\"file system plugins are not loaded: {e}\")\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/tfx/utils/deprecation_utils.py:188\n",
      "  /opt/conda/lib/python3.10/site-packages/tfx/utils/deprecation_utils.py:188: TfxDeprecationWarning: DEFAULT_FILE_NAME will be deprecated soon\n",
      "    warnings.warn(msg, TfxDeprecationWarning)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m src/tests/pipeline_deployment_tests.py - AttributeError: module 'tensorflow_model_analysis' has no attribute 'EvalRe...\n",
      "\u001b[31m======================== \u001b[33m30 warnings\u001b[0m, \u001b[31m\u001b[1m1 error\u001b[0m\u001b[31m in 5.87s\u001b[0m\u001b[31m =========================\u001b[0m\n",
      "Error importing: /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /opt/conda/lib/python3.10/site-packages/zmq/backend/cython/../../../../../libzmq.so.5)\u001b[31mERROR: not found: /home/jupyter/mlops-with-vertex-ai_0/src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "(no name '/home/jupyter/mlops-with-vertex-ai_0/src/tests/pipeline_deployment_tests.py::test_e2e_pipeline' in any of [<Module pipeline_deployment_tests.py>])\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test src/tests/pipeline_deployment_tests.py::test_e2e_pipeline -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5704bcb",
   "metadata": {},
   "source": [
    "## 2. Run the training pipeline using Vertex Pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7db74",
   "metadata": {},
   "source": [
    "### Set the pipeline configurations for the Vertex AI run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2fe69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATASET_DISPLAY_NAME\"] = DATASET_DISPLAY_NAME\n",
    "os.environ[\"MODEL_DISPLAY_NAME\"] = MODEL_DISPLAY_NAME\n",
    "os.environ[\"PIPELINE_NAME\"] = PIPELINE_NAME\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"GCS_LOCATION\"] = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}\"\n",
    "os.environ[\"TRAIN_LIMIT\"] = \"85000\"\n",
    "os.environ[\"TEST_LIMIT\"] = \"15000\"\n",
    "os.environ[\"BEAM_RUNNER\"] = \"DataflowRunner\"\n",
    "os.environ[\"TRAINING_RUNNER\"] = \"vertex\"\n",
    "os.environ[\"TFX_IMAGE_URI\"] = f\"gcr.io/{PROJECT}/{DATASET_DISPLAY_NAME}:{VERSION}\"\n",
    "os.environ[\"ENABLE_CACHE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ef31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tfx_pipelines import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f3164f",
   "metadata": {},
   "source": [
    "### Build the ML container image\n",
    "\n",
    "This is the `TFX` runtime environment for the training pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $TFX_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag $TFX_IMAGE_URI . --timeout=15m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155568ca",
   "metadata": {},
   "source": [
    "### Compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d5ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tfx_pipelines import runner\n",
    "\n",
    "pipeline_definition_file = f'{config.PIPELINE_NAME}.json'\n",
    "pipeline_definition = runner.compile_training_pipeline(pipeline_definition_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6644c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINES_STORE = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/compiled_pipelines/\"\n",
    "!gsutil cp {pipeline_definition_file} {PIPELINES_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb943e",
   "metadata": {},
   "source": [
    "### Submit run to Vertex Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c6b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "pipeline_client = AIPlatformClient(\n",
    "    project_id=PROJECT, region=REGION)\n",
    "                 \n",
    "job = pipeline_client.create_run_from_job_spec(\n",
    "    job_spec_path=pipeline_definition_file,\n",
    "    parameter_values={\n",
    "        'learning_rate': 0.003,\n",
    "        'batch_size': 512,\n",
    "        'hidden_units': '128,128',\n",
    "        'num_epochs': 30,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9536c81-6a64-4a19-976e-ccc155e92e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_12538/2836409088.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m SCRIPT_DIR = os.path.dirname(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSCRIPT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "SCRIPT_DIR = os.path.dirname(\n",
    "    os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser(__file__)))\n",
    ")\n",
    "sys.path.append(os.path.normpath(os.path.join(SCRIPT_DIR, \"..\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888be1fd",
   "metadata": {},
   "source": [
    "### Extracting pipeline runs metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae4aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "pipeline_df = vertex_ai.get_pipeline_df(PIPELINE_NAME)\n",
    "pipeline_df = pipeline_df[pipeline_df.pipeline_name == PIPELINE_NAME]\n",
    "pipeline_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b454fe9",
   "metadata": {},
   "source": [
    "## 3. Execute the pipeline deployment CI/CD steps in Cloud Build\n",
    "\n",
    "The CI/CD routine is defined in the [pipeline-deployment.yaml](build/pipeline-deployment.yaml) file, and consists of the following steps:\n",
    "1. Clone the repository to the build environment.\n",
    "2. Run unit tests.\n",
    "3. Run a local e2e test of the pipeline.\n",
    "4. Build the ML container image for pipeline steps.\n",
    "5. Compile the pipeline.\n",
    "6. Upload the pipeline to Cloud Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29688d4d",
   "metadata": {},
   "source": [
    "### Build CI/CD container Image for Cloud Build\n",
    "\n",
    "This is the runtime environment where the steps of testing and deploying the pipeline will be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4759b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $CICD_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc09c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag $CICD_IMAGE_URI build/. --timeout=15m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc9b2af",
   "metadata": {},
   "source": [
    "### Run CI/CD from pipeline deployment using Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b55593",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_URL = \"https://github.com/GoogleCloudPlatform/mlops-with-vertex-ai.git\" # Change to your github repo.\n",
    "BRANCH = \"main\"\n",
    "\n",
    "GCS_LOCATION = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/\"\n",
    "TEST_GCS_LOCATION = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/e2e_tests\"\n",
    "CI_TRAIN_LIMIT = 1000\n",
    "CI_TEST_LIMIT = 100\n",
    "CI_UPLOAD_MODEL = 0\n",
    "CI_ACCURACY_THRESHOLD = 0.1\n",
    "BEAM_RUNNER = \"DataflowRunner\"\n",
    "TRAINING_RUNNER = \"vertex\"\n",
    "VERSION = 'tfx-1.2'\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-train-pipeline'\n",
    "PIPELINES_STORE = os.path.join(GCS_LOCATION, \"compiled_pipelines\")\n",
    "\n",
    "TFX_IMAGE_URI = f\"gcr.io/{PROJECT}/{DATASET_DISPLAY_NAME}:{VERSION}\"\n",
    "\n",
    "SUBSTITUTIONS=f\"\"\"\\\n",
    "_REPO_URL='{REPO_URL}',\\\n",
    "_BRANCH={BRANCH},\\\n",
    "_CICD_IMAGE_URI={CICD_IMAGE_URI},\\\n",
    "_PROJECT={PROJECT},\\\n",
    "_REGION={REGION},\\\n",
    "_GCS_LOCATION={GCS_LOCATION},\\\n",
    "_TEST_GCS_LOCATION={TEST_GCS_LOCATION},\\\n",
    "_BQ_LOCATION={BQ_LOCATION},\\\n",
    "_BQ_DATASET_NAME={BQ_DATASET_NAME},\\\n",
    "_BQ_TABLE_NAME={BQ_TABLE_NAME},\\\n",
    "_DATASET_DISPLAY_NAME={DATASET_DISPLAY_NAME},\\\n",
    "_MODEL_DISPLAY_NAME={MODEL_DISPLAY_NAME},\\\n",
    "_CI_TRAIN_LIMIT={CI_TRAIN_LIMIT},\\\n",
    "_CI_TEST_LIMIT={CI_TEST_LIMIT},\\\n",
    "_CI_UPLOAD_MODEL={CI_UPLOAD_MODEL},\\\n",
    "_CI_ACCURACY_THRESHOLD={CI_ACCURACY_THRESHOLD},\\\n",
    "_BEAM_RUNNER={BEAM_RUNNER},\\\n",
    "_TRAINING_RUNNER={TRAINING_RUNNER},\\\n",
    "_TFX_IMAGE_URI={TFX_IMAGE_URI},\\\n",
    "_PIPELINE_NAME={PIPELINE_NAME},\\\n",
    "_PIPELINES_STORE={PIPELINES_STORE}\\\n",
    "\"\"\"\n",
    "\n",
    "!echo $SUBSTITUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc589ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --no-source --timeout=60m --config build/pipeline-deployment.yaml --substitutions {SUBSTITUTIONS} --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16028a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m79"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
