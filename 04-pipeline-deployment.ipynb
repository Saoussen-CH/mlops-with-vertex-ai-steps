{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109707f2",
   "metadata": {},
   "source": [
    "# 04 - Test and Deploy Training Pipeline to Vertex Pipelines\n",
    "\n",
    "The purpose of this notebook is to test, deploy, and run the `TFX` pipeline on `Vertex Pipelines`. The notebook covers the following tasks:\n",
    "1. Run the tests locally.\n",
    "2. Run the pipeline using `Vertex Pipelines`\n",
    "3. Execute the pipeline deployment `CI/CD` steps using `Cloud Build`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a51af1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133e7d80",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3bb184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 13:44:43.723399: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n",
      "TFX Version: 1.14.0\n",
      "KFP Version: 1.8.22\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kfp\n",
    "from tfx import v1 as tfx\n",
    "\n",
    "print(\"TFX Version:\", tfx.__version__)\n",
    "print(\"KFP Version:\", kfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7bdded",
   "metadata": {},
   "source": [
    "### Setup Google Cloud project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b4b22be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://stellar-orb-408015/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'stellar-orb-408015' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n",
      "\n",
      "Project ID: stellar-orb-408015\n",
      "Region: us-central1\n",
      "Bucket name: stellar-orb-408015\n",
      "Service Account: 13110252891-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "PROJECT = 'stellar-orb-408015' # Change to your project id.\n",
    "REGION = 'us-central1' # Change to your region.\n",
    "BUCKET =  'stellar-orb-408015-bucket' # Change to your bucket name.\n",
    "SERVICE_ACCOUNT = \"pipelines-sa@stellar-orb-408015.iam.gserviceaccount.com\"\n",
    "\n",
    "if PROJECT == \"\" or PROJECT is None or PROJECT == \"stellar-orb-408015\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT = shell_output[0]\n",
    "    \n",
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"pipelines-sa@stellar-orb-408015.iam.gserviceaccount.com\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.account)' 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[0]\n",
    "    \n",
    "if BUCKET == \"\" or BUCKET is None or BUCKET == \"stellar-orb-408015-bucket\":\n",
    "    # Get your bucket name to GCP project id\n",
    "    BUCKET = PROJECT\n",
    "    # Try to create the bucket if it doesn't exists\n",
    "    ! gsutil mb -l $REGION gs://$BUCKET\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"Project ID:\", PROJECT)\n",
    "print(\"Region:\", REGION)\n",
    "print(\"Bucket name:\", BUCKET)\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660a9dc9",
   "metadata": {},
   "source": [
    "### Set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a75e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_LOCATION = 'US'\n",
    "BQ_DATASET_NAME = 'playground_us' # Change to your BQ dataset name.\n",
    "BQ_TABLE_NAME = 'chicago_taxitrips_final'\n",
    "\n",
    "VERSION = 'v01'\n",
    "DATASET_DISPLAY_NAME = 'chicago-taxi-tips'\n",
    "MODEL_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}-classifier-{VERSION}'\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-train-pipeline'\n",
    "\n",
    "CICD_IMAGE_NAME = 'cicd:latest'\n",
    "CICD_IMAGE_URI = f\"gcr.io/{PROJECT}/{CICD_IMAGE_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r src/raw_schema/.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44678373",
   "metadata": {},
   "source": [
    "## 1. Run the CICD steps locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68204ed",
   "metadata": {},
   "source": [
    "### Set pipeline configurations for the local run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a1d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DATASET_DISPLAY_NAME\"] = DATASET_DISPLAY_NAME\n",
    "os.environ[\"MODEL_DISPLAY_NAME\"] =  MODEL_DISPLAY_NAME\n",
    "os.environ[\"PIPELINE_NAME\"] = f'{DATASET_DISPLAY_NAME}-pipeline-v01-'\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"BQ_LOCATION\"] = BQ_LOCATION\n",
    "os.environ[\"BQ_DATASET_NAME\"] = BQ_DATASET_NAME\n",
    "os.environ[\"BQ_TABLE_NAME\"] = BQ_TABLE_NAME\n",
    "os.environ[\"GCS_LOCATION\"] = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/e2e_tests\"\n",
    "os.environ[\"TRAIN_LIMIT\"] = \"10\" #\"1000\"\n",
    "os.environ[\"TEST_LIMIT\"] = \"2\" #\"100\"\n",
    "os.environ[\"UPLOAD_MODEL\"] = \"0\"\n",
    "os.environ[\"ACCURACY_THRESHOLD\"] = \"0.1\"\n",
    "os.environ[\"BEAM_RUNNER\"] = \"DirectRunner\"\n",
    "os.environ[\"TRAINING_RUNNER\"] = \"local\"\n",
    "os.environ[\"ENDPOINT_NAME\"] = 'predict-explain-for-' + f'{DATASET_DISPLAY_NAME}-pipeline-v01-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d353e3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: stellar-orb-408015\n",
      "REGION: us-central1\n",
      "GCS_LOCATION: gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests\n",
      "ARTIFACT_STORE_URI: gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts\n",
      "MODEL_REGISTRY_URI: gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/model_registry\n",
      "DATASET_DISPLAY_NAME: chicago-taxi-tips\n",
      "MODEL_DISPLAY_NAME: chicago-taxi-tips-classifier-v01\n",
      "PIPELINE_NAME: chicago-taxi-tips-pipeline-v01-\n",
      "PIPELINE_ROOT: gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-\n",
      "PIPELINE_DEFINITION_FILE: chicago-taxi-tips-pipeline-v01-info_pipeline.json\n",
      "ML_USE_COLUMN: ml_use\n",
      "EXCLUDE_COLUMNS: trip_start_timestamp\n",
      "TRAIN_LIMIT: 10\n",
      "TEST_LIMIT: 2\n",
      "SERVE_LIMIT: 0\n",
      "NUM_TRAIN_SPLITS: 4\n",
      "NUM_EVAL_SPLITS: 1\n",
      "ACCURACY_THRESHOLD: 0.1\n",
      "USE_KFP_SA: False\n",
      "TFX_IMAGE_URI: \n",
      "BEAM_RUNNER: DirectRunner\n",
      "BEAM_DIRECT_PIPELINE_ARGS: ['--project=stellar-orb-408015', '--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp']\n",
      "BEAM_DATAFLOW_PIPELINE_ARGS: ['--project=stellar-orb-408015', '--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp', '--region=us-east1', '--runner=DirectRunner', '--disk_size_gb=50', '--experiments=disable_worker_container_image_prepull', '--sdk_container_image=']\n",
      "TRAINING_RUNNER: local\n",
      "VERTEX_TRAINING_ARGS: {'project': 'stellar-orb-408015', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': ''}}]}\n",
      "VERTEX_TRAINING_CONFIG: {'ai_platform_training_enable_ucaip': True, 'ai_platform_training_ucaip_region': 'us-central1', 'ai_platform_training_args': {'project': 'stellar-orb-408015', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': ''}}]}, 'use_gpu': False}\n",
      "SERVING_RUNTIME: tf2-cpu.2-12\n",
      "SERVING_IMAGE_URI: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest\n",
      "ENDPOINT_NAME: predict-explain-for-chicago-taxi-tips-pipeline-v01-\n",
      "VERTEX_SERVING_SPEC_PUSHER: {'project': 'stellar-orb-408015', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'gcr.io/tfx-oss-public/tfx:1.14.0'}}], 'explanation_metadata': {'inputs': {'trip_month': {'input_tensor_name': 'trip_month'}, 'trip_day': {'input_tensor_name': 'trip_day'}, 'trip_day_of_week': {'input_tensor_name': 'trip_day_of_week'}, 'trip_hour': {'input_tensor_name': 'trip_hour'}, 'trip_seconds': {'input_tensor_name': 'trip_seconds'}, 'trip_miles': {'input_tensor_name': 'trip_miles'}, 'payment_type': {'input_tensor_name': 'payment_type'}, 'pickup_grid': {'input_tensor_name': 'pickup_grid'}, 'dropoff_grid': {'input_tensor_name': 'dropoff_grid'}, 'euclidean': {'input_tensor_name': 'euclidean'}, 'loc_cross': {'input_tensor_name': 'loc_cross'}}, 'outputs': {'scores': {'output_tensor_name': 'scores'}}}, 'explanation_parameters': {'sampled_shapley_attribution': {'path_count': 10}}}\n",
      "VERTEX_PREDICTION_SPEC: {'project': 'stellar-orb-408015', 'endpoint_name': 'predict-explain-for-chicago-taxi-tips-pipeline-v01-', 'deployed_model_display_name': 'chicago-taxi-tips-classifier-v01', 'machine_type': 'n1-standard-4', 'explanation_metadata': {'inputs': {'trip_month': {'input_tensor_name': 'trip_month'}, 'trip_day': {'input_tensor_name': 'trip_day'}, 'trip_day_of_week': {'input_tensor_name': 'trip_day_of_week'}, 'trip_hour': {'input_tensor_name': 'trip_hour'}, 'trip_seconds': {'input_tensor_name': 'trip_seconds'}, 'trip_miles': {'input_tensor_name': 'trip_miles'}, 'payment_type': {'input_tensor_name': 'payment_type'}, 'pickup_grid': {'input_tensor_name': 'pickup_grid'}, 'dropoff_grid': {'input_tensor_name': 'dropoff_grid'}, 'euclidean': {'input_tensor_name': 'euclidean'}, 'loc_cross': {'input_tensor_name': 'loc_cross'}}, 'outputs': {'scores': {'output_tensor_name': 'scores'}}}, 'explanation_parameters': {'sampled_shapley_attribution': {'path_count': 10}}}\n",
      "VERTEX_PREDICTION_CONFIG: {'ai_platform_enable_vertex': True, 'ai_platform_vertex_region': 'us-central1', 'ai_platform_vertex_container_image_uri': 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest', 'ai_platform_serving_args': {'project': 'stellar-orb-408015', 'endpoint_name': 'predict-explain-for-chicago-taxi-tips-pipeline-v01-', 'deployed_model_display_name': 'chicago-taxi-tips-classifier-v01', 'machine_type': 'n1-standard-4', 'explanation_metadata': {'inputs': {'trip_month': {'input_tensor_name': 'trip_month'}, 'trip_day': {'input_tensor_name': 'trip_day'}, 'trip_day_of_week': {'input_tensor_name': 'trip_day_of_week'}, 'trip_hour': {'input_tensor_name': 'trip_hour'}, 'trip_seconds': {'input_tensor_name': 'trip_seconds'}, 'trip_miles': {'input_tensor_name': 'trip_miles'}, 'payment_type': {'input_tensor_name': 'payment_type'}, 'pickup_grid': {'input_tensor_name': 'pickup_grid'}, 'dropoff_grid': {'input_tensor_name': 'dropoff_grid'}, 'euclidean': {'input_tensor_name': 'euclidean'}, 'loc_cross': {'input_tensor_name': 'loc_cross'}}, 'outputs': {'scores': {'output_tensor_name': 'scores'}}}, 'explanation_parameters': {'sampled_shapley_attribution': {'path_count': 10}}}}\n",
      "BATCH_PREDICTION_BQ_DATASET_NAME: playground_us\n",
      "BATCH_PREDICTION_BQ_TABLE_NAME: chicago_taxitrips_final\n",
      "BATCH_PREDICTION_BEAM_ARGS: {'runner': 'DirectRunner', 'temporary_dir': 'gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp', 'gcs_location': 'gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp', 'project': 'stellar-orb-408015', 'region': 'us-central1', 'setup_file': './setup.py'}\n",
      "BATCH_PREDICTION_JOB_RESOURCES: {'machine_type': 'n1-standard-2', 'starting_replica_count': 1, 'max_replica_count': 10}\n",
      "DATASTORE_PREDICTION_KIND: chicago-taxi-tips-classifier-v01-predictions\n",
      "ENABLE_CACHE: 0\n",
      "UPLOAD_MODEL: 0\n"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d51c12",
   "metadata": {},
   "source": [
    "### Run unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c296235-8901-4066-806c-78a2b65dcec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting typing_extensions==4.7.1\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: typing_extensions\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastapi 0.104.1 requires typing-extensions>=4.8.0, but you have typing-extensions 4.7.1 which is incompatible.\n",
      "tensorflow 2.13.1 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed typing_extensions-4.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install typing_extensions==4.7.1 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9be84a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.13, pytest-8.0.0, pluggy-1.3.0\n",
      "rootdir: /home/jupyter/mlops-with-vertex-ai-steps\n",
      "plugins: typeguard-4.1.5, anyio-3.7.1\n",
      "collected 2 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "src/tests/datasource_utils_tests.py BigQuery Source: stellar-orb-408015.playground_us.chicago_taxitrips_final\n",
      "\u001b[32m.\u001b[0mBigQuery Source: stellar-orb-408015.playground_us.chicago_taxitrips_final\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../../opt/conda/lib/python3.10/site-packages/google/rpc/__init__.py:18\n",
      "  /opt/conda/lib/python3.10/site-packages/google/rpc/__init__.py:18: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "    import pkg_resources\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: 20 warnings\n",
      "  /opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: 17 warnings\n",
      "  /opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2350\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2350\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2350\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2350\n",
      "  /opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(parent)\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871\n",
      "  /opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.logging')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871\n",
      "  /opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.iam')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871\n",
      "  /opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/google/rpc/__init__.py:20\n",
      "  /opt/conda/lib/python3.10/site-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    pkg_resources.declare_namespace(__name__)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m======================== \u001b[32m2 passed\u001b[0m, \u001b[33m\u001b[1m46 warnings\u001b[0m\u001b[33m in 6.29s\u001b[0m\u001b[33m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest src/tests/datasource_utils_tests.py -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4358f955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.13, pytest-8.0.0, pluggy-1.3.0\n",
      "rootdir: /home/jupyter/mlops-with-vertex-ai-steps\n",
      "plugins: typeguard-4.1.5, anyio-3.7.1\n",
      "\u001b[1mcollecting ... \u001b[0m2024-01-31 16:24:45.884715: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "collected 2 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "src/tests/model_tests.py \u001b[32m.\u001b[0mModel: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " trip_month_xf (InputLayer)  [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " trip_day_xf (InputLayer)    [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " trip_day_of_week_xf (Input  [(None,)]                    0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " trip_hour_xf (InputLayer)   [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " trip_seconds_xf (InputLaye  [(None,)]                    0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " trip_miles_xf (InputLayer)  [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " payment_type_xf (InputLaye  [(None,)]                    0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " pickup_grid_xf (InputLayer  [(None,)]                    0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropoff_grid_xf (InputLaye  [(None,)]                    0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " euclidean_xf (InputLayer)   [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " loc_cross_xf (InputLayer)   [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " trip_month_xf_embedding (E  (None, 2)                    202       ['trip_month_xf[0][0]']       \n",
      " mbedding)                                                                                        \n",
      "                                                                                                  \n",
      " trip_day_xf_embedding (Emb  (None, 4)                    404       ['trip_day_xf[0][0]']         \n",
      " edding)                                                                                          \n",
      "                                                                                                  \n",
      " trip_day_of_week_xf_onehot  (None, 101)                  0         ['trip_day_of_week_xf[0][0]'] \n",
      "  (CategoryEncoding)                                                                              \n",
      "                                                                                                  \n",
      " trip_hour_xf_embedding (Em  (None, 3)                    303       ['trip_hour_xf[0][0]']        \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda  (None, 1)                    0         ['trip_seconds_xf[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_1 (TFOpLamb  (None, 1)                    0         ['trip_miles_xf[0][0]']       \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " payment_type_xf_onehot (Ca  (None, 101)                  0         ['payment_type_xf[0][0]']     \n",
      " tegoryEncoding)                                                                                  \n",
      "                                                                                                  \n",
      " pickup_grid_xf_embedding (  (None, 3)                    303       ['pickup_grid_xf[0][0]']      \n",
      " Embedding)                                                                                       \n",
      "                                                                                                  \n",
      " dropoff_grid_xf_embedding   (None, 3)                    303       ['dropoff_grid_xf[0][0]']     \n",
      " (Embedding)                                                                                      \n",
      "                                                                                                  \n",
      " tf.expand_dims_2 (TFOpLamb  (None, 1)                    0         ['euclidean_xf[0][0]']        \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " loc_cross_xf_embedding (Em  (None, 10)                   1010      ['loc_cross_xf[0][0]']        \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " combines_inputs (Concatena  (None, 230)                  0         ['trip_month_xf_embedding[0][0\n",
      " te)                                                                ]',                           \n",
      "                                                                     'trip_day_xf_embedding[0][0]'\n",
      "                                                                    , 'trip_day_of_week_xf_onehot[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'trip_hour_xf_embedding[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'tf.expand_dims[0][0]',      \n",
      "                                                                     'tf.expand_dims_1[0][0]',    \n",
      "                                                                     'payment_type_xf_onehot[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'pickup_grid_xf_embedding[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'dropoff_grid_xf_embedding[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'tf.expand_dims_2[0][0]',    \n",
      "                                                                     'loc_cross_xf_embedding[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " feedforward_network (Seque  (None, 32)                   16864     ['combines_inputs[0][0]']     \n",
      " ntial)                                                                                           \n",
      "                                                                                                  \n",
      " logits (Dense)              (None, 1)                    33        ['feedforward_network[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19422 (75.87 KB)\n",
      "Trainable params: 19422 (75.87 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 3.61s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test src/tests/model_tests.py -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa00fd5",
   "metadata": {},
   "source": [
    "### Run e2e pipeline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb9aad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.13, pytest-8.0.0, pluggy-1.3.0\n",
      "rootdir: /home/jupyter/mlops-with-vertex-ai-steps\n",
      "plugins: typeguard-4.1.5, anyio-3.7.1\n",
      "\u001b[1mcollecting ... \u001b[0m2024-01-31 16:32:34.389665: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Using TensorFlow backend\n",
      "TFX Version: 1.14.0\n",
      "Tensorflow Version: 2.13.1\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py upload_model: 0\n",
      "Pipeline e2e test artifacts stored in: gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests\n",
      "ML metadata store is ready.\n",
      "Excluding no splits because exclude_splits is not set.\n",
      "Excluding no splits because exclude_splits is not set.\n",
      "Pipeline components: ['HyperparamsGen', 'TrainDataGen', 'TestDataGen', 'StatisticsGen', 'SchemaImporter', 'ExampleValidator', 'DataTransformer', 'WarmstartModelResolver', 'ImportSchemaGen', 'ModelTrainer', 'BaselineModelResolver', 'ModelEvaluator', 'Pusher']\n",
      "Beam pipeline args: ['--project=stellar-orb-408015', '--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp']\n",
      "Generating ephemeral wheel package for '/home/jupyter/mlops-with-vertex-ai-steps/src/preprocessing/transformations.py' (including modules: ['etl', 'transformations']).\n",
      "User module package has hash fingerprint version de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.\n",
      "Executing: ['/opt/conda/bin/python3.10', '/var/tmp/tmpgqciat8i/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/var/tmp/tmpaf73duv6', '--dist-dir', '/var/tmp/tmpl6z_h4wk']\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying etl.py -> build/lib\n",
      "copying transformations.py -> build/lib\n",
      "/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n",
      "installing to /var/tmp/tmpaf73duv6\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/etl.py -> /var/tmp/tmpaf73duv6\n",
      "copying build/lib/transformations.py -> /var/tmp/tmpaf73duv6\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_DataTransformer.egg-info\n",
      "writing tfx_user_code_DataTransformer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_DataTransformer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_DataTransformer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_DataTransformer.egg-info to /var/tmp/tmpaf73duv6/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3.10.egg-info\n",
      "running install_scripts\n",
      "creating /var/tmp/tmpaf73duv6/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/WHEEL\n",
      "creating '/var/tmp/tmpl6z_h4wk/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl' and adding '/var/tmp/tmpaf73duv6' to it\n",
      "adding 'etl.py'\n",
      "adding 'transformations.py'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/METADATA'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/RECORD'\n",
      "removing /var/tmp/tmpaf73duv6\n",
      "Successfully built user code wheel distribution at 'gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl'; target user module is 'transformations'.\n",
      "Full user module path is 'transformations@gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl'\n",
      "Generating ephemeral wheel package for '/home/jupyter/mlops-with-vertex-ai-steps/src/tfx_model_training/model_runner.py' (including modules: ['model_exporter', 'features', 'defaults', 'model_trainer', 'model_runner', 'model_input', 'model']).\n",
      "User module package has hash fingerprint version b2e2657f2c58d6b17ae2108a61517fbe1be6ffa4a912f5b2d0055ab70b96bd4c.\n",
      "Executing: ['/opt/conda/bin/python3.10', '/var/tmp/tmpnurbi17p/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/var/tmp/tmpuys8120q', '--dist-dir', '/var/tmp/tmpetwtatri']\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying model_exporter.py -> build/lib\n",
      "copying features.py -> build/lib\n",
      "copying defaults.py -> build/lib\n",
      "copying model_trainer.py -> build/lib\n",
      "copying model_runner.py -> build/lib\n",
      "copying model_input.py -> build/lib\n",
      "copying model.py -> build/lib\n",
      "/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n",
      "installing to /var/tmp/tmpuys8120q\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/model_input.py -> /var/tmp/tmpuys8120q\n",
      "copying build/lib/model_exporter.py -> /var/tmp/tmpuys8120q\n",
      "copying build/lib/features.py -> /var/tmp/tmpuys8120q\n",
      "copying build/lib/defaults.py -> /var/tmp/tmpuys8120q\n",
      "copying build/lib/model_trainer.py -> /var/tmp/tmpuys8120q\n",
      "copying build/lib/model_runner.py -> /var/tmp/tmpuys8120q\n",
      "copying build/lib/model.py -> /var/tmp/tmpuys8120q\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_ModelTrainer.egg-info\n",
      "writing tfx_user_code_ModelTrainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_ModelTrainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_ModelTrainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_ModelTrainer.egg-info to /var/tmp/tmpuys8120q/tfx_user_code_ModelTrainer-0.0+b2e2657f2c58d6b17ae2108a61517fbe1be6ffa4a912f5b2d0055ab70b96bd4c-py3.10.egg-info\n",
      "running install_scripts\n",
      "creating /var/tmp/tmpuys8120q/tfx_user_code_ModelTrainer-0.0+b2e2657f2c58d6b17ae2108a61517fbe1be6ffa4a912f5b2d0055ab70b96bd4c.dist-info/WHEEL\n",
      "creating '/var/tmp/tmpetwtatri/tfx_user_code_ModelTrainer-0.0+b2e2657f2c58d6b17ae2108a61517fbe1be6ffa4a912f5b2d0055ab70b96bd4c-py3-none-any.whl' and adding '/var/tmp/tmpuys8120q' to it\n",
      "adding 'defaults.py'\n",
      "adding 'features.py'\n",
      "adding 'model.py'\n",
      "adding 'model_exporter.py'\n",
      "adding 'model_input.py'\n",
      "adding 'model_runner.py'\n",
      "adding 'model_trainer.py'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+b2e2657f2c58d6b17ae2108a61517fbe1be6ffa4a912f5b2d0055ab70b96bd4c.dist-info/METADATA'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+b2e2657f2c58d6b17ae2108a61517fbe1be6ffa4a912f5b2d0055ab70b96bd4c.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+b2e2657f2c58d6b17ae2108a61517fbe1be6ffa4a912f5b2d0055ab70b96bd4c.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+b2e2657f2c58d6b17ae2108a61517fbe1be6ffa4a912f5b2d0055ab70b96bd4c.dist-info/RECORD'\n",
      "removing /var/tmp/tmpuys8120q\n",
      "Successfully built user code wheel distribution at 'gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/_wheels/tfx_user_code_ModelTrainer-0.0+b2e2657f2c58d6b17ae2108a61517fbe1be6ffa4a912f5b2d0055ab70b96bd4c-py3-none-any.whl'; target user module is 'model_runner'.\n",
      "Full user module path is 'model_runner@gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/_wheels/tfx_user_code_ModelTrainer-0.0+b2e2657f2c58d6b17ae2108a61517fbe1be6ffa4a912f5b2d0055ab70b96bd4c-py3-none-any.whl'\n",
      "Using deployment config:\n",
      " executor_specs {\n",
      "  key: \"DataTransformer\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.transform.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=stellar-orb-408015\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp\"\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--project=stellar-orb-408015\"\n",
      "        }\n",
      "      }\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ExampleValidator\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_validator.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"HyperparamsGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"src.tfx_pipelines.components.hyperparameters_gen_Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ImportSchemaGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.schema_gen.import_schema_gen.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ModelEvaluator\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.evaluator.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=stellar-orb-408015\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp\"\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--project=stellar-orb-408015\"\n",
      "        }\n",
      "      }\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ModelTrainer\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Pusher\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.extensions.google_cloud_ai_platform.pusher.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"StatisticsGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.statistics_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=stellar-orb-408015\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp\"\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--project=stellar-orb-408015\"\n",
      "        }\n",
      "      }\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"TestDataGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.extensions.google_cloud_big_query.example_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=stellar-orb-408015\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp\"\n",
      "      beam_pipeline_args: \"--project=stellar-orb-408015\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp\"\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--project=stellar-orb-408015\"\n",
      "        }\n",
      "      }\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp\"\n",
      "        }\n",
      "      }\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--project=stellar-orb-408015\"\n",
      "        }\n",
      "      }\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"TrainDataGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.extensions.google_cloud_big_query.example_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=stellar-orb-408015\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp\"\n",
      "      beam_pipeline_args: \"--project=stellar-orb-408015\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp\"\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--project=stellar-orb-408015\"\n",
      "        }\n",
      "      }\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp\"\n",
      "        }\n",
      "      }\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--project=stellar-orb-408015\"\n",
      "        }\n",
      "      }\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/temp\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"TestDataGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.QueryBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"TrainDataGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.QueryBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata_connection_config {\n",
      "  database_connection_config {\n",
      "    sqlite {\n",
      "      filename_uri: \"mlmd.sqllite\"\n",
      "      connection_mode: READWRITE_OPENCREATE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Using connection config:\n",
      " sqlite {\n",
      "  filename_uri: \"mlmd.sqllite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "Component BaselineModelResolver is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "  }\n",
      "  id: \"BaselineModelResolver\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-01-31T16:32:55.662553\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-.BaselineModelResolver\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"_generated_model_3\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      hidden: true\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"_generated_modelblessing_4\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      hidden: true\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      input_graph_ref {\n",
      "        graph_id: \"graph_1\"\n",
      "        key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      input_graph_ref {\n",
      "        graph_id: \"graph_1\"\n",
      "        key: \"model_blessing\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  input_graphs {\n",
      "    key: \"graph_1\"\n",
      "    value {\n",
      "      nodes {\n",
      "        key: \"dict_2\"\n",
      "        value {\n",
      "          output_data_type: ARTIFACT_MULTIMAP\n",
      "          dict_node {\n",
      "            node_ids {\n",
      "              key: \"model\"\n",
      "              value: \"input_3\"\n",
      "            }\n",
      "            node_ids {\n",
      "              key: \"model_blessing\"\n",
      "              value: \"input_4\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      nodes {\n",
      "        key: \"input_3\"\n",
      "        value {\n",
      "          output_data_type: ARTIFACT_LIST\n",
      "          input_node {\n",
      "            input_key: \"_generated_model_3\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      nodes {\n",
      "        key: \"input_4\"\n",
      "        value {\n",
      "          output_data_type: ARTIFACT_LIST\n",
      "          input_node {\n",
      "            input_key: \"_generated_modelblessing_4\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      nodes {\n",
      "        key: \"op_1\"\n",
      "        value {\n",
      "          output_data_type: ARTIFACT_MULTIMAP\n",
      "          op_node {\n",
      "            op_type: \"tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy\"\n",
      "            args {\n",
      "              node_id: \"dict_2\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      result_node: \"op_1\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "Running as an resolver node.\n",
      "MetadataStore with DB connection initialized\n",
      "[BaselineModelResolver] Resolved inputs: ({'model_blessing': [], 'model': []},)\n",
      "Component BaselineModelResolver is finished.\n",
      "Component HyperparamsGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"src.tfx_pipelines.components.hyperparameters_gen\"\n",
      "  }\n",
      "  id: \"HyperparamsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-01-31T16:32:55.662553\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-.HyperparamsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"hyperparameters\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"HyperParameters\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"batch_size\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 512\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"learning_rate\"\n",
      "    value {\n",
      "      field_value {\n",
      "        double_value: 0.001\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"num_epochs\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "[HyperparamsGen] Resolved inputs: ({},)\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 2\n",
      "Going to run a new execution: ExecutionInfo(execution_id=2, input_dict={}, output_dict=defaultdict(<class 'list'>, {'hyperparameters': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/HyperparamsGen/hyperparameters/2\"\n",
      ", artifact_type: name: \"HyperParameters\"\n",
      ")]}), exec_properties={'learning_rate': 0.001, 'batch_size': 512, 'num_epochs': 1}, execution_output_uri='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/HyperparamsGen/.system/executor_execution/2/executor_output.pb', stateful_working_dir='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/HyperparamsGen/.system/stateful_working_dir/2024-01-31T16:32:55.662553', tmp_dir='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/HyperparamsGen/.system/executor_execution/2/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"src.tfx_pipelines.components.hyperparameters_gen\"\n",
      "  }\n",
      "  id: \"HyperparamsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-01-31T16:32:55.662553\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-.HyperparamsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"hyperparameters\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"HyperParameters\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"batch_size\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 512\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"learning_rate\"\n",
      "    value {\n",
      "      field_value {\n",
      "        double_value: 0.001\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"num_epochs\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"chicago-taxi-tips-pipeline-v01-\"\n",
      ", pipeline_run_id='2024-01-31T16:32:55.662553', top_level_pipeline_run_id=None)\n",
      "Hyperparameters: {'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001}\n",
      "Hyperparameters are written to: gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/HyperparamsGen/hyperparameters/2/hyperparameters.json\n",
      "Cleaning up stateless execution info.\n",
      "Execution 2 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai-steps/gs:/stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/HyperparamsGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'hyperparameters': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/HyperparamsGen/hyperparameters/2\"\n",
      ", artifact_type: name: \"HyperParameters\"\n",
      ")]}) for execution 2\n",
      "MetadataStore with DB connection initialized\n",
      "Component HyperparamsGen is finished.\n",
      "Component ImportSchemaGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.schema_gen.import_schema_gen.component.ImportSchemaGen\"\n",
      "  }\n",
      "  id: \"ImportSchemaGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-01-31T16:32:55.662553\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-.ImportSchemaGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"schema_file\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"src/tfx_model_training/raw_schema/schema.pbtxt\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "[ImportSchemaGen] Resolved inputs: ({},)\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 3\n",
      "Going to run a new execution: ExecutionInfo(execution_id=3, input_dict={}, output_dict=defaultdict(<class 'list'>, {'schema': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ImportSchemaGen/schema/3\"\n",
      ", artifact_type: name: \"Schema\"\n",
      ")]}), exec_properties={'schema_file': 'src/tfx_model_training/raw_schema/schema.pbtxt'}, execution_output_uri='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ImportSchemaGen/.system/executor_execution/3/executor_output.pb', stateful_working_dir='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ImportSchemaGen/.system/stateful_working_dir/2024-01-31T16:32:55.662553', tmp_dir='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ImportSchemaGen/.system/executor_execution/3/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.schema_gen.import_schema_gen.component.ImportSchemaGen\"\n",
      "  }\n",
      "  id: \"ImportSchemaGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-01-31T16:32:55.662553\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-.ImportSchemaGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"schema_file\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"src/tfx_model_training/raw_schema/schema.pbtxt\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"chicago-taxi-tips-pipeline-v01-\"\n",
      ", pipeline_run_id='2024-01-31T16:32:55.662553', top_level_pipeline_run_id=None)\n",
      "Copied a schema file from src/tfx_model_training/raw_schema/schema.pbtxt to gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ImportSchemaGen/schema/3/schema.pbtxt.\n",
      "Cleaning up stateless execution info.\n",
      "Execution 3 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai-steps/gs:/stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ImportSchemaGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'schema': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ImportSchemaGen/schema/3\"\n",
      ", artifact_type: name: \"Schema\"\n",
      ")]}) for execution 3\n",
      "MetadataStore with DB connection initialized\n",
      "Component ImportSchemaGen is finished.\n",
      "Component SchemaImporter is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.importer.Importer\"\n",
      "  }\n",
      "  id: \"SchemaImporter\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-01-31T16:32:55.662553\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-.SchemaImporter\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"result\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"artifact_uri\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"src/tfx_model_training/raw_schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_key\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"result\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"reimport\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "Running as an importer node.\n",
      "MetadataStore with DB connection initialized\n",
      "Processing source uri: src/tfx_model_training/raw_schema, properties: {}, custom_properties: {}\n",
      "Component SchemaImporter is finished.\n",
      "Component TestDataGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TestDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-01-31T16:32:55.662553\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-.TestDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM playground_us.chicago_taxitrips_final \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 2\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"test\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "[TestDataGen] Resolved inputs: ({},)\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 5\n",
      "Going to run a new execution: ExecutionInfo(execution_id=5, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/TestDataGen/examples/5\"\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'output_data_format': 6, 'output_file_format': 5, 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM playground_us.chicago_taxitrips_final \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 2\"\\n    }\\n  ]\\n}', 'custom_config': '{}', 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"test\"\\n      }\\n    ]\\n  }\\n}', 'span': 0, 'version': None, 'input_fingerprint': None}, execution_output_uri='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/TestDataGen/.system/executor_execution/5/executor_output.pb', stateful_working_dir='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/TestDataGen/.system/stateful_working_dir/2024-01-31T16:32:55.662553', tmp_dir='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/TestDataGen/.system/executor_execution/5/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TestDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-01-31T16:32:55.662553\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-.TestDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM playground_us.chicago_taxitrips_final \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 2\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"test\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"chicago-taxi-tips-pipeline-v01-\"\n",
      ", pipeline_run_id='2024-01-31T16:32:55.662553', top_level_pipeline_run_id=None)\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /opt/conda/lib/python3.10/site-packages/tfx to temp dir /var/tmp/tmpadeba37y/build/tfx\n",
      "Generating a temp setup file at /var/tmp/tmpadeba37y/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /var/tmp/tmpadeba37y/build/tfx/setup.log\n",
      "Added --extra_package=/var/tmp/tmpadeba37y/build/tfx/dist/tfx_ephemeral-1.14.0.tar.gz to beam args\n",
      "Length of label `tfx-extensions-google_cloud_big_query-example_gen-executor-executor` exceeds maximum length(63), trimmed.\n",
      "Generating examples.\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f65dd9aad40> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f65dd9aae60> ====================\n",
      "==================== <function pack_combiners at 0x7f65dd9ab370> ====================\n",
      "==================== <function lift_combiners at 0x7f65dd9ab400> ====================\n",
      "==================== <function expand_sdf at 0x7f65dd9ab5b0> ====================\n",
      "==================== <function expand_gbk at 0x7f65dd9ab640> ====================\n",
      "==================== <function sink_flattens at 0x7f65dd9ab760> ====================\n",
      "==================== <function greedily_fuse at 0x7f65dd9ab7f0> ====================\n",
      "==================== <function read_to_impulse at 0x7f65dd9ab880> ====================\n",
      "==================== <function impulse_to_input at 0x7f65dd9ab910> ====================\n",
      "==================== <function sort_stages at 0x7f65dd9abb50> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7f65dd9abc70> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f65dd9abac0> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f65dd9abbe0> ====================\n",
      "Creating state cache with size 104857600\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f65d8c2b3d0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Setting socket default timeout to 60 seconds.\n",
      "socket default timeout is 60.0 seconds.\n",
      "Started BigQuery job: <JobReference\n",
      " location: 'US'\n",
      " projectId: 'stellar-orb-408015'>\n",
      " bq show -j --format=prettyjson --project_id=stellar-orb-408015 None\n",
      "Using location 'US' from table <TableReference\n",
      " datasetId: 'playground_us'\n",
      " projectId: 'stellar-orb-408015'\n",
      " tableId: 'chicago_taxitrips_final'> referenced by query \n",
      "    SELECT \n",
      "        IF(trip_month IS NULL, -1, trip_month) trip_month,\n",
      "        IF(trip_day IS NULL, -1, trip_day) trip_day,\n",
      "        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\n",
      "        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\n",
      "        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\n",
      "        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\n",
      "        IF(payment_type IS NULL, 'NA', payment_type) payment_type,\n",
      "        IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\n",
      "        IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\n",
      "        IF(euclidean IS NULL, -1, euclidean) euclidean,\n",
      "        IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross,\n",
      "        tip_bin\n",
      "    FROM playground_us.chicago_taxitrips_final \n",
      "    WHERE ML_use = 'TEST'\n",
      "    LIMIT 2\n",
      "Dataset stellar-orb-408015:beam_temp_dataset_09596f66189b4ff792eb4898fffd5e1f does not exist so we will create it as temporary with location=US\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_QUERY_BQ_EXPORT_JOB_4b519a16-6_1706718788_537'\n",
      " location: 'US'\n",
      " projectId: 'stellar-orb-408015'>\n",
      " bq show -j --format=prettyjson --project_id=stellar-orb-408015 beam_bq_job_QUERY_BQ_EXPORT_JOB_4b519a16-6_1706718788_537\n",
      "Job stellar-orb-408015:US.beam_bq_job_QUERY_BQ_EXPORT_JOB_4b519a16-6_1706718788_537 status: RUNNING\n",
      "Job stellar-orb-408015:US.beam_bq_job_QUERY_BQ_EXPORT_JOB_4b519a16-6_1706718788_537 status: DONE\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_EXPORT_BQ_EXPORT_JOB_4b519a16-6_1706718794_741'\n",
      " location: 'US'\n",
      " projectId: 'stellar-orb-408015'>\n",
      " bq show -j --format=prettyjson --project_id=stellar-orb-408015 beam_bq_job_EXPORT_BQ_EXPORT_JOB_4b519a16-6_1706718794_741\n",
      "Job stellar-orb-408015:US.beam_bq_job_EXPORT_BQ_EXPORT_JOB_4b519a16-6_1706718794_741 status: RUNNING\n",
      "Job stellar-orb-408015:US.beam_bq_job_EXPORT_BQ_EXPORT_JOB_4b519a16-6_1706718794_741 status: DONE\n",
      "Finished listing 1 files in 0.05000472068786621 seconds.\n",
      "Finished listing 1 files in 0.053696632385253906 seconds.\n",
      "Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "Finished listing 1 files in 0.047202110290527344 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Examples generated.\n",
      "Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "Value type <class 'NoneType'> of key input_fingerprint in exec_properties is not supported, going to drop it\n",
      "Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "Cleaning up stateless execution info.\n",
      "Execution 5 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai-steps/gs:/stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/TestDataGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/TestDataGen/examples/5\"\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}) for execution 5\n",
      "MetadataStore with DB connection initialized\n",
      "Component TestDataGen is finished.\n",
      "Component TrainDataGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TrainDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-01-31T16:32:55.662553\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-.TrainDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM playground_us.chicago_taxitrips_final \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 10\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "[TrainDataGen] Resolved inputs: ({},)\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 6\n",
      "Going to run a new execution: ExecutionInfo(execution_id=6, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/TrainDataGen/examples/6\"\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 4,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'output_data_format': 6, 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM playground_us.chicago_taxitrips_final \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 10\"\\n    }\\n  ]\\n}', 'custom_config': '{}', 'output_file_format': 5, 'span': 0, 'version': None, 'input_fingerprint': None}, execution_output_uri='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/TrainDataGen/.system/executor_execution/6/executor_output.pb', stateful_working_dir='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/TrainDataGen/.system/stateful_working_dir/2024-01-31T16:32:55.662553', tmp_dir='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/TrainDataGen/.system/executor_execution/6/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TrainDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-01-31T16:32:55.662553\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-.TrainDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM playground_us.chicago_taxitrips_final \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 10\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"chicago-taxi-tips-pipeline-v01-\"\n",
      ", pipeline_run_id='2024-01-31T16:32:55.662553', top_level_pipeline_run_id=None)\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /opt/conda/lib/python3.10/site-packages/tfx to temp dir /var/tmp/tmp0lsnm4h6/build/tfx\n",
      "Generating a temp setup file at /var/tmp/tmp0lsnm4h6/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /var/tmp/tmp0lsnm4h6/build/tfx/setup.log\n",
      "Added --extra_package=/var/tmp/tmp0lsnm4h6/build/tfx/dist/tfx_ephemeral-1.14.0.tar.gz to beam args\n",
      "Length of label `tfx-extensions-google_cloud_big_query-example_gen-executor-executor` exceeds maximum length(63), trimmed.\n",
      "Generating examples.\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f65dd9aad40> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f65dd9aae60> ====================\n",
      "==================== <function pack_combiners at 0x7f65dd9ab370> ====================\n",
      "==================== <function lift_combiners at 0x7f65dd9ab400> ====================\n",
      "==================== <function expand_sdf at 0x7f65dd9ab5b0> ====================\n",
      "==================== <function expand_gbk at 0x7f65dd9ab640> ====================\n",
      "==================== <function sink_flattens at 0x7f65dd9ab760> ====================\n",
      "==================== <function greedily_fuse at 0x7f65dd9ab7f0> ====================\n",
      "==================== <function read_to_impulse at 0x7f65dd9ab880> ====================\n",
      "==================== <function impulse_to_input at 0x7f65dd9ab910> ====================\n",
      "==================== <function sort_stages at 0x7f65dd9abb50> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7f65dd9abc70> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f65dd9abac0> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f65dd9abbe0> ====================\n",
      "Creating state cache with size 104857600\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f65d8b2a0e0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Started BigQuery job: <JobReference\n",
      " location: 'US'\n",
      " projectId: 'stellar-orb-408015'>\n",
      " bq show -j --format=prettyjson --project_id=stellar-orb-408015 None\n",
      "Using location 'US' from table <TableReference\n",
      " datasetId: 'playground_us'\n",
      " projectId: 'stellar-orb-408015'\n",
      " tableId: 'chicago_taxitrips_final'> referenced by query \n",
      "    SELECT \n",
      "        IF(trip_month IS NULL, -1, trip_month) trip_month,\n",
      "        IF(trip_day IS NULL, -1, trip_day) trip_day,\n",
      "        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\n",
      "        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\n",
      "        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\n",
      "        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\n",
      "        IF(payment_type IS NULL, 'NA', payment_type) payment_type,\n",
      "        IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\n",
      "        IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\n",
      "        IF(euclidean IS NULL, -1, euclidean) euclidean,\n",
      "        IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross,\n",
      "        tip_bin\n",
      "    FROM playground_us.chicago_taxitrips_final \n",
      "    WHERE ML_use = 'UNASSIGNED'\n",
      "    LIMIT 10\n",
      "Dataset stellar-orb-408015:beam_temp_dataset_22c76cbdae814668839d7e84bdaece87 does not exist so we will create it as temporary with location=US\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_QUERY_BQ_EXPORT_JOB_f1aef823-5_1706718809_941'\n",
      " location: 'US'\n",
      " projectId: 'stellar-orb-408015'>\n",
      " bq show -j --format=prettyjson --project_id=stellar-orb-408015 beam_bq_job_QUERY_BQ_EXPORT_JOB_f1aef823-5_1706718809_941\n",
      "Job stellar-orb-408015:US.beam_bq_job_QUERY_BQ_EXPORT_JOB_f1aef823-5_1706718809_941 status: RUNNING\n",
      "Job stellar-orb-408015:US.beam_bq_job_QUERY_BQ_EXPORT_JOB_f1aef823-5_1706718809_941 status: DONE\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_EXPORT_BQ_EXPORT_JOB_f1aef823-5_1706718815_816'\n",
      " location: 'US'\n",
      " projectId: 'stellar-orb-408015'>\n",
      " bq show -j --format=prettyjson --project_id=stellar-orb-408015 beam_bq_job_EXPORT_BQ_EXPORT_JOB_f1aef823-5_1706718815_816\n",
      "Job stellar-orb-408015:US.beam_bq_job_EXPORT_BQ_EXPORT_JOB_f1aef823-5_1706718815_816 status: RUNNING\n",
      "Job stellar-orb-408015:US.beam_bq_job_EXPORT_BQ_EXPORT_JOB_f1aef823-5_1706718815_816 status: DONE\n",
      "Finished listing 1 files in 0.0474088191986084 seconds.\n",
      "Finished listing 1 files in 0.050019025802612305 seconds.\n",
      "Finished listing 1 files in 0.051239967346191406 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Finished listing 1 files in 0.046669960021972656 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.19 seconds.\n",
      "Examples generated.\n",
      "Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "Value type <class 'NoneType'> of key input_fingerprint in exec_properties is not supported, going to drop it\n",
      "Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "Cleaning up stateless execution info.\n",
      "Execution 6 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai-steps/gs:/stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/TrainDataGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/TrainDataGen/examples/6\"\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}) for execution 6\n",
      "MetadataStore with DB connection initialized\n",
      "Component TrainDataGen is finished.\n",
      "Component WarmstartModelResolver is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "  }\n",
      "  id: \"WarmstartModelResolver\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-01-31T16:32:55.662553\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-.WarmstartModelResolver\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"_generated_model_2\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      hidden: true\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      input_graph_ref {\n",
      "        graph_id: \"graph_1\"\n",
      "        key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  input_graphs {\n",
      "    key: \"graph_1\"\n",
      "    value {\n",
      "      nodes {\n",
      "        key: \"dict_2\"\n",
      "        value {\n",
      "          output_data_type: ARTIFACT_MULTIMAP\n",
      "          dict_node {\n",
      "            node_ids {\n",
      "              key: \"model\"\n",
      "              value: \"input_3\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      nodes {\n",
      "        key: \"input_3\"\n",
      "        value {\n",
      "          output_data_type: ARTIFACT_LIST\n",
      "          input_node {\n",
      "            input_key: \"_generated_model_2\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      nodes {\n",
      "        key: \"op_1\"\n",
      "        value {\n",
      "          output_data_type: ARTIFACT_MULTIMAP\n",
      "          op_node {\n",
      "            op_type: \"tfx.dsl.input_resolution.strategies.latest_artifact_strategy.LatestArtifactStrategy\"\n",
      "            args {\n",
      "              node_id: \"dict_2\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      result_node: \"op_1\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "Running as an resolver node.\n",
      "MetadataStore with DB connection initialized\n",
      "[WarmstartModelResolver] Input resolution error: Error while resolving inputs for WarmstartModelResolver: <tfx.dsl.input_resolution.strategies.latest_artifact_strategy.LatestArtifactStrategy object at 0x7f65d892b5b0> returned None.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tfx/orchestration/portable/resolver_node_handler.py\", line 74, in run\n",
      "    resolved_inputs = inputs_utils.resolve_input_artifacts(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tfx/orchestration/portable/inputs_utils.py\", line 66, in resolve_input_artifacts\n",
      "    resolved = node_inputs_resolver.resolve(metadata_handler, node_inputs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tfx/orchestration/portable/input_resolution/node_inputs_resolver.py\", line 462, in resolve\n",
      "    _resolve_input_graph_ref(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tfx/orchestration/portable/input_resolution/node_inputs_resolver.py\", line 325, in _resolve_input_graph_ref\n",
      "    result = graph_fn(input_dict)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tfx/orchestration/portable/input_resolution/input_graph_resolver.py\", line 254, in facade_fn\n",
      "    return graph_fn({\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tfx/orchestration/portable/input_resolution/input_graph_resolver.py\", line 204, in new_graph_fn\n",
      "    return graph_fn({**data, node_id: output})\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tfx/orchestration/portable/input_resolution/input_graph_resolver.py\", line 197, in new_graph_fn\n",
      "    output = node_fn(data)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tfx/orchestration/portable/input_resolution/input_graph_resolver.py\", line 149, in _evaluate_op_node\n",
      "    raise exceptions.InputResolutionError(f'{strategy} returned None.')\n",
      "tfx.orchestration.portable.input_resolution.exceptions.InputResolutionError: Error while resolving inputs for WarmstartModelResolver: <tfx.dsl.input_resolution.strategies.latest_artifact_strategy.LatestArtifactStrategy object at 0x7f65d892b5b0> returned None.\n",
      "Component WarmstartModelResolver is finished.\n",
      "Component StatisticsGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-01-31T16:32:55.662553\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-01-31T16:32:55.662553\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "ArtifactQuery.property_predicate is not supported.\n",
      "[StatisticsGen] Resolved inputs: ({'examples': [Artifact(artifact: id: 5\n",
      "type_id: 21\n",
      "uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/TrainDataGen/examples/6\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.14.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "type: \"Examples\"\n",
      "create_time_since_epoch: 1706718823518\n",
      "last_update_time_since_epoch: 1706718823518\n",
      ", artifact_type: id: 21\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]},)\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 8\n",
      "Going to run a new execution: ExecutionInfo(execution_id=8, input_dict={'examples': [Artifact(artifact: id: 5\n",
      "type_id: 21\n",
      "uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/TrainDataGen/examples/6\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.14.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "type: \"Examples\"\n",
      "create_time_since_epoch: 1706718823518\n",
      "last_update_time_since_epoch: 1706718823518\n",
      ", artifact_type: id: 21\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/StatisticsGen/statistics/8\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/StatisticsGen/.system/executor_execution/8/executor_output.pb', stateful_working_dir='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/StatisticsGen/.system/stateful_working_dir/2024-01-31T16:32:55.662553', tmp_dir='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/StatisticsGen/.system/executor_execution/8/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-01-31T16:32:55.662553\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-01-31T16:32:55.662553\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"chicago-taxi-tips-pipeline-v01-\"\n",
      ", pipeline_run_id='2024-01-31T16:32:55.662553', top_level_pipeline_run_id=None)\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /opt/conda/lib/python3.10/site-packages/tfx to temp dir /var/tmp/tmpp6atio6p/build/tfx\n",
      "Generating a temp setup file at /var/tmp/tmpp6atio6p/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /var/tmp/tmpp6atio6p/build/tfx/setup.log\n",
      "Added --extra_package=/var/tmp/tmpp6atio6p/build/tfx/dist/tfx_ephemeral-1.14.0.tar.gz to beam args\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Generating statistics for split train.\n",
      "Finished listing 1 files in 0.05154585838317871 seconds.\n",
      "Using Any for unsupported type: typing.Sequence[str]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.utils.path.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.utils.path.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.utils.path.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Statistics for split train written to gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/StatisticsGen/statistics/8/Split-train.\n",
      "Generating statistics for split eval.\n",
      "Finished listing 1 files in 0.05462288856506348 seconds.\n",
      "Using Any for unsupported type: typing.Sequence[str]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.utils.path.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.utils.path.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.utils.path.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Statistics for split eval written to gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/StatisticsGen/statistics/8/Split-eval.\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f65dd9aad40> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f65dd9aae60> ====================\n",
      "==================== <function pack_combiners at 0x7f65dd9ab370> ====================\n",
      "==================== <function lift_combiners at 0x7f65dd9ab400> ====================\n",
      "==================== <function expand_sdf at 0x7f65dd9ab5b0> ====================\n",
      "==================== <function expand_gbk at 0x7f65dd9ab640> ====================\n",
      "==================== <function sink_flattens at 0x7f65dd9ab760> ====================\n",
      "==================== <function greedily_fuse at 0x7f65dd9ab7f0> ====================\n",
      "==================== <function read_to_impulse at 0x7f65dd9ab880> ====================\n",
      "==================== <function impulse_to_input at 0x7f65dd9ab910> ====================\n",
      "==================== <function sort_stages at 0x7f65dd9abb50> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7f65dd9abc70> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f65dd9abac0> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f65dd9abbe0> ====================\n",
      "Creating state cache with size 104857600\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f65d870b220> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Finished listing 1 files in 0.05188441276550293 seconds.\n",
      "Finished listing 1 files in 0.05156874656677246 seconds.\n",
      "BatchElements statistics: element_count=7 batch_count=4 next_batch_size=6 timings=[(1, 0.0030221939086914062), (2, 0.0029799938201904297), (3, 0.0025894641876220703)]\n",
      "BatchElements statistics: element_count=3 batch_count=3 next_batch_size=1 timings=[(1, 0.0022346973419189453), (1, 0.0023756027221679688)]\n",
      "Finished listing 1 files in 0.04337668418884277 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.19 seconds.\n",
      "Finished listing 1 files in 0.05371570587158203 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.17 seconds.\n",
      "Cleaning up stateless execution info.\n",
      "Execution 8 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai-steps/gs:/stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/StatisticsGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/StatisticsGen/statistics/8\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}) for execution 8\n",
      "MetadataStore with DB connection initialized\n",
      "Component StatisticsGen is finished.\n",
      "Component ExampleValidator is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "  }\n",
      "  id: \"ExampleValidator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-01-31T16:32:55.662553\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-.ExampleValidator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-01-31T16:32:55.662553\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-01-31T16:32:55.662553\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "            base_type: STATISTICS\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "ArtifactQuery.property_predicate is not supported.\n",
      "ArtifactQuery.property_predicate is not supported.\n",
      "[ExampleValidator] Resolved inputs: ({'statistics': [Artifact(artifact: id: 6\n",
      "type_id: 23\n",
      "uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/StatisticsGen/statistics/8\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"stats_dashboard_link\"\n",
      "  value {\n",
      "    string_value: \"\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.14.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "type: \"ExampleStatistics\"\n",
      "create_time_since_epoch: 1706718834170\n",
      "last_update_time_since_epoch: 1706718834170\n",
      ", artifact_type: id: 23\n",
      "name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'schema': [Artifact(artifact: id: 3\n",
      "type_id: 18\n",
      "uri: \"src/tfx_model_training/raw_schema\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.14.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "type: \"Schema\"\n",
      "create_time_since_epoch: 1706718781341\n",
      "last_update_time_since_epoch: 1706718781341\n",
      ", artifact_type: id: 18\n",
      "name: \"Schema\"\n",
      ")]},)\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 9\n",
      "Going to run a new execution: ExecutionInfo(execution_id=9, input_dict={'statistics': [Artifact(artifact: id: 6\n",
      "type_id: 23\n",
      "uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/StatisticsGen/statistics/8\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"stats_dashboard_link\"\n",
      "  value {\n",
      "    string_value: \"\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.14.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "type: \"ExampleStatistics\"\n",
      "create_time_since_epoch: 1706718834170\n",
      "last_update_time_since_epoch: 1706718834170\n",
      ", artifact_type: id: 23\n",
      "name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'schema': [Artifact(artifact: id: 3\n",
      "type_id: 18\n",
      "uri: \"src/tfx_model_training/raw_schema\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.14.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "type: \"Schema\"\n",
      "create_time_since_epoch: 1706718781341\n",
      "last_update_time_since_epoch: 1706718781341\n",
      ", artifact_type: id: 18\n",
      "name: \"Schema\"\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ExampleValidator/anomalies/9\"\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ExampleValidator/.system/executor_execution/9/executor_output.pb', stateful_working_dir='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ExampleValidator/.system/stateful_working_dir/2024-01-31T16:32:55.662553', tmp_dir='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ExampleValidator/.system/executor_execution/9/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "  }\n",
      "  id: \"ExampleValidator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-01-31T16:32:55.662553\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-.ExampleValidator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-01-31T16:32:55.662553\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-01-31T16:32:55.662553\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "            base_type: STATISTICS\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"chicago-taxi-tips-pipeline-v01-\"\n",
      ", pipeline_run_id='2024-01-31T16:32:55.662553', top_level_pipeline_run_id=None)\n",
      "Validating schema against the computed statistics for split train.\n",
      "Validation complete for split train. Anomalies written to gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ExampleValidator/anomalies/9/Split-train.\n",
      "Validating schema against the computed statistics for split eval.\n",
      "Validation complete for split eval. Anomalies written to gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ExampleValidator/anomalies/9/Split-eval.\n",
      "Cleaning up stateless execution info.\n",
      "Execution 9 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai-steps/gs:/stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ExampleValidator/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ExampleValidator/anomalies/9\"\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")]}) for execution 9\n",
      "MetadataStore with DB connection initialized\n",
      "Component ExampleValidator is finished.\n",
      "Component DataTransformer is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.transform.component.Transform\"\n",
      "    base_type: TRANSFORM\n",
      "  }\n",
      "  id: \"DataTransformer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-01-31T16:32:55.662553\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-.DataTransformer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-01-31T16:32:55.662553\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-01-31T16:32:55.662553\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"post_transform_anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformGraph\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transformed_examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"updated_analyzer_cache\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformCache\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"disable_statistics\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"force_tf_compat_v1\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"transformations@gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"splits_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"analyze\\\": [\\n    \\\"train\\\"\\n  ],\\n  \\\"transform\\\": [\\n    \\\"train\\\",\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"ExampleValidator\"\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "ArtifactQuery.property_predicate is not supported.\n",
      "ArtifactQuery.property_predicate is not supported.\n",
      "[DataTransformer] Resolved inputs: ({'examples': [Artifact(artifact: id: 5\n",
      "type_id: 21\n",
      "uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/TrainDataGen/examples/6\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.14.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "type: \"Examples\"\n",
      "create_time_since_epoch: 1706718823518\n",
      "last_update_time_since_epoch: 1706718823518\n",
      ", artifact_type: id: 21\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'schema': [Artifact(artifact: id: 3\n",
      "type_id: 18\n",
      "uri: \"src/tfx_model_training/raw_schema\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.14.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "type: \"Schema\"\n",
      "create_time_since_epoch: 1706718781341\n",
      "last_update_time_since_epoch: 1706718781341\n",
      ", artifact_type: id: 18\n",
      "name: \"Schema\"\n",
      ")]},)\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 10\n",
      "Going to run a new execution: ExecutionInfo(execution_id=10, input_dict={'examples': [Artifact(artifact: id: 5\n",
      "type_id: 21\n",
      "uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/TrainDataGen/examples/6\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.14.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "type: \"Examples\"\n",
      "create_time_since_epoch: 1706718823518\n",
      "last_update_time_since_epoch: 1706718823518\n",
      ", artifact_type: id: 21\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'schema': [Artifact(artifact: id: 3\n",
      "type_id: 18\n",
      "uri: \"src/tfx_model_training/raw_schema\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.14.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "type: \"Schema\"\n",
      "create_time_since_epoch: 1706718781341\n",
      "last_update_time_since_epoch: 1706718781341\n",
      ", artifact_type: id: 18\n",
      "name: \"Schema\"\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'pre_transform_stats': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/DataTransformer/pre_transform_stats/10\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'post_transform_anomalies': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/DataTransformer/post_transform_anomalies/10\"\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")], 'post_transform_stats': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/DataTransformer/post_transform_stats/10\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'transform_graph': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/DataTransformer/transform_graph/10\"\n",
      ", artifact_type: name: \"TransformGraph\"\n",
      ")], 'transformed_examples': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/DataTransformer/transformed_examples/10\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'updated_analyzer_cache': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/DataTransformer/updated_analyzer_cache/10\"\n",
      ", artifact_type: name: \"TransformCache\"\n",
      ")], 'post_transform_schema': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/DataTransformer/post_transform_schema/10\"\n",
      ", artifact_type: name: \"Schema\"\n",
      ")], 'pre_transform_schema': [Artifact(artifact: uri: \"gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/DataTransformer/pre_transform_schema/10\"\n",
      ", artifact_type: name: \"Schema\"\n",
      ")]}), exec_properties={'module_path': 'transformations@gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl', 'custom_config': 'null', 'splits_config': '{\\n  \"analyze\": [\\n    \"train\"\\n  ],\\n  \"transform\": [\\n    \"train\",\\n    \"eval\"\\n  ]\\n}', 'disable_statistics': 0, 'force_tf_compat_v1': 0}, execution_output_uri='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/DataTransformer/.system/executor_execution/10/executor_output.pb', stateful_working_dir='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/DataTransformer/.system/stateful_working_dir/2024-01-31T16:32:55.662553', tmp_dir='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/DataTransformer/.system/executor_execution/10/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.transform.component.Transform\"\n",
      "    base_type: TRANSFORM\n",
      "  }\n",
      "  id: \"DataTransformer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2024-01-31T16:32:55.662553\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-pipeline-v01-.DataTransformer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-01-31T16:32:55.662553\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2024-01-31T16:32:55.662553\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-pipeline-v01-.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"post_transform_anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformGraph\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transformed_examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"updated_analyzer_cache\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformCache\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"disable_statistics\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"force_tf_compat_v1\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"transformations@gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"splits_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"analyze\\\": [\\n    \\\"train\\\"\\n  ],\\n  \\\"transform\\\": [\\n    \\\"train\\\",\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"ExampleValidator\"\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"chicago-taxi-tips-pipeline-v01-\"\n",
      ", pipeline_run_id='2024-01-31T16:32:55.662553', top_level_pipeline_run_id=None)\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /opt/conda/lib/python3.10/site-packages/tfx to temp dir /var/tmp/tmp91nan_uh/build/tfx\n",
      "Generating a temp setup file at /var/tmp/tmp91nan_uh/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /var/tmp/tmp91nan_uh/build/tfx/setup.log\n",
      "Added --extra_package=/var/tmp/tmp91nan_uh/build/tfx/dist/tfx_ephemeral-1.14.0.tar.gz to beam args\n",
      "udf_utils.get_fn {'module_file': None, 'module_path': 'transformations@gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl', 'preprocessing_fn': None} 'preprocessing_fn'\n",
      "Installing '/var/tmp/tmp4bu84fjt/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl' to a temporary directory.\n",
      "Executing: ['/opt/conda/bin/python3.10', '-m', 'pip', 'install', '--target', '/var/tmp/tmpyvdv2hp4', '/var/tmp/tmp4bu84fjt/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl']\n",
      "Processing /var/tmp/tmp4bu84fjt/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-DataTransformer\n",
      "Successfully installed tfx-user-code-DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d\n",
      "Successfully installed '/var/tmp/tmp4bu84fjt/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl'.\n",
      "udf_utils.get_fn {'module_file': None, 'module_path': 'transformations@gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl', 'stats_options_updater_fn': None} 'stats_options_updater_fn'\n",
      "Installing '/var/tmp/tmpzda6rcqr/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl' to a temporary directory.\n",
      "Executing: ['/opt/conda/bin/python3.10', '-m', 'pip', 'install', '--target', '/var/tmp/tmpuvkhbi3h', '/var/tmp/tmpzda6rcqr/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl']\n",
      "Processing /var/tmp/tmpzda6rcqr/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-DataTransformer\n",
      "Successfully installed tfx-user-code-DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d\n",
      "Successfully installed '/var/tmp/tmpzda6rcqr/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl'.\n",
      "Installing '/var/tmp/tmpkzibv62n/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl' to a temporary directory.\n",
      "Executing: ['/opt/conda/bin/python3.10', '-m', 'pip', 'install', '--target', '/var/tmp/tmp9aey8jzj', '/var/tmp/tmpkzibv62n/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl']\n",
      "Processing /var/tmp/tmpkzibv62n/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-DataTransformer\n",
      "Successfully installed tfx-user-code-DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d\n",
      "Successfully installed '/var/tmp/tmpkzibv62n/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl'.\n",
      "Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_1/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_2/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_3/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_4/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_5/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_6/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_7/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_1/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_2/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_3/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_4/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_5/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_6/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_7/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Finished listing 1 files in 0.046331167221069336 seconds.\n",
      "Using Any for unsupported type: typing.Sequence[str]\n",
      "Finished listing 1 files in 0.05011296272277832 seconds.\n",
      "Using Any for unsupported type: typing.Sequence[str]\n",
      "Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Finished listing 1 files in 0.05797243118286133 seconds.\n",
      "Using Any for unsupported type: typing.Sequence[str]\n",
      "Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f65dd9aad40> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f65dd9aae60> ====================\n",
      "==================== <function pack_combiners at 0x7f65dd9ab370> ====================\n",
      "==================== <function lift_combiners at 0x7f65dd9ab400> ====================\n",
      "==================== <function expand_sdf at 0x7f65dd9ab5b0> ====================\n",
      "==================== <function expand_gbk at 0x7f65dd9ab640> ====================\n",
      "==================== <function sink_flattens at 0x7f65dd9ab760> ====================\n",
      "==================== <function greedily_fuse at 0x7f65dd9ab7f0> ====================\n",
      "==================== <function read_to_impulse at 0x7f65dd9ab880> ====================\n",
      "==================== <function impulse_to_input at 0x7f65dd9ab910> ====================\n",
      "==================== <function sort_stages at 0x7f65dd9abb50> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7f65dd9abc70> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f65dd9abac0> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f65dd9abbe0> ====================\n",
      "Creating state cache with size 104857600\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f65d865b1c0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Finished listing 1 files in 0.05417060852050781 seconds.\n",
      "Finished listing 1 files in 0.048604488372802734 seconds.\n",
      "Finished listing 1 files in 0.04957437515258789 seconds.\n",
      "BatchElements statistics: element_count=7 batch_count=4 next_batch_size=6 timings=[(1, 0.008206844329833984), (2, 0.007919549942016602), (3, 0.008599281311035156)]\n",
      "Finished listing 1 files in 0.04883742332458496 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.17 seconds.\n",
      "Finished listing 1 files in 0.05184674263000488 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.17 seconds.\n",
      "BatchElements statistics: element_count=3 batch_count=1 next_batch_size=150000 timings=[]\n",
      "BatchElements statistics: element_count=6 batch_count=1 next_batch_size=150000 timings=[]\n",
      "BatchElements statistics: element_count=4 batch_count=1 next_batch_size=150000 timings=[]\n",
      "BatchElements statistics: element_count=6 batch_count=1 next_batch_size=150000 timings=[]\n",
      "BatchElements statistics: element_count=3 batch_count=1 next_batch_size=150000 timings=[]\n",
      "BatchElements statistics: element_count=1 batch_count=1 next_batch_size=150000 timings=[]\n",
      "BatchElements statistics: element_count=1 batch_count=1 next_batch_size=150000 timings=[]\n",
      "BatchElements statistics: element_count=1 batch_count=1 next_batch_size=150000 timings=[]\n",
      "Finished listing 1 files in 0.05080437660217285 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.18 seconds.\n",
      "Finished listing 1 files in 0.04777646064758301 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.18 seconds.\n",
      "Finished listing 1 files in 0.047124385833740234 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.18 seconds.\n",
      "Finished listing 1 files in 0.04869508743286133 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.17 seconds.\n",
      "Finished listing 1 files in 0.04627180099487305 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.19 seconds.\n",
      "Finished listing 1 files in 0.04587697982788086 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.18 seconds.\n",
      "Finished listing 1 files in 0.04872322082519531 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.18 seconds.\n",
      "Finished listing 1 files in 0.05143141746520996 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.18 seconds.\n",
      "Finished listing 1 files in 0.05302786827087402 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.18 seconds.\n",
      "Finished listing 1 files in 0.060973167419433594 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.17 seconds.\n",
      "Finished listing 1 files in 0.06273698806762695 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.19 seconds.\n",
      "Finished listing 1 files in 0.06917786598205566 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.18 seconds.\n",
      "Finished listing 1 files in 0.05231976509094238 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.16 seconds.\n",
      "Finished listing 1 files in 0.053693294525146484 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.19 seconds.\n",
      "Finished listing 1 files in 0.052480220794677734 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.18 seconds.\n",
      "Finished listing 1 files in 0.05007290840148926 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.17 seconds.\n",
      "Finished listing 1 files in 0.04661750793457031 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.18 seconds.\n",
      "Finished listing 1 files in 0.05037069320678711 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.17 seconds.\n",
      "Finished listing 1 files in 0.04836916923522949 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.16 seconds.\n",
      "Finished listing 1 files in 0.048569679260253906 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.17 seconds.\n",
      "BatchElements statistics: element_count=3 batch_count=3 next_batch_size=1 timings=[(1, 0.0064487457275390625), (1, 0.005770683288574219)]\n",
      "BatchElements statistics: element_count=7 batch_count=4 next_batch_size=6 timings=[(1, 0.009437322616577148), (2, 0.011780500411987305), (3, 0.00920414924621582)]\n",
      "Finished listing 1 files in 0.04342174530029297 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.17 seconds.\n",
      "Finished listing 1 files in 0.0450594425201416 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.18 seconds.\n",
      "Finished listing 1 files in 0.04361152648925781 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.19 seconds.\n",
      "Finished listing 1 files in 0.05030679702758789 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.16 seconds.\n",
      "Finished listing 1 files in 0.03933095932006836 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.16 seconds.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai-steps/gs:/stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/DataTransformer/.system/stateful_working_dir is not found, not going to delete it.\n",
      "ArtifactQuery.property_predicate is not supported.\n",
      "ArtifactQuery.property_predicate is not supported.\n",
      "ArtifactQuery.property_predicate is not supported.\n",
      "ArtifactQuery.property_predicate is not supported.\n",
      "Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Processing /var/tmp/tmpqxo_2e25/tfx_user_code_ModelTrainer-0.0+b2e2657f2c58d6b17ae2108a61517fbe1be6ffa4a912f5b2d0055ab70b96bd4c-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-ModelTrainer\n",
      "Successfully installed tfx-user-code-ModelTrainer-0.0+b2e2657f2c58d6b17ae2108a61517fbe1be6ffa4a912f5b2d0055ab70b96bd4c\n",
      "Model Runner started...\n",
      "fn_args: FnArgs(working_dir=None, train_files=['gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/DataTransformer/transformed_examples/10/Split-train/*'], eval_files=['gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/DataTransformer/transformed_examples/10/Split-eval/*'], train_steps=10, eval_steps=5, schema_path='src/tfx_model_training/raw_schema/schema.pbtxt', schema_file='src/tfx_model_training/raw_schema/schema.pbtxt', transform_graph_path='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/DataTransformer/transform_graph/10', transform_output='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/DataTransformer/transform_graph/10', data_accessor=DataAccessor(tf_dataset_factory=<function get_tf_dataset_factory_from_artifact.<locals>.dataset_factory at 0x7f65d016ab90>, record_batch_factory=<function get_record_batch_factory_from_artifact.<locals>.record_batch_factory at 0x7f65d016aa70>, data_view_decode_fn=None), serving_model_dir='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ModelTrainer/model/11/Format-Serving', eval_model_dir='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ModelTrainer/model/11/Format-TFMA', model_run_dir='gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ModelTrainer/model_run/11', base_model=None, hyperparameters={'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001}, custom_config=None)\n",
      "\n",
      "Hyperparameter:\n",
      "{'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001}\n",
      "\n",
      "Model Runner executing model trainer...\n",
      "Loading tft output from gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/DataTransformer/transform_graph/10\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " trip_month_xf (InputLayer)  [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " trip_day_xf (InputLayer)    [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " trip_day_of_week_xf (Input  [(None,)]                    0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " trip_hour_xf (InputLayer)   [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " trip_seconds_xf (InputLaye  [(None,)]                    0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " trip_miles_xf (InputLayer)  [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " payment_type_xf (InputLaye  [(None,)]                    0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " pickup_grid_xf (InputLayer  [(None,)]                    0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropoff_grid_xf (InputLaye  [(None,)]                    0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " euclidean_xf (InputLayer)   [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " loc_cross_xf (InputLayer)   [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " trip_month_xf_embedding (E  (None, 2)                    8         ['trip_month_xf[0][0]']       \n",
      " mbedding)                                                                                        \n",
      "                                                                                                  \n",
      " trip_day_xf_embedding (Emb  (None, 4)                    28        ['trip_day_xf[0][0]']         \n",
      " edding)                                                                                          \n",
      "                                                                                                  \n",
      " trip_day_of_week_xf_onehot  (None, 5)                    0         ['trip_day_of_week_xf[0][0]'] \n",
      "  (CategoryEncoding)                                                                              \n",
      "                                                                                                  \n",
      " trip_hour_xf_embedding (Em  (None, 3)                    21        ['trip_hour_xf[0][0]']        \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda  (None, 1)                    0         ['trip_seconds_xf[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_1 (TFOpLamb  (None, 1)                    0         ['trip_miles_xf[0][0]']       \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " payment_type_xf_onehot (Ca  (None, 4)                    0         ['payment_type_xf[0][0]']     \n",
      " tegoryEncoding)                                                                                  \n",
      "                                                                                                  \n",
      " pickup_grid_xf_embedding (  (None, 3)                    6         ['pickup_grid_xf[0][0]']      \n",
      " Embedding)                                                                                       \n",
      "                                                                                                  \n",
      " dropoff_grid_xf_embedding   (None, 3)                    6         ['dropoff_grid_xf[0][0]']     \n",
      " (Embedding)                                                                                      \n",
      "                                                                                                  \n",
      " tf.expand_dims_2 (TFOpLamb  (None, 1)                    0         ['euclidean_xf[0][0]']        \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " loc_cross_xf_embedding (Em  (None, 10)                   20        ['loc_cross_xf[0][0]']        \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " combines_inputs (Concatena  (None, 37)                   0         ['trip_month_xf_embedding[0][0\n",
      " te)                                                                ]',                           \n",
      "                                                                     'trip_day_xf_embedding[0][0]'\n",
      "                                                                    , 'trip_day_of_week_xf_onehot[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'trip_hour_xf_embedding[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'tf.expand_dims[0][0]',      \n",
      "                                                                     'tf.expand_dims_1[0][0]',    \n",
      "                                                                     'payment_type_xf_onehot[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'pickup_grid_xf_embedding[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'dropoff_grid_xf_embedding[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'tf.expand_dims_2[0][0]',    \n",
      "                                                                     'loc_cross_xf_embedding[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " feedforward_network (Seque  (None, 32)                   4512      ['combines_inputs[0][0]']     \n",
      " ntial)                                                                                           \n",
      "                                                                                                  \n",
      " logits (Dense)              (None, 1)                    33        ['feedforward_network[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4634 (18.10 KB)\n",
      "Trainable params: 4634 (18.10 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Model training started...\n",
      "10/10 [==============================] - 135s 14s/step - loss: 0.6373 - accuracy: 0.8572 - val_loss: 0.6882 - val_accuracy: 0.6668\n",
      "Model training completed.\n",
      "Model Runner executing model evaluation...\n",
      "Loading tft output from gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/DataTransformer/transform_graph/10\n",
      "Model evaluation started...\n",
      "5/5 [==============================] - 57s 11s/step - loss: 0.6882 - accuracy: 0.6668\n",
      "Model evaluation completed.\n",
      "Model Runner executing exporter...\n",
      "Model export started...\n",
      "Model export completed.\n",
      "Model Runner completed.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai-steps/gs:/stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ModelTrainer/.system/stateful_working_dir is not found, not going to delete it.\n",
      "ArtifactQuery.property_predicate is not supported.\n",
      "ArtifactQuery.property_predicate is not supported.\n",
      "ArtifactQuery.property_predicate is not supported.\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Finished listing 1 files in 0.06383323669433594 seconds.\n",
      "Using Any for unsupported type: typing.Sequence[str]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Using Any for unsupported type: typing.Callable[[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]], typing.Dict[str, typing.Tuple[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]]\n",
      "Using Any for unsupported type: typing.Callable[[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]], typing.Dict[str, typing.Tuple[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]]\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f65dd9aad40> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f65dd9aae60> ====================\n",
      "==================== <function pack_combiners at 0x7f65dd9ab370> ====================\n",
      "==================== <function lift_combiners at 0x7f65dd9ab400> ====================\n",
      "==================== <function expand_sdf at 0x7f65dd9ab5b0> ====================\n",
      "==================== <function expand_gbk at 0x7f65dd9ab640> ====================\n",
      "==================== <function sink_flattens at 0x7f65dd9ab760> ====================\n",
      "==================== <function greedily_fuse at 0x7f65dd9ab7f0> ====================\n",
      "==================== <function read_to_impulse at 0x7f65dd9ab880> ====================\n",
      "==================== <function impulse_to_input at 0x7f65dd9ab910> ====================\n",
      "==================== <function sort_stages at 0x7f65dd9abb50> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7f65dd9abc70> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f65dd9abac0> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f65dd9abbe0> ====================\n",
      "Creating state cache with size 104857600\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f6590431b40> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Finished listing 1 files in 0.048560380935668945 seconds.\n",
      "BatchElements statistics: element_count=2 batch_count=2 next_batch_size=2 timings=[(1, 0.008414745330810547)]\n",
      "Finished listing 1 files in 0.04874992370605469 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.17 seconds.\n",
      "Finished listing 1 files in 0.04905366897583008 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.20 seconds.\n",
      "Finished listing 1 files in 0.07324957847595215 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.19 seconds.\n",
      "Finished listing 1 files in 0.06288361549377441 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.18 seconds.\n",
      "Finished listing 1 files in 0.05041694641113281 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.18 seconds.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai-steps/gs:/stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/ModelEvaluator/.system/stateful_working_dir is not found, not going to delete it.\n",
      "ArtifactQuery.property_predicate is not supported.\n",
      "ArtifactQuery.property_predicate is not supported.\n",
      "Length of label `tfx-extensions-google_cloud_ai_platform-pusher-executor-executor` exceeds maximum length(63), trimmed.\n",
      "Current python version is not the same as default of 3.7.\n",
      "Endpoint predict-explain-for-chicago-taxi-tips-pipeline-v01- already exists\n",
      "Creating Model\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/13110252891/locations/us-central1/models/5650713511559430144/operations/8709315097777930240\n",
      "Create Model backing LRO: projects/13110252891/locations/us-central1/models/5650713511559430144/operations/8709315097777930240\n",
      "Model created. Resource name: projects/13110252891/locations/us-central1/models/5650713511559430144@1\n",
      "Model created. Resource name: projects/13110252891/locations/us-central1/models/5650713511559430144@1\n",
      "To use this Model in another session:\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/13110252891/locations/us-central1/models/5650713511559430144@1')\n",
      "model = aiplatform.Model('projects/13110252891/locations/us-central1/models/5650713511559430144@1')\n",
      "Deploying model to Endpoint : projects/13110252891/locations/us-central1/endpoints/2473630682635567104\n",
      "Deploying model to Endpoint : projects/13110252891/locations/us-central1/endpoints/2473630682635567104\n",
      "Deploy Endpoint model backing LRO: projects/13110252891/locations/us-central1/endpoints/2473630682635567104/operations/8956450127329886208\n",
      "Deploy Endpoint model backing LRO: projects/13110252891/locations/us-central1/endpoints/2473630682635567104/operations/8956450127329886208\n",
      "Endpoint model deployed. Resource name: projects/13110252891/locations/us-central1/endpoints/2473630682635567104\n",
      "Endpoint model deployed. Resource name: projects/13110252891/locations/us-central1/endpoints/2473630682635567104\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai-steps/gs:/stellar-orb-408015/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-pipeline-v01-/Pusher/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Calling endpoint: <google.cloud.aiplatform.models.Endpoint object at 0x7f6566921030> \n",
      "resource name: projects/13110252891/locations/us-central1/endpoints/2473630682635567104.\n",
      "Prediction output: {'classes': ['tip<20%', 'tip>=20%'], 'scores': [7.34294808e-05, 0.999926567]}\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../../opt/conda/lib/python3.10/site-packages/apache_beam/runners/portability/stager.py:63\n",
      "  /opt/conda/lib/python3.10/site-packages/apache_beam/runners/portability/stager.py:63: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "    import pkg_resources\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: 20 warnings\n",
      "  /opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: 17 warnings\n",
      "  /opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2350\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2350\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2350\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2350\n",
      "  /opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(parent)\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871\n",
      "  /opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.logging')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871\n",
      "  /opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.iam')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871\n",
      "  /opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    declare_namespace(pkg)\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/hdfs/config.py:28\n",
      "  /opt/conda/lib/python3.10/site-packages/hdfs/config.py:28: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "    from imp import load_source\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/google/rpc/__init__.py:20\n",
      "  /opt/conda/lib/python3.10/site-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
      "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "    pkg_resources.declare_namespace(__name__)\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/jupyter_client/connect.py:27\n",
      "  /opt/conda/lib/python3.10/site-packages/jupyter_client/connect.py:27: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs\n",
      "  given by the platformdirs library.  To remove this warning and\n",
      "  see the appropriate new directories, set the environment variable\n",
      "  `JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.\n",
      "  The use of platformdirs will be the default in `jupyter_core` v6\n",
      "    from jupyter_core.paths import jupyter_data_dir\n",
      "\n",
      "../../../opt/conda/lib/python3.10/site-packages/tfx/utils/deprecation_utils.py:188\n",
      "  /opt/conda/lib/python3.10/site-packages/tfx/utils/deprecation_utils.py:188: TfxDeprecationWarning: DEFAULT_FILE_NAME will be deprecated soon\n",
      "    warnings.warn(msg, TfxDeprecationWarning)\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /opt/conda/lib/python3.10/site-packages/apache_beam/io/gcp/bigquery.py:2748: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "    temp_location = pcoll.pipeline.options.view_as(\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /opt/conda/lib/python3.10/site-packages/apache_beam/io/gcp/bigquery.py:2750: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "    job_name = pcoll.pipeline.options.view_as(GoogleCloudOptions).job_name\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /opt/conda/lib/python3.10/site-packages/apache_beam/io/gcp/bigquery.py:2774: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "    pipeline_options=pcoll.pipeline.options,\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /opt/conda/lib/python3.10/site-packages/tfx/extensions/google_cloud_ai_platform/prediction_clients.py:428: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "    logging.warn('Current python version is not the same as default of 3.7.')\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /opt/conda/lib/python3.10/site-packages/tfx/extensions/google_cloud_ai_platform/prediction_clients.py:518: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "    logging.warn('Endpoint %s already exists', endpoint_name)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================= \u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m57 warnings\u001b[0m\u001b[33m in 1193.55s (0:19:53)\u001b[0m\u001b[33m ==================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test src/tests/pipeline_deployment_tests.py::test_e2e_pipeline -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5704bcb",
   "metadata": {},
   "source": [
    "## 2. Run the training pipeline using Vertex Pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7db74",
   "metadata": {},
   "source": [
    "### Set the pipeline configurations for the Vertex AI run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86576ab2-ca46-4a45-8f75-5fbaf42f6edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tfx_pipelines import config\n",
    "os.environ[\"DATASET_DISPLAY_NAME\"] = DATASET_DISPLAY_NAME\n",
    "os.environ[\"MODEL_DISPLAY_NAME\"] = MODEL_DISPLAY_NAME\n",
    "os.environ[\"PIPELINE_NAME\"] = f'{DATASET_DISPLAY_NAME}-pipeline-v01-'\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"GCS_LOCATION\"] = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/source\"\n",
    "os.environ[\"TRAIN_LIMIT\"] = \"10\"#\"85000\"\n",
    "os.environ[\"TEST_LIMIT\"] = \"2\"#\"15000\"\n",
    "os.environ[\"BEAM_RUNNER\"] = \"DataflowRunner\"\n",
    "os.environ[\"TRAINING_RUNNER\"] = \"vertex\"\n",
    "os.environ[\"TFX_IMAGE_URI\"] = f\"gcr.io/{PROJECT}/{DATASET_DISPLAY_NAME}:03\"\n",
    "os.environ[\"ENABLE_CACHE\"] = \"1\"\n",
    "os.environ[\"ENDPOINT_NAME\"] = 'predict-explain-for-' + f'{DATASET_DISPLAY_NAME}-pipeline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d83ef31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: stellar-orb-408015\n",
      "REGION: us-central1\n",
      "GCS_LOCATION: gs://stellar-orb-408015/chicago-taxi-tips/source\n",
      "ARTIFACT_STORE_URI: gs://stellar-orb-408015/chicago-taxi-tips/source/tfx_artifacts\n",
      "MODEL_REGISTRY_URI: model_registry\n",
      "DATASET_DISPLAY_NAME: chicago-taxi-tips\n",
      "MODEL_DISPLAY_NAME: chicago-taxi-tips-classifier-v01\n",
      "PIPELINE_NAME: chicago-taxi-tips-pipeline-v01-\n",
      "PIPELINE_ROOT: gs://stellar-orb-408015/chicago-taxi-tips/source/tfx_artifacts/chicago-taxi-tips-pipeline-v01-\n",
      "PIPELINE_DEFINITION_FILE: chicago-taxi-tips-pipeline-v01-info_pipeline.json\n",
      "ML_USE_COLUMN: ml_use\n",
      "EXCLUDE_COLUMNS: trip_start_timestamp\n",
      "TRAIN_LIMIT: 10\n",
      "TEST_LIMIT: 2\n",
      "SERVE_LIMIT: 0\n",
      "NUM_TRAIN_SPLITS: 4\n",
      "NUM_EVAL_SPLITS: 1\n",
      "ACCURACY_THRESHOLD: 0.8\n",
      "USE_KFP_SA: False\n",
      "TFX_IMAGE_URI: gcr.io/stellar-orb-408015/chicago-taxi-tips:03\n",
      "BEAM_RUNNER: DataflowRunner\n",
      "BEAM_DIRECT_PIPELINE_ARGS: ['--project=stellar-orb-408015', '--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/source/temp']\n",
      "BEAM_DATAFLOW_PIPELINE_ARGS: ['--project=stellar-orb-408015', '--temp_location=gs://stellar-orb-408015/chicago-taxi-tips/source/temp', '--region=us-east1', '--runner=DataflowRunner', '--disk_size_gb=50', '--experiments=disable_worker_container_image_prepull', '--sdk_container_image=gcr.io/stellar-orb-408015/chicago-taxi-tips:03']\n",
      "TRAINING_RUNNER: vertex\n",
      "VERTEX_TRAINING_ARGS: {'project': 'stellar-orb-408015', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'gcr.io/stellar-orb-408015/chicago-taxi-tips:03'}}]}\n",
      "VERTEX_TRAINING_CONFIG: {'ai_platform_training_enable_ucaip': True, 'ai_platform_training_ucaip_region': 'us-central1', 'ai_platform_training_args': {'project': 'stellar-orb-408015', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'gcr.io/stellar-orb-408015/chicago-taxi-tips:03'}}]}, 'use_gpu': False}\n",
      "SERVING_RUNTIME: tf2-cpu.2-12\n",
      "SERVING_IMAGE_URI: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest\n",
      "ENDPOINT_NAME: predict-explain-for-chicago-taxi-tips-pipeline\n",
      "VERTEX_SERVING_SPEC_PUSHER: {'project': 'stellar-orb-408015', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'gcr.io/tfx-oss-public/tfx:1.14.0'}}], 'explanation_metadata': {'inputs': {'trip_month': {'input_tensor_name': 'trip_month'}, 'trip_day': {'input_tensor_name': 'trip_day'}, 'trip_day_of_week': {'input_tensor_name': 'trip_day_of_week'}, 'trip_hour': {'input_tensor_name': 'trip_hour'}, 'trip_seconds': {'input_tensor_name': 'trip_seconds'}, 'trip_miles': {'input_tensor_name': 'trip_miles'}, 'payment_type': {'input_tensor_name': 'payment_type'}, 'pickup_grid': {'input_tensor_name': 'pickup_grid'}, 'dropoff_grid': {'input_tensor_name': 'dropoff_grid'}, 'euclidean': {'input_tensor_name': 'euclidean'}, 'loc_cross': {'input_tensor_name': 'loc_cross'}}, 'outputs': {'scores': {'output_tensor_name': 'scores'}}}, 'explanation_parameters': {'sampled_shapley_attribution': {'path_count': 10}}}\n",
      "VERTEX_PREDICTION_SPEC: {'project': 'stellar-orb-408015', 'endpoint_name': 'predict-explain-for-chicago-taxi-tips-pipeline', 'deployed_model_display_name': 'chicago-taxi-tips-classifier-v01', 'machine_type': 'n1-standard-4', 'explanation_metadata': {'inputs': {'trip_month': {'input_tensor_name': 'trip_month'}, 'trip_day': {'input_tensor_name': 'trip_day'}, 'trip_day_of_week': {'input_tensor_name': 'trip_day_of_week'}, 'trip_hour': {'input_tensor_name': 'trip_hour'}, 'trip_seconds': {'input_tensor_name': 'trip_seconds'}, 'trip_miles': {'input_tensor_name': 'trip_miles'}, 'payment_type': {'input_tensor_name': 'payment_type'}, 'pickup_grid': {'input_tensor_name': 'pickup_grid'}, 'dropoff_grid': {'input_tensor_name': 'dropoff_grid'}, 'euclidean': {'input_tensor_name': 'euclidean'}, 'loc_cross': {'input_tensor_name': 'loc_cross'}}, 'outputs': {'scores': {'output_tensor_name': 'scores'}}}, 'explanation_parameters': {'sampled_shapley_attribution': {'path_count': 10}}}\n",
      "VERTEX_PREDICTION_CONFIG: {'ai_platform_enable_vertex': True, 'ai_platform_vertex_region': 'us-central1', 'ai_platform_vertex_container_image_uri': 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest', 'ai_platform_serving_args': {'project': 'stellar-orb-408015', 'endpoint_name': 'predict-explain-for-chicago-taxi-tips-pipeline', 'deployed_model_display_name': 'chicago-taxi-tips-classifier-v01', 'machine_type': 'n1-standard-4', 'explanation_metadata': {'inputs': {'trip_month': {'input_tensor_name': 'trip_month'}, 'trip_day': {'input_tensor_name': 'trip_day'}, 'trip_day_of_week': {'input_tensor_name': 'trip_day_of_week'}, 'trip_hour': {'input_tensor_name': 'trip_hour'}, 'trip_seconds': {'input_tensor_name': 'trip_seconds'}, 'trip_miles': {'input_tensor_name': 'trip_miles'}, 'payment_type': {'input_tensor_name': 'payment_type'}, 'pickup_grid': {'input_tensor_name': 'pickup_grid'}, 'dropoff_grid': {'input_tensor_name': 'dropoff_grid'}, 'euclidean': {'input_tensor_name': 'euclidean'}, 'loc_cross': {'input_tensor_name': 'loc_cross'}}, 'outputs': {'scores': {'output_tensor_name': 'scores'}}}, 'explanation_parameters': {'sampled_shapley_attribution': {'path_count': 10}}}}\n",
      "BATCH_PREDICTION_BQ_DATASET_NAME: playground_us\n",
      "BATCH_PREDICTION_BQ_TABLE_NAME: chicago_taxitrips_final\n",
      "BATCH_PREDICTION_BEAM_ARGS: {'runner': 'DataflowRunner', 'temporary_dir': 'gs://stellar-orb-408015/chicago-taxi-tips/source/temp', 'gcs_location': 'gs://stellar-orb-408015/chicago-taxi-tips/source/temp', 'project': 'stellar-orb-408015', 'region': 'us-central1', 'setup_file': './setup.py'}\n",
      "BATCH_PREDICTION_JOB_RESOURCES: {'machine_type': 'n1-standard-2', 'starting_replica_count': 1, 'max_replica_count': 10}\n",
      "DATASTORE_PREDICTION_KIND: chicago-taxi-tips-classifier-v01-predictions\n",
      "ENABLE_CACHE: 1\n",
      "UPLOAD_MODEL: 1\n"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f3164f",
   "metadata": {},
   "source": [
    "### Build the ML container image\n",
    "\n",
    "This is the `TFX` runtime environment for the training pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a0e729b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/stellar-orb-408015/chicago-taxi-tips:03\n"
     ]
    }
   ],
   "source": [
    "!echo $TFX_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag $TFX_IMAGE_URI . --timeout=2h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155568ca",
   "metadata": {},
   "source": [
    "### Compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c1d5ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFX Version: 1.14.0\n",
      "Tensorflow Version: 2.13.1\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying etl.py -> build/lib\n",
      "copying transformations.py -> build/lib\n",
      "installing to /var/tmp/tmpnha_c6h0\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/etl.py -> /var/tmp/tmpnha_c6h0\n",
      "copying build/lib/transformations.py -> /var/tmp/tmpnha_c6h0\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_DataTransformer.egg-info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing tfx_user_code_DataTransformer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_DataTransformer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_DataTransformer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_DataTransformer.egg-info to /var/tmp/tmpnha_c6h0/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3.10.egg-info\n",
      "running install_scripts\n",
      "creating /var/tmp/tmpnha_c6h0/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/WHEEL\n",
      "creating '/var/tmp/tmp5r8x7bnl/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl' and adding '/var/tmp/tmpnha_c6h0' to it\n",
      "adding 'etl.py'\n",
      "adding 'transformations.py'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/METADATA'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/RECORD'\n",
      "removing /var/tmp/tmpnha_c6h0\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying model_exporter.py -> build/lib\n",
      "copying features.py -> build/lib\n",
      "copying defaults.py -> build/lib\n",
      "copying model_trainer.py -> build/lib\n",
      "copying model_runner.py -> build/lib\n",
      "copying model_input.py -> build/lib\n",
      "copying model.py -> build/lib\n",
      "installing to /var/tmp/tmppzjf4ojl\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/model_input.py -> /var/tmp/tmppzjf4ojl\n",
      "copying build/lib/model_exporter.py -> /var/tmp/tmppzjf4ojl\n",
      "copying build/lib/features.py -> /var/tmp/tmppzjf4ojl\n",
      "copying build/lib/defaults.py -> /var/tmp/tmppzjf4ojl\n",
      "copying build/lib/model_trainer.py -> /var/tmp/tmppzjf4ojl\n",
      "copying build/lib/model_runner.py -> /var/tmp/tmppzjf4ojl\n",
      "copying build/lib/model.py -> /var/tmp/tmppzjf4ojl\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_ModelTrainer.egg-info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing tfx_user_code_ModelTrainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_ModelTrainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_ModelTrainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_ModelTrainer.egg-info to /var/tmp/tmppzjf4ojl/tfx_user_code_ModelTrainer-0.0+b2e2657f2c58d6b17ae2108a61517fbe1be6ffa4a912f5b2d0055ab70b96bd4c-py3.10.egg-info\n",
      "running install_scripts\n",
      "creating /var/tmp/tmppzjf4ojl/tfx_user_code_ModelTrainer-0.0+b2e2657f2c58d6b17ae2108a61517fbe1be6ffa4a912f5b2d0055ab70b96bd4c.dist-info/WHEEL\n",
      "creating '/var/tmp/tmpaexp4np7/tfx_user_code_ModelTrainer-0.0+b2e2657f2c58d6b17ae2108a61517fbe1be6ffa4a912f5b2d0055ab70b96bd4c-py3-none-any.whl' and adding '/var/tmp/tmppzjf4ojl' to it\n",
      "adding 'defaults.py'\n",
      "adding 'features.py'\n",
      "adding 'model.py'\n",
      "adding 'model_exporter.py'\n",
      "adding 'model_input.py'\n",
      "adding 'model_runner.py'\n",
      "adding 'model_trainer.py'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+b2e2657f2c58d6b17ae2108a61517fbe1be6ffa4a912f5b2d0055ab70b96bd4c.dist-info/METADATA'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+b2e2657f2c58d6b17ae2108a61517fbe1be6ffa4a912f5b2d0055ab70b96bd4c.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+b2e2657f2c58d6b17ae2108a61517fbe1be6ffa4a912f5b2d0055ab70b96bd4c.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+b2e2657f2c58d6b17ae2108a61517fbe1be6ffa4a912f5b2d0055ab70b96bd4c.dist-info/RECORD'\n",
      "removing /var/tmp/tmppzjf4ojl\n"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import runner\n",
    "\n",
    "pipeline_definition = runner.compile_training_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6644c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://chicago-taxi-tips-pipeline-v01-info_pipeline.json [Content-Type=application/json]...\n",
      "/ [1 files][ 31.0 KiB/ 31.0 KiB]                                                \n",
      "Operation completed over 1 objects/31.0 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "PIPELINES_STORE = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/compiled_pipelines/\"\n",
    "!gsutil cp {config.PIPELINE_DEFINITION_FILE} {PIPELINES_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb943e",
   "metadata": {},
   "source": [
    "### Submit run to Vertex Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62c6b0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/13110252891/locations/us-central1/pipelineJobs/chicago-taxi-tips-pipeline-v01-20240201105706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/13110252891/locations/us-central1/pipelineJobs/chicago-taxi-tips-pipeline-v01-20240201105706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/13110252891/locations/us-central1/pipelineJobs/chicago-taxi-tips-pipeline-v01-20240201105706')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/13110252891/locations/us-central1/pipelineJobs/chicago-taxi-tips-pipeline-v01-20240201105706')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/chicago-taxi-tips-pipeline-v01-20240201105706?project=13110252891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/chicago-taxi-tips-pipeline-v01-20240201105706?project=13110252891\n"
     ]
    }
   ],
   "source": [
    "runner.submit_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888be1fd",
   "metadata": {},
   "source": [
    "### Extracting pipeline runs metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37ae4aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pipeline_name</th>\n",
       "      <td>chicago-taxi-tips-pipeline-v01-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_name</th>\n",
       "      <td>chicago-taxi-tips-pipeline-v01-20240201105706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param.vmlmd_lineage_integration</th>\n",
       "      <td>{'pipeline_run_component': {'pipeline_run_id':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param.input:num_epochs</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param.input:learning_rate</th>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param.input:batch_size</th>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 0\n",
       "pipeline_name                                      chicago-taxi-tips-pipeline-v01-\n",
       "run_name                             chicago-taxi-tips-pipeline-v01-20240201105706\n",
       "param.vmlmd_lineage_integration  {'pipeline_run_component': {'pipeline_run_id':...\n",
       "param.input:num_epochs                                                          30\n",
       "param.input:learning_rate                                                    0.003\n",
       "param.input:batch_size                                                         512"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "pipeline_df = vertex_ai.get_pipeline_df(config.PIPELINE_NAME)\n",
    "pipeline_df = pipeline_df[pipeline_df.pipeline_name == config.PIPELINE_NAME]\n",
    "pipeline_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b454fe9",
   "metadata": {},
   "source": [
    "## 3. Execute the pipeline deployment CI/CD steps in Cloud Build\n",
    "\n",
    "The CI/CD routine is defined in the [pipeline-deployment.yaml](build/pipeline-deployment.yaml) file, and consists of the following steps:\n",
    "1. Clone the repository to the build environment.\n",
    "2. Run unit tests.\n",
    "3. Run a local e2e test of the pipeline.\n",
    "4. Build the ML container image for pipeline steps.\n",
    "5. Compile the pipeline.\n",
    "6. Upload the pipeline to Cloud Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29688d4d",
   "metadata": {},
   "source": [
    "### Build CI/CD container Image for Cloud Build\n",
    "\n",
    "This is the runtime environment where the steps of testing and deploying the pipeline will be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4759b85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/stellar-orb-408015/cicd:latest\n"
     ]
    }
   ],
   "source": [
    "!echo $CICD_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fc09c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 11 file(s) totalling 29.1 KiB before compression.\n",
      "Uploading tarball of [build/.] to [gs://stellar-orb-408015_cloudbuild/source/1706786804.361925-54d3a9151f3e4645bfa1c49c1781a36b.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/stellar-orb-408015/locations/global/builds/8ff3f9f2-3eae-4a7c-8258-c8099588d430].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/8ff3f9f2-3eae-4a7c-8258-c8099588d430?project=13110252891 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"8ff3f9f2-3eae-4a7c-8258-c8099588d430\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://stellar-orb-408015_cloudbuild/source/1706786804.361925-54d3a9151f3e4645bfa1c49c1781a36b.tgz#1706786804603102\n",
      "Copying gs://stellar-orb-408015_cloudbuild/source/1706786804.361925-54d3a9151f3e4645bfa1c49c1781a36b.tgz#1706786804603102...\n",
      "/ [1 files][  6.0 KiB/  6.0 KiB]                                                \n",
      "Operation completed over 1 objects/6.0 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  39.94kB\n",
      "Step 1/5 : FROM gcr.io/tfx-oss-public/tfx:1.14.0\n",
      "1.14.0: Pulling from tfx-oss-public/tfx\n",
      "6b851dcae6ca: Pulling fs layer\n",
      "4586c00479c6: Pulling fs layer\n",
      "4304fa233a80: Pulling fs layer\n",
      "afa3f70b397f: Pulling fs layer\n",
      "d963a42bc712: Pulling fs layer\n",
      "68cd1e6a2dfe: Pulling fs layer\n",
      "c4a5e6c74f13: Pulling fs layer\n",
      "afec03310895: Pulling fs layer\n",
      "44d8a5c35cf0: Pulling fs layer\n",
      "e1bab5cae66b: Pulling fs layer\n",
      "e5f5c15a6664: Pulling fs layer\n",
      "8171a8ea64b5: Pulling fs layer\n",
      "f2cce5337510: Pulling fs layer\n",
      "bf57496e69b2: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "b92b03a816a7: Pulling fs layer\n",
      "b380eaa239fb: Pulling fs layer\n",
      "efa3e4f3e5d5: Pulling fs layer\n",
      "92c73a937523: Pulling fs layer\n",
      "a3015607fa0a: Pulling fs layer\n",
      "13e720aabed6: Pulling fs layer\n",
      "afa3f70b397f: Waiting\n",
      "d963a42bc712: Waiting\n",
      "68cd1e6a2dfe: Waiting\n",
      "c4a5e6c74f13: Waiting\n",
      "afec03310895: Waiting\n",
      "44d8a5c35cf0: Waiting\n",
      "e1bab5cae66b: Waiting\n",
      "e5f5c15a6664: Waiting\n",
      "8171a8ea64b5: Waiting\n",
      "f2cce5337510: Waiting\n",
      "bf57496e69b2: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "b92b03a816a7: Waiting\n",
      "b380eaa239fb: Waiting\n",
      "efa3e4f3e5d5: Waiting\n",
      "92c73a937523: Waiting\n",
      "a3015607fa0a: Waiting\n",
      "d05f19b7557a: Pulling fs layer\n",
      "39dad3dd32fd: Pulling fs layer\n",
      "b3ce6241fc7b: Pulling fs layer\n",
      "96efdf86b632: Pulling fs layer\n",
      "d6f4a5794be5: Pulling fs layer\n",
      "345953e9c024: Pulling fs layer\n",
      "d3335c5e415f: Pulling fs layer\n",
      "c816cd7f015b: Pulling fs layer\n",
      "c6b568ddf77b: Pulling fs layer\n",
      "b38e9c0f6d23: Pulling fs layer\n",
      "7235b37060a3: Pulling fs layer\n",
      "8a92903185b5: Pulling fs layer\n",
      "c888973f4523: Pulling fs layer\n",
      "561c12469805: Pulling fs layer\n",
      "42b63e437267: Pulling fs layer\n",
      "1585c1e8c3eb: Pulling fs layer\n",
      "ad56939f5408: Pulling fs layer\n",
      "b802cf3a5fb8: Pulling fs layer\n",
      "09e1a0a2a6bd: Pulling fs layer\n",
      "485ad09fb3d1: Pulling fs layer\n",
      "198c04c08ef0: Pulling fs layer\n",
      "92ec9b9ce8df: Pulling fs layer\n",
      "cd332e212e19: Pulling fs layer\n",
      "87f7a61f7d61: Pulling fs layer\n",
      "c05c937a6822: Pulling fs layer\n",
      "0ab246240d0d: Pulling fs layer\n",
      "13e720aabed6: Waiting\n",
      "d05f19b7557a: Waiting\n",
      "39dad3dd32fd: Waiting\n",
      "b3ce6241fc7b: Waiting\n",
      "96efdf86b632: Waiting\n",
      "d6f4a5794be5: Waiting\n",
      "345953e9c024: Waiting\n",
      "d3335c5e415f: Waiting\n",
      "c816cd7f015b: Waiting\n",
      "c6b568ddf77b: Waiting\n",
      "b38e9c0f6d23: Waiting\n",
      "7235b37060a3: Waiting\n",
      "8a92903185b5: Waiting\n",
      "c888973f4523: Waiting\n",
      "561c12469805: Waiting\n",
      "42b63e437267: Waiting\n",
      "1585c1e8c3eb: Waiting\n",
      "ad56939f5408: Waiting\n",
      "b802cf3a5fb8: Waiting\n",
      "09e1a0a2a6bd: Waiting\n",
      "485ad09fb3d1: Waiting\n",
      "198c04c08ef0: Waiting\n",
      "92ec9b9ce8df: Waiting\n",
      "cd332e212e19: Waiting\n",
      "87f7a61f7d61: Waiting\n",
      "c05c937a6822: Waiting\n",
      "0ab246240d0d: Waiting\n",
      "4586c00479c6: Verifying Checksum\n",
      "4586c00479c6: Download complete\n",
      "afa3f70b397f: Verifying Checksum\n",
      "afa3f70b397f: Download complete\n",
      "6b851dcae6ca: Verifying Checksum\n",
      "6b851dcae6ca: Download complete\n",
      "d963a42bc712: Verifying Checksum\n",
      "d963a42bc712: Download complete\n",
      "c4a5e6c74f13: Verifying Checksum\n",
      "c4a5e6c74f13: Download complete\n",
      "afec03310895: Verifying Checksum\n",
      "afec03310895: Download complete\n",
      "4304fa233a80: Verifying Checksum\n",
      "4304fa233a80: Download complete\n",
      "44d8a5c35cf0: Verifying Checksum\n",
      "44d8a5c35cf0: Download complete\n",
      "e5f5c15a6664: Verifying Checksum\n",
      "e5f5c15a6664: Download complete\n",
      "6b851dcae6ca: Pull complete\n",
      "4586c00479c6: Pull complete\n",
      "68cd1e6a2dfe: Verifying Checksum\n",
      "68cd1e6a2dfe: Download complete\n",
      "f2cce5337510: Download complete\n",
      "8171a8ea64b5: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "bf57496e69b2: Verifying Checksum\n",
      "bf57496e69b2: Download complete\n",
      "4304fa233a80: Pull complete\n",
      "afa3f70b397f: Pull complete\n",
      "d963a42bc712: Pull complete\n",
      "b380eaa239fb: Verifying Checksum\n",
      "b380eaa239fb: Download complete\n",
      "efa3e4f3e5d5: Verifying Checksum\n",
      "efa3e4f3e5d5: Download complete\n",
      "92c73a937523: Verifying Checksum\n",
      "92c73a937523: Download complete\n",
      "b92b03a816a7: Verifying Checksum\n",
      "b92b03a816a7: Download complete\n",
      "a3015607fa0a: Verifying Checksum\n",
      "a3015607fa0a: Download complete\n",
      "13e720aabed6: Verifying Checksum\n",
      "13e720aabed6: Download complete\n",
      "d05f19b7557a: Verifying Checksum\n",
      "d05f19b7557a: Download complete\n",
      "b3ce6241fc7b: Verifying Checksum\n",
      "b3ce6241fc7b: Download complete\n",
      "96efdf86b632: Verifying Checksum\n",
      "96efdf86b632: Download complete\n",
      "d6f4a5794be5: Verifying Checksum\n",
      "d6f4a5794be5: Download complete\n",
      "345953e9c024: Verifying Checksum\n",
      "345953e9c024: Download complete\n",
      "d3335c5e415f: Verifying Checksum\n",
      "d3335c5e415f: Download complete\n",
      "c816cd7f015b: Verifying Checksum\n",
      "c816cd7f015b: Download complete\n",
      "c6b568ddf77b: Verifying Checksum\n",
      "c6b568ddf77b: Download complete\n",
      "b38e9c0f6d23: Verifying Checksum\n",
      "b38e9c0f6d23: Download complete\n",
      "7235b37060a3: Verifying Checksum\n",
      "7235b37060a3: Download complete\n",
      "8a92903185b5: Verifying Checksum\n",
      "8a92903185b5: Download complete\n",
      "c888973f4523: Verifying Checksum\n",
      "c888973f4523: Download complete\n",
      "39dad3dd32fd: Verifying Checksum\n",
      "39dad3dd32fd: Download complete\n",
      "42b63e437267: Verifying Checksum\n",
      "42b63e437267: Download complete\n",
      "e1bab5cae66b: Verifying Checksum\n",
      "e1bab5cae66b: Download complete\n",
      "ad56939f5408: Download complete\n",
      "b802cf3a5fb8: Verifying Checksum\n",
      "b802cf3a5fb8: Download complete\n",
      "09e1a0a2a6bd: Verifying Checksum\n",
      "09e1a0a2a6bd: Download complete\n",
      "561c12469805: Verifying Checksum\n",
      "561c12469805: Download complete\n",
      "198c04c08ef0: Verifying Checksum\n",
      "198c04c08ef0: Download complete\n",
      "92ec9b9ce8df: Verifying Checksum\n",
      "92ec9b9ce8df: Download complete\n",
      "cd332e212e19: Verifying Checksum\n",
      "cd332e212e19: Download complete\n",
      "87f7a61f7d61: Verifying Checksum\n",
      "87f7a61f7d61: Download complete\n",
      "c05c937a6822: Verifying Checksum\n",
      "c05c937a6822: Download complete\n",
      "485ad09fb3d1: Verifying Checksum\n",
      "485ad09fb3d1: Download complete\n",
      "1585c1e8c3eb: Verifying Checksum\n",
      "1585c1e8c3eb: Download complete\n",
      "0ab246240d0d: Verifying Checksum\n",
      "0ab246240d0d: Download complete\n",
      "68cd1e6a2dfe: Pull complete\n",
      "c4a5e6c74f13: Pull complete\n",
      "afec03310895: Pull complete\n",
      "44d8a5c35cf0: Pull complete\n",
      "e1bab5cae66b: Pull complete\n",
      "e5f5c15a6664: Pull complete\n",
      "8171a8ea64b5: Pull complete\n",
      "f2cce5337510: Pull complete\n",
      "bf57496e69b2: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "b92b03a816a7: Pull complete\n",
      "b380eaa239fb: Pull complete\n",
      "efa3e4f3e5d5: Pull complete\n",
      "92c73a937523: Pull complete\n",
      "a3015607fa0a: Pull complete\n",
      "13e720aabed6: Pull complete\n",
      "d05f19b7557a: Pull complete\n",
      "39dad3dd32fd: Pull complete\n",
      "b3ce6241fc7b: Pull complete\n",
      "96efdf86b632: Pull complete\n",
      "d6f4a5794be5: Pull complete\n",
      "345953e9c024: Pull complete\n",
      "d3335c5e415f: Pull complete\n",
      "c816cd7f015b: Pull complete\n",
      "c6b568ddf77b: Pull complete\n",
      "b38e9c0f6d23: Pull complete\n",
      "7235b37060a3: Pull complete\n",
      "8a92903185b5: Pull complete\n",
      "c888973f4523: Pull complete\n",
      "561c12469805: Pull complete\n",
      "42b63e437267: Pull complete\n",
      "1585c1e8c3eb: Pull complete\n",
      "ad56939f5408: Pull complete\n",
      "b802cf3a5fb8: Pull complete\n",
      "09e1a0a2a6bd: Pull complete\n",
      "485ad09fb3d1: Pull complete\n",
      "198c04c08ef0: Pull complete\n",
      "92ec9b9ce8df: Pull complete\n",
      "cd332e212e19: Pull complete\n",
      "87f7a61f7d61: Pull complete\n",
      "c05c937a6822: Pull complete\n",
      "0ab246240d0d: Pull complete\n",
      "Digest: sha256:c4d0a110f09c20800f0edbcc0864763cb3d87a8f7ee4129aa8d6c2edaae00fae\n",
      "Status: Downloaded newer image for gcr.io/tfx-oss-public/tfx:1.14.0\n",
      " ---> ded9ed31f6fd\n",
      "Step 2/5 : ENV RUN_PYTHON_SDK_IN_DEFAULT_ENVIRONMENT=1\n",
      " ---> Running in 9bc1d1c4c316\n",
      "Removing intermediate container 9bc1d1c4c316\n",
      " ---> 9ef6483eeec3\n",
      "Step 3/5 : COPY requirements.txt requirements.txt\n",
      " ---> 508473dd353b\n",
      "Step 4/5 : RUN sed -i 's/python3/python/g' /usr/bin/pip\n",
      " ---> Running in 0fed327e470a\n",
      "Removing intermediate container 0fed327e470a\n",
      " ---> b5dfebcb9d27\n",
      "Step 5/5 : RUN pip install -r requirements.txt\n",
      " ---> Running in 49f141428122\n",
      "Requirement already satisfied: absl-py==1.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (1.4.0)\n",
      "Collecting anyio==4.2.0 (from -r requirements.txt (line 2))\n",
      "  Obtaining dependency information for anyio==4.2.0 from https://files.pythonhosted.org/packages/bf/cd/d6d9bb1dadf73e7af02d18225cbd2c93f8552e13130484f1c8dcfece292b/anyio-4.2.0-py3-none-any.whl.metadata\n",
      "  Downloading anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: apache-beam==2.50.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (2.50.0)\n",
      "Requirement already satisfied: argon2-cffi==23.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (23.1.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings==21.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (21.2.0)\n",
      "Collecting arrow==1.3.0 (from -r requirements.txt (line 6))\n",
      "  Obtaining dependency information for arrow==1.3.0 from https://files.pythonhosted.org/packages/f8/ed/e97229a566617f2ae958a6b13e7cc0f585470eac730a73e9e82c32a3cdd2/arrow-1.3.0-py3-none-any.whl.metadata\n",
      "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (1.6.3)\n",
      "Requirement already satisfied: attrs==21.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (21.4.0)\n",
      "Requirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (0.2.0)\n",
      "Collecting beautifulsoup4==4.12.3 (from -r requirements.txt (line 10))\n",
      "  Obtaining dependency information for beautifulsoup4==4.12.3 from https://files.pythonhosted.org/packages/b1/fe/e8c672695b37eecc5cbf43e1d0638d88d66ba3a44c4d321c796f4e59167f/beautifulsoup4-4.12.3-py3-none-any.whl.metadata\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bleach==6.1.0 (from -r requirements.txt (line 11))\n",
      "  Obtaining dependency information for bleach==6.1.0 from https://files.pythonhosted.org/packages/ea/63/da7237f805089ecc28a3f36bca6a21c31fcbc2eb380f3b8f1be3312abd14/bleach-6.1.0-py3-none-any.whl.metadata\n",
      "  Downloading bleach-6.1.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting cachetools==5.3.2 (from -r requirements.txt (line 12))\n",
      "  Obtaining dependency information for cachetools==5.3.2 from https://files.pythonhosted.org/packages/a2/91/2d843adb9fbd911e0da45fbf6f18ca89d07a087c3daa23e955584f90ebf4/cachetools-5.3.2-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting cffi==1.16.0 (from -r requirements.txt (line 13))\n",
      "  Obtaining dependency information for cffi==1.16.0 from https://files.pythonhosted.org/packages/f1/c9/326611aa83e16b13b6db4dbb73b5455c668159a003c4c2f0c3bcb2ddabaf/cffi-1.16.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading cffi-1.16.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting charset-normalizer==3.3.2 (from -r requirements.txt (line 14))\n",
      "  Obtaining dependency information for charset-normalizer==3.3.2 from https://files.pythonhosted.org/packages/3d/09/d82fe4a34c5f0585f9ea1df090e2a71eb9bb1e469723053e1ee9f57c16f3/charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: click==8.1.7 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 15)) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 16)) (2.2.1)\n",
      "Collecting comm==0.2.1 (from -r requirements.txt (line 17))\n",
      "  Obtaining dependency information for comm==0.2.1 from https://files.pythonhosted.org/packages/6e/c1/e7335bd49aa3fa3bd453e34a4580b0076804f219897ad76d4d5aa4d8f22f/comm-0.2.1-py3-none-any.whl.metadata\n",
      "  Downloading comm-0.2.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: crcmod==1.7 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 18)) (1.7)\n",
      "Collecting debugpy==1.8.0 (from -r requirements.txt (line 19))\n",
      "  Obtaining dependency information for debugpy==1.8.0 from https://files.pythonhosted.org/packages/89/37/aea8b74a35f2e963814c2d672fbc6b487457a14f5f1f561d9b0a270e8f28/debugpy-1.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading debugpy-1.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: decorator==5.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 20)) (5.1.1)\n",
      "Requirement already satisfied: defusedxml==0.7.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 21)) (0.7.1)\n",
      "Requirement already satisfied: dill==0.3.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 22)) (0.3.1.1)\n",
      "Requirement already satisfied: dm-tree==0.1.8 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 23)) (0.1.8)\n",
      "Collecting dnspython==2.5.0 (from -r requirements.txt (line 24))\n",
      "  Obtaining dependency information for dnspython==2.5.0 from https://files.pythonhosted.org/packages/b6/83/4a684a63d395007670bc95c1947c07045fe66141574e2f7e9e347df8499a/dnspython-2.5.0-py3-none-any.whl.metadata\n",
      "  Downloading dnspython-2.5.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: docker==4.4.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 25)) (4.4.4)\n",
      "Requirement already satisfied: docopt==0.6.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 26)) (0.6.2)\n",
      "Collecting docstring-parser==0.15 (from -r requirements.txt (line 27))\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: entrypoints==0.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 28)) (0.4)\n",
      "Collecting exceptiongroup==1.2.0 (from -r requirements.txt (line 29))\n",
      "  Obtaining dependency information for exceptiongroup==1.2.0 from https://files.pythonhosted.org/packages/b8/9a/5028fd52db10e600f1c4674441b968cf2ea4959085bfb5b99fb1250e5f68/exceptiongroup-1.2.0-py3-none-any.whl.metadata\n",
      "  Downloading exceptiongroup-1.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting fastavro==1.9.3 (from -r requirements.txt (line 30))\n",
      "  Obtaining dependency information for fastavro==1.9.3 from https://files.pythonhosted.org/packages/ba/09/ae347323cc0bfae01505970e0a9b41ef4b81e503fe6387ae6665ea33be49/fastavro-1.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading fastavro-1.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting fasteners==0.19 (from -r requirements.txt (line 31))\n",
      "  Obtaining dependency information for fasteners==0.19 from https://files.pythonhosted.org/packages/61/bf/fd60001b3abc5222d8eaa4a204cd8c0ae78e75adc688f33ce4bf25b7fafa/fasteners-0.19-py3-none-any.whl.metadata\n",
      "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting fastjsonschema==2.19.1 (from -r requirements.txt (line 32))\n",
      "  Obtaining dependency information for fastjsonschema==2.19.1 from https://files.pythonhosted.org/packages/9c/b9/79691036d4a8f9857e74d1728b23f34f583b81350a27492edda58d5604e1/fastjsonschema-2.19.1-py3-none-any.whl.metadata\n",
      "  Downloading fastjsonschema-2.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fire==0.5.0 (from -r requirements.txt (line 33))\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 1.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: flatbuffers==23.5.26 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 34)) (23.5.26)\n",
      "Requirement already satisfied: fqdn==1.5.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 35)) (1.5.1)\n",
      "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 36)) (0.4.0)\n",
      "Collecting google-api-core==2.15.0 (from -r requirements.txt (line 37))\n",
      "  Obtaining dependency information for google-api-core==2.15.0 from https://files.pythonhosted.org/packages/d6/c9/0462f037b62796fbda4801be62d0ae3147eaeb99e2939661580c98abe3eb/google_api_core-2.15.0-py3-none-any.whl.metadata\n",
      "  Downloading google_api_core-2.15.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: google-api-python-client==1.12.11 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 38)) (1.12.11)\n",
      "Requirement already satisfied: google-apitools==0.5.31 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 39)) (0.5.31)\n",
      "Collecting google-auth==2.27.0 (from -r requirements.txt (line 40))\n",
      "  Obtaining dependency information for google-auth==2.27.0 from https://files.pythonhosted.org/packages/82/41/7fb855444cead5b2213e053447ce3a0b7bf2c3529c443e0cf75b2f13b405/google_auth-2.27.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.27.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-httplib2==0.1.1 (from -r requirements.txt (line 41))\n",
      "  Obtaining dependency information for google-auth-httplib2==0.1.1 from https://files.pythonhosted.org/packages/d3/3d/e4991229886c0d522d9552151a43ff7adcc61e026e60ce8bd508387f84cf/google_auth_httplib2-0.1.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth_httplib2-0.1.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: google-auth-oauthlib==1.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 42)) (1.0.0)\n",
      "Collecting google-cloud-aiplatform==1.34.0 (from -r requirements.txt (line 43))\n",
      "  Obtaining dependency information for google-cloud-aiplatform==1.34.0 from https://files.pythonhosted.org/packages/20/ee/af1e3c9cb2bed3aa91297f8362da3879a793d2555c2e587a9c3d2f1ed900/google_cloud_aiplatform-1.34.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_aiplatform-1.34.0-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: google-cloud-bigquery==2.34.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 44)) (2.34.4)\n",
      "Collecting google-cloud-bigquery-storage==2.24.0 (from -r requirements.txt (line 45))\n",
      "  Obtaining dependency information for google-cloud-bigquery-storage==2.24.0 from https://files.pythonhosted.org/packages/75/93/a4192dd34b42ab31c8411810db896deca31c48f845807a733602ac38d849/google_cloud_bigquery_storage-2.24.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_bigquery_storage-2.24.0-py2.py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: google-cloud-bigtable==2.21.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 46)) (2.21.0)\n",
      "Collecting google-cloud-core==2.4.1 (from -r requirements.txt (line 47))\n",
      "  Obtaining dependency information for google-cloud-core==2.4.1 from https://files.pythonhosted.org/packages/5e/0f/2e2061e3fbcb9d535d5da3f58cc8de4947df1786fe6a1355960feb05a681/google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-cloud-datastore==2.19.0 (from -r requirements.txt (line 48))\n",
      "  Obtaining dependency information for google-cloud-datastore==2.19.0 from https://files.pythonhosted.org/packages/a3/45/bd74899a0ed936118b60661c0bd983279e5d9c7307996e03b9501fc1a78b/google_cloud_datastore-2.19.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_datastore-2.19.0-py2.py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting google-cloud-dlp==3.12.3 (from -r requirements.txt (line 49))\n",
      "  Obtaining dependency information for google-cloud-dlp==3.12.3 from https://files.pythonhosted.org/packages/9a/2c/5a62598d1696246c056e512ac69983e0d736cd2cd2ac1107075743a4ec48/google_cloud_dlp-3.12.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_dlp-3.12.3-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting google-cloud-language==2.12.0 (from -r requirements.txt (line 50))\n",
      "  Obtaining dependency information for google-cloud-language==2.12.0 from https://files.pythonhosted.org/packages/fe/e6/4bbe75934e6fb507043273970d3bfe034f342282e1832b63d14c3567fbf0/google_cloud_language-2.12.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_language-2.12.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting google-cloud-pubsub==2.18.4 (from -r requirements.txt (line 51))\n",
      "  Obtaining dependency information for google-cloud-pubsub==2.18.4 from https://files.pythonhosted.org/packages/d2/58/fbdedd07206514f09f029458d95d391829b09399c1459d8892e6e20022d1/google_cloud_pubsub-2.18.4-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_pubsub-2.18.4-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: google-cloud-pubsublite==1.8.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 52)) (1.8.3)\n",
      "Collecting google-cloud-recommendations-ai==0.10.5 (from -r requirements.txt (line 53))\n",
      "  Obtaining dependency information for google-cloud-recommendations-ai==0.10.5 from https://files.pythonhosted.org/packages/f9/7f/c6554fcbc09a861e11214df359c1d048791c8a2d6b8909be023ba5fe0997/google_cloud_recommendations_ai-0.10.5-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_recommendations_ai-0.10.5-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting google-cloud-resource-manager==1.10.4 (from -r requirements.txt (line 54))\n",
      "  Obtaining dependency information for google-cloud-resource-manager==1.10.4 from https://files.pythonhosted.org/packages/8b/9c/6807473e69fddc9bf33413b7db966fbcfeb0deade2f5ed324cef2b98ec16/google_cloud_resource_manager-1.10.4-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_resource_manager-1.10.4-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: google-cloud-spanner==3.40.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 55)) (3.40.1)\n",
      "Collecting google-cloud-storage==2.14.0 (from -r requirements.txt (line 56))\n",
      "  Obtaining dependency information for google-cloud-storage==2.14.0 from https://files.pythonhosted.org/packages/3d/48/574463fbf30c7021341ab0620e56103a8c49ad864bdd177935306c057986/google_cloud_storage-2.14.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_storage-2.14.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting google-cloud-videointelligence==2.11.4 (from -r requirements.txt (line 57))\n",
      "  Obtaining dependency information for google-cloud-videointelligence==2.11.4 from https://files.pythonhosted.org/packages/ca/0e/ce99f6c36d5e7ccbc5827d714a947da5b864a30e060f2e5dddad4166f663/google_cloud_videointelligence-2.11.4-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_videointelligence-2.11.4-py2.py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: google-cloud-vision==3.4.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 58)) (3.4.4)\n",
      "Requirement already satisfied: google-crc32c==1.5.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 59)) (1.5.0)\n",
      "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 60)) (0.2.0)\n",
      "Collecting google-resumable-media==2.7.0 (from -r requirements.txt (line 61))\n",
      "  Obtaining dependency information for google-resumable-media==2.7.0 from https://files.pythonhosted.org/packages/b2/c6/1202ef64a9336d846f713107dac1c7a0b016cb3840ca3d5615c7005a23d1/google_resumable_media-2.7.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_resumable_media-2.7.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting googleapis-common-protos==1.62.0 (from -r requirements.txt (line 62))\n",
      "  Obtaining dependency information for googleapis-common-protos==1.62.0 from https://files.pythonhosted.org/packages/f0/43/c9d8f75ddf08e2a0a27db243c13a700c3cc7ec615b545b697cf6f715ad92/googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting grpc-google-iam-v1==0.13.0 (from -r requirements.txt (line 63))\n",
      "  Obtaining dependency information for grpc-google-iam-v1==0.13.0 from https://files.pythonhosted.org/packages/66/a0/d27ec874fb0a86b3609b73161a15cf633924888afa05c1673b3ab5a6c3f4/grpc_google_iam_v1-0.13.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading grpc_google_iam_v1-0.13.0-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting grpcio==1.60.0 (from -r requirements.txt (line 64))\n",
      "  Obtaining dependency information for grpcio==1.60.0 from https://files.pythonhosted.org/packages/df/f3/f158fa88b8149369f9ef2e8c40de125f6d0923cc248a63780380de7cd9a3/grpcio-1.60.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading grpcio-1.60.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: grpcio-status==1.48.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 65)) (1.48.2)\n",
      "Collecting h5py==3.10.0 (from -r requirements.txt (line 66))\n",
      "  Obtaining dependency information for h5py==3.10.0 from https://files.pythonhosted.org/packages/6b/31/b5965f76e0bb2b02f273d87ec9cb59c77b9864ac27a0078c4229baa45dfc/h5py-3.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading h5py-3.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: hdfs==2.7.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 67)) (2.7.2)\n",
      "Collecting httplib2==0.22.0 (from -r requirements.txt (line 68))\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.9/96.9 kB 7.1 MB/s eta 0:00:00\n",
      "Collecting idna==3.6 (from -r requirements.txt (line 69))\n",
      "  Obtaining dependency information for idna==3.6 from https://files.pythonhosted.org/packages/c2/e7/a82b05cf63a603df6e68d59ae6a68bf5064484a0718ea5033660af4b54a9/idna-3.6-py3-none-any.whl.metadata\n",
      "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting ipykernel==6.29.0 (from -r requirements.txt (line 70))\n",
      "  Obtaining dependency information for ipykernel==6.29.0 from https://files.pythonhosted.org/packages/5a/65/6eb5e3efc439dd552b6e0bb88feb350b9c3b226c36773d2348b88dccd7f7/ipykernel-6.29.0-py3-none-any.whl.metadata\n",
      "  Downloading ipykernel-6.29.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: ipython==7.34.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 71)) (7.34.0)\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 72)) (0.2.0)\n",
      "Collecting ipywidgets==7.8.1 (from -r requirements.txt (line 73))\n",
      "  Obtaining dependency information for ipywidgets==7.8.1 from https://files.pythonhosted.org/packages/14/3f/fa7fcf85061819f5a10ed09eaef38fe97d0f3f91d14674bbb26c3fc2a622/ipywidgets-7.8.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading ipywidgets-7.8.1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: isoduration==20.11.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 74)) (20.11.0)\n",
      "Collecting jedi==0.19.1 (from -r requirements.txt (line 75))\n",
      "  Obtaining dependency information for jedi==0.19.1 from https://files.pythonhosted.org/packages/20/9f/bc63f0f0737ad7a60800bfd472a4836661adae21f9c2535f3957b1e54ceb/jedi-0.19.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jinja2==3.1.3 (from -r requirements.txt (line 76))\n",
      "  Obtaining dependency information for jinja2==3.1.3 from https://files.pythonhosted.org/packages/30/6d/6de6be2d02603ab56e72997708809e8a5b0fbfee080735109b40a3564843/Jinja2-3.1.3-py3-none-any.whl.metadata\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: joblib==1.3.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 77)) (1.3.2)\n",
      "Requirement already satisfied: jsonpointer==2.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 78)) (2.4)\n",
      "Requirement already satisfied: jsonschema==4.17.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 79)) (4.17.3)\n",
      "Requirement already satisfied: jupyter-client==7.4.9 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 80)) (7.4.9)\n",
      "Collecting jupyter-core==5.7.1 (from -r requirements.txt (line 81))\n",
      "  Obtaining dependency information for jupyter-core==5.7.1 from https://files.pythonhosted.org/packages/86/a1/354cade6907f2fbbd32d89872ec64b62406028e7645ac13acfdb5732829e/jupyter_core-5.7.1-py3-none-any.whl.metadata\n",
      "  Downloading jupyter_core-5.7.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: jupyter-events==0.6.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 82)) (0.6.3)\n",
      "Collecting jupyter-server==2.10.0 (from -r requirements.txt (line 83))\n",
      "  Obtaining dependency information for jupyter-server==2.10.0 from https://files.pythonhosted.org/packages/e6/42/d200e2aaed1ce8f755b499aea7f83632760654ab75dae07f9038594c7418/jupyter_server-2.10.0-py3-none-any.whl.metadata\n",
      "  Downloading jupyter_server-2.10.0-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting jupyter-server-terminals==0.5.2 (from -r requirements.txt (line 84))\n",
      "  Obtaining dependency information for jupyter-server-terminals==0.5.2 from https://files.pythonhosted.org/packages/7c/ec/ebb52454525e1d346bfa2ea91b3dcda3b92687bb73b2c25a6d621d9eeaf1/jupyter_server_terminals-0.5.2-py3-none-any.whl.metadata\n",
      "  Downloading jupyter_server_terminals-0.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting jupyterlab-pygments==0.3.0 (from -r requirements.txt (line 85))\n",
      "  Obtaining dependency information for jupyterlab-pygments==0.3.0 from https://files.pythonhosted.org/packages/b1/dd/ead9d8ea85bf202d90cc513b533f9c363121c7792674f78e0d8a854b63b4/jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata\n",
      "  Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting jupyterlab-widgets==1.1.7 (from -r requirements.txt (line 86))\n",
      "  Obtaining dependency information for jupyterlab-widgets==1.1.7 from https://files.pythonhosted.org/packages/06/c9/50a16b6e7410d661ea16160f8c650c444ab83740b437b3c202ca7d8e2b73/jupyterlab_widgets-1.1.7-py3-none-any.whl.metadata\n",
      "  Downloading jupyterlab_widgets-1.1.7-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: keras==2.13.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 87)) (2.13.1)\n",
      "Collecting keras-tuner==1.4.4 (from -r requirements.txt (line 88))\n",
      "  Obtaining dependency information for keras-tuner==1.4.4 from https://files.pythonhosted.org/packages/b3/fb/35dab32ffc45faefcb6762088a1c105e421fbd253eb3e86002b70916e6f3/keras_tuner-1.4.4-py3-none-any.whl.metadata\n",
      "  Downloading keras_tuner-1.4.4-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting kfp==1.8.22 (from -r requirements.txt (line 89))\n",
      "  Downloading kfp-1.8.22.tar.gz (304 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 304.9/304.9 kB 5.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.1.16 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 90)) (0.1.16)\n",
      "Collecting kfp-server-api==1.8.5 (from -r requirements.txt (line 91))\n",
      "  Downloading kfp-server-api-1.8.5.tar.gz (58 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.1/58.1 kB 5.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: kt-legacy==1.0.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 92)) (1.0.5)\n",
      "Requirement already satisfied: kubernetes==12.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 93)) (12.0.1)\n",
      "Requirement already satisfied: libclang==16.0.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 94)) (16.0.6)\n",
      "Collecting markdown==3.5.2 (from -r requirements.txt (line 95))\n",
      "  Obtaining dependency information for markdown==3.5.2 from https://files.pythonhosted.org/packages/42/f4/f0031854de10a0bc7821ef9fca0b92ca0d7aa6fbfbf504c5473ba825e49c/Markdown-3.5.2-py3-none-any.whl.metadata\n",
      "  Downloading Markdown-3.5.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting markdown-it-py==3.0.0 (from -r requirements.txt (line 96))\n",
      "  Obtaining dependency information for markdown-it-py==3.0.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 97)) (0.1.6)\n",
      "Collecting mdurl==0.1.2 (from -r requirements.txt (line 98))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting mistune==3.0.2 (from -r requirements.txt (line 99))\n",
      "  Obtaining dependency information for mistune==3.0.2 from https://files.pythonhosted.org/packages/f0/74/c95adcdf032956d9ef6c89a9b8a5152bf73915f8c633f3e3d88d06bd699c/mistune-3.0.2-py3-none-any.whl.metadata\n",
      "  Downloading mistune-3.0.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: ml-metadata==1.14.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 100)) (1.14.0)\n",
      "Requirement already satisfied: ml-pipelines-sdk==1.14.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 101)) (1.14.0)\n",
      "Collecting namex==0.0.7 (from -r requirements.txt (line 102))\n",
      "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: nbclassic==1.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 103)) (1.0.0)\n",
      "Collecting nbclient==0.9.0 (from -r requirements.txt (line 104))\n",
      "  Obtaining dependency information for nbclient==0.9.0 from https://files.pythonhosted.org/packages/6b/3a/607149974149f847125c38a62b9ea2b8267eb74823bbf8d8c54ae0212a00/nbclient-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading nbclient-0.9.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting nbconvert==7.14.2 (from -r requirements.txt (line 105))\n",
      "  Obtaining dependency information for nbconvert==7.14.2 from https://files.pythonhosted.org/packages/f4/50/275525adbd3dcef9aee708b97f146a094c4f7f24c15c668a6e7cb4120181/nbconvert-7.14.2-py3-none-any.whl.metadata\n",
      "  Downloading nbconvert-7.14.2-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: nbformat==5.9.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 106)) (5.9.2)\n",
      "Collecting nest-asyncio==1.6.0 (from -r requirements.txt (line 107))\n",
      "  Obtaining dependency information for nest-asyncio==1.6.0 from https://files.pythonhosted.org/packages/a0/c4/c2971a3ba4c6103a3d10c4b0f24f461ddc027f0f09763220cf35ca1401b3/nest_asyncio-1.6.0-py3-none-any.whl.metadata\n",
      "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting notebook==6.5.6 (from -r requirements.txt (line 108))\n",
      "  Obtaining dependency information for notebook==6.5.6 from https://files.pythonhosted.org/packages/f2/b0/24277b431a62f50e85ae5f7cad478419ed9645f4ba1dd035f2ce11326777/notebook-6.5.6-py3-none-any.whl.metadata\n",
      "  Downloading notebook-6.5.6-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: notebook-shim==0.2.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 109)) (0.2.3)\n",
      "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 110)) (1.24.3)\n",
      "Requirement already satisfied: oauth2client==4.1.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 111)) (4.1.3)\n",
      "Collecting oauthlib==3.2.2 (from -r requirements.txt (line 112))\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 10.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: objsize==0.6.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 113)) (0.6.1)\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 114)) (3.3.0)\n",
      "Collecting orjson==3.9.12 (from -r requirements.txt (line 115))\n",
      "  Obtaining dependency information for orjson==3.9.12 from https://files.pythonhosted.org/packages/b2/fa/bbed27b635949be48e60e08e88a53090edcefb0ab45b9d379ef05f8e3628/orjson-3.9.12-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading orjson-3.9.12-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.4/49.4 kB 3.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: overrides==6.5.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 116)) (6.5.0)\n",
      "Requirement already satisfied: packaging==20.9 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 117)) (20.9)\n",
      "Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 118)) (1.5.3)\n",
      "Collecting pandocfilters==1.5.1 (from -r requirements.txt (line 119))\n",
      "  Obtaining dependency information for pandocfilters==1.5.1 from https://files.pythonhosted.org/packages/ef/af/4fbc8cab944db5d21b7e2a5b8e9211a03a79852b1157e2c102fcc61ac440/pandocfilters-1.5.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: parso==0.8.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 120)) (0.8.3)\n",
      "Collecting pexpect==4.9.0 (from -r requirements.txt (line 121))\n",
      "  Obtaining dependency information for pexpect==4.9.0 from https://files.pythonhosted.org/packages/9e/c3/059298687310d527a58bb01f3b1965787ee3b40dce76752eda8b44e9a2c5/pexpect-4.9.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 122)) (0.7.5)\n",
      "Collecting pillow==10.2.0 (from -r requirements.txt (line 123))\n",
      "  Obtaining dependency information for pillow==10.2.0 from https://files.pythonhosted.org/packages/41/a3/8644f5e4680e9e4b51b306a4042699bed29ae035181d412971218e95fd40/pillow-10.2.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading pillow-10.2.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting platformdirs==4.1.0 (from -r requirements.txt (line 124))\n",
      "  Obtaining dependency information for platformdirs==4.1.0 from https://files.pythonhosted.org/packages/be/53/42fe5eab4a09d251a76d0043e018172db324a23fcdac70f77a551c11f618/platformdirs-4.1.0-py3-none-any.whl.metadata\n",
      "  Downloading platformdirs-4.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: portpicker==1.6.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 125)) (1.6.0)\n",
      "Collecting prometheus-client==0.19.0 (from -r requirements.txt (line 126))\n",
      "  Obtaining dependency information for prometheus-client==0.19.0 from https://files.pythonhosted.org/packages/bb/9f/ad934418c48d01269fc2af02229ff64bcf793fd5d7f8f82dc5e7ea7ef149/prometheus_client-0.19.0-py3-none-any.whl.metadata\n",
      "  Downloading prometheus_client-0.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting prompt-toolkit==3.0.43 (from -r requirements.txt (line 127))\n",
      "  Obtaining dependency information for prompt-toolkit==3.0.43 from https://files.pythonhosted.org/packages/ee/fd/ca7bf3869e7caa7a037e23078539467b433a4e01eebd93f77180ab927766/prompt_toolkit-3.0.43-py3-none-any.whl.metadata\n",
      "  Downloading prompt_toolkit-3.0.43-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting proto-plus==1.23.0 (from -r requirements.txt (line 128))\n",
      "  Obtaining dependency information for proto-plus==1.23.0 from https://files.pythonhosted.org/packages/ad/41/7361075f3a31dcd05a6a38cfd807a6eecbfb6dbfe420d922cd400fc03ac1/proto_plus-1.23.0-py3-none-any.whl.metadata\n",
      "  Downloading proto_plus-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 129)) (3.20.3)\n",
      "Collecting psutil==5.9.8 (from -r requirements.txt (line 130))\n",
      "  Obtaining dependency information for psutil==5.9.8 from https://files.pythonhosted.org/packages/c5/4f/0e22aaa246f96d6ac87fe5ebb9c5a693fbe8877f537a1022527c47ca43c5/psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 131)) (0.7.0)\n",
      "Requirement already satisfied: pyarrow==10.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 132)) (10.0.1)\n",
      "Collecting pyasn1==0.5.1 (from -r requirements.txt (line 133))\n",
      "  Obtaining dependency information for pyasn1==0.5.1 from https://files.pythonhosted.org/packages/d1/75/4686d2872bf2fc0b37917cbc8bbf0dd3a5cdb0990799be1b9cbf1e1eb733/pyasn1-0.5.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: pyasn1-modules==0.3.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 134)) (0.3.0)\n",
      "Requirement already satisfied: pycparser==2.21 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 135)) (2.21)\n",
      "Collecting pydantic==1.10.14 (from -r requirements.txt (line 136))\n",
      "  Obtaining dependency information for pydantic==1.10.14 from https://files.pythonhosted.org/packages/f6/24/7c457e3b40d0ad4adaeec8580b5ba67f46c04ed2b59f814d8f8f051a445c/pydantic-1.10.14-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pydantic-1.10.14-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (150 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 150.2/150.2 kB 10.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pydot==1.4.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 137)) (1.4.2)\n",
      "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 138)) (2.31.0)\n",
      "Requirement already satisfied: requests-oauthlib==1.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 139)) (1.3.1)\n",
      "Collecting requests-toolbelt==0.10.1 (from -r requirements.txt (line 140))\n",
      "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.5/54.5 kB 5.4 MB/s eta 0:00:00\n",
      "Collecting rich==13.7.0 (from -r requirements.txt (line 141))\n",
      "  Obtaining dependency information for rich==13.7.0 from https://files.pythonhosted.org/packages/be/be/1520178fa01eabe014b16e72a952b9f900631142ccd03dc36cf93e30c1ce/rich-13.7.0-py3-none-any.whl.metadata\n",
      "  Downloading rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: rsa==4.9 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 142)) (4.9)\n",
      "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 143)) (1.10.1)\n",
      "Requirement already satisfied: send2trash==1.8.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 144)) (1.8.2)\n",
      "Requirement already satisfied: six==1.16.0 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 145)) (1.16.0)\n",
      "Requirement already satisfied: sniffio==1.3.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 146)) (1.3.0)\n",
      "Requirement already satisfied: soupsieve==2.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 147)) (2.5)\n",
      "Requirement already satisfied: sqlparse==0.4.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 148)) (0.4.4)\n",
      "Collecting strip-hints==0.1.10 (from -r requirements.txt (line 149))\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tabulate==0.9.0 (from -r requirements.txt (line 150))\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: tensorboard==2.13.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 151)) (2.13.0)\n",
      "Collecting tensorboard-data-server==0.7.2 (from -r requirements.txt (line 152))\n",
      "  Obtaining dependency information for tensorboard-data-server==0.7.2 from https://files.pythonhosted.org/packages/73/c6/825dab04195756cf8ff2e12698f22513b3db2f64925bdd41671bfb33aaa5/tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting tensorflow==2.13.1 (from -r requirements.txt (line 153))\n",
      "  Obtaining dependency information for tensorflow==2.13.1 from https://files.pythonhosted.org/packages/c2/20/b15abac0be474f12cf51a104c9dd935b053081b502c103e9e947e8be7b84/tensorflow-2.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorflow-2.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: tensorflow-data-validation==1.14.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 154)) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-estimator==2.13.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 155)) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-hub==0.13.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 156)) (0.13.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 157)) (0.24.0)\n",
      "Requirement already satisfied: tensorflow-metadata==1.14.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 158)) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-model-analysis==0.45.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 159)) (0.45.0)\n",
      "Collecting tensorflow-serving-api==2.13.1 (from -r requirements.txt (line 160))\n",
      "  Obtaining dependency information for tensorflow-serving-api==2.13.1 from https://files.pythonhosted.org/packages/14/d7/11e3d0a3f6c50a38e05550be85e35ca1f1bfd85219977b18694367c5688f/tensorflow_serving_api-2.13.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_serving_api-2.13.1-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow-transform==1.14.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 161)) (1.14.0)\n",
      "Collecting termcolor==2.4.0 (from -r requirements.txt (line 162))\n",
      "  Obtaining dependency information for termcolor==2.4.0 from https://files.pythonhosted.org/packages/d9/5f/8c716e47b3a50cbd7c146f45881e11d9414def768b7cd9c5e6650ec2a80a/termcolor-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting terminado==0.18.0 (from -r requirements.txt (line 163))\n",
      "  Obtaining dependency information for terminado==0.18.0 from https://files.pythonhosted.org/packages/69/df/deebc9fb14a49062a3330f673e80b100e665b54d998163b3f62620b6240c/terminado-0.18.0-py3-none-any.whl.metadata\n",
      "  Downloading terminado-0.18.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: tfx==1.14.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 164)) (1.14.0)\n",
      "Requirement already satisfied: tfx-bsl==1.14.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 165)) (1.14.0)\n",
      "Requirement already satisfied: tinycss2==1.2.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 166)) (1.2.1)\n",
      "Collecting tornado==6.4 (from -r requirements.txt (line 167))\n",
      "  Obtaining dependency information for tornado==6.4 from https://files.pythonhosted.org/packages/9f/12/11d0a757bb67278d3380d41955ae98527d5ad18330b2edbdc8de222b569b/tornado-6.4-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tornado-6.4-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting traitlets==5.14.1 (from -r requirements.txt (line 168))\n",
      "  Obtaining dependency information for traitlets==5.14.1 from https://files.pythonhosted.org/packages/45/34/5dc77fdc7bb4bd198317eea5679edf9cc0a186438b5b19dbb9062fb0f4d5/traitlets-5.14.1-py3-none-any.whl.metadata\n",
      "  Downloading traitlets-5.14.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting typer==0.9.0 (from -r requirements.txt (line 169))\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.9/45.9 kB 5.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 170)) (4.5.0)\n",
      "Requirement already satisfied: uri-template==1.3.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 171)) (1.3.0)\n",
      "Requirement already satisfied: uritemplate==3.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 172)) (3.0.1)\n",
      "Collecting urllib3==1.26.18 (from -r requirements.txt (line 173))\n",
      "  Obtaining dependency information for urllib3==1.26.18 from https://files.pythonhosted.org/packages/b0/53/aa91e163dcfd1e5b82d8a890ecf13314e3e149c05270cc644581f77f17fd/urllib3-1.26.18-py2.py3-none-any.whl.metadata\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.9/48.9 kB 6.8 MB/s eta 0:00:00\n",
      "Collecting wcwidth==0.2.13 (from -r requirements.txt (line 174))\n",
      "  Obtaining dependency information for wcwidth==0.2.13 from https://files.pythonhosted.org/packages/fd/84/fd2ba7aafacbad3c4201d395674fc6348826569da3c0937e75505ead3528/wcwidth-0.2.13-py2.py3-none-any.whl.metadata\n",
      "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: webcolors==1.13 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 175)) (1.13)\n",
      "Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 176)) (0.5.1)\n",
      "Collecting websocket-client==1.7.0 (from -r requirements.txt (line 177))\n",
      "  Obtaining dependency information for websocket-client==1.7.0 from https://files.pythonhosted.org/packages/1e/70/1e88138a9afbed1d37093b85f0bebc3011623c4f47c166431599fe9d6c93/websocket_client-1.7.0-py3-none-any.whl.metadata\n",
      "  Downloading websocket_client-1.7.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting werkzeug==3.0.1 (from -r requirements.txt (line 178))\n",
      "  Obtaining dependency information for werkzeug==3.0.1 from https://files.pythonhosted.org/packages/c3/fc/254c3e9b5feb89ff5b9076a23218dafbc99c96ac5941e900b71206e6313b/werkzeug-3.0.1-py3-none-any.whl.metadata\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting widgetsnbextension==3.6.6 (from -r requirements.txt (line 179))\n",
      "  Obtaining dependency information for widgetsnbextension==3.6.6 from https://files.pythonhosted.org/packages/9c/a0/ba2634cd75b7d7f8f9aeb38edf854cd6c9877ec064013a62630b4541b88f/widgetsnbextension-3.6.6-py2.py3-none-any.whl.metadata\n",
      "  Downloading widgetsnbextension-3.6.6-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting wrapt==1.16.0 (from -r requirements.txt (line 180))\n",
      "  Obtaining dependency information for wrapt==1.16.0 from https://files.pythonhosted.org/packages/ef/c6/56e718e2c58a4078518c14d97e531ef1e9e8a5c1ddafdc0d264a92be1a1a/wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting zstandard==0.22.0 (from -r requirements.txt (line 181))\n",
      "  Obtaining dependency information for zstandard==0.22.0 from https://files.pythonhosted.org/packages/ef/e7/1cce80b1abc3b2d07eeb0a41a179adb2a49aba8b3064518497664a3ba3ba/zstandard-0.22.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading zstandard-0.22.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
      "Collecting pytest (from -r requirements.txt (line 182))\n",
      "  Obtaining dependency information for pytest from https://files.pythonhosted.org/packages/c7/10/727155d44c5e04bb08e880668e53079547282e4f950535234e5a80690564/pytest-8.0.0-py3-none-any.whl.metadata\n",
      "  Downloading pytest-8.0.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam==2.50.0->-r requirements.txt (line 3)) (4.5.0)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam==2.50.0->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.8/dist-packages (from apache-beam==2.50.0->-r requirements.txt (line 3)) (2023.3.post1)\n",
      "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.8/dist-packages (from apache-beam==2.50.0->-r requirements.txt (line 3)) (2023.8.8)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow==1.3.0->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for types-python-dateutil>=2.8.10 from https://files.pythonhosted.org/packages/28/50/8ed67814241e2684369f4b8b881c7d31a0816e76c8690ea8518017a35b7e/types_python_dateutil-2.8.19.20240106-py3-none-any.whl.metadata\n",
      "  Downloading types_python_dateutil-2.8.19.20240106-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse==1.6.3->-r requirements.txt (line 7)) (0.37.1)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform==1.34.0->-r requirements.txt (line 43)) (2.11.1)\n",
      "Requirement already satisfied: shapely<2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform==1.34.0->-r requirements.txt (line 43)) (1.8.5.post1)\n",
      "Requirement already satisfied: googleapis-common-protos[grpc]<2.0.0dev,>=1.56.0 in /usr/local/lib/python3.8/dist-packages (from grpc-google-iam-v1==0.13.0->-r requirements.txt (line 63)) (1.60.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/lib/python3/dist-packages (from httplib2==0.22.0->-r requirements.txt (line 68)) (2.4.7)\n",
      "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.8/dist-packages (from ipykernel==6.29.0->-r requirements.txt (line 70)) (24.0.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/lib/python3/dist-packages (from ipython==7.34.0->-r requirements.txt (line 71)) (59.6.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython==7.34.0->-r requirements.txt (line 71)) (2.16.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2==3.1.3->-r requirements.txt (line 76)) (2.1.3)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema==4.17.3->-r requirements.txt (line 79)) (6.0.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /usr/local/lib/python3.8/dist-packages (from jsonschema==4.17.3->-r requirements.txt (line 79)) (1.3.10)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema==4.17.3->-r requirements.txt (line 79)) (0.19.3)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.8/dist-packages (from jupyter-events==0.6.3->-r requirements.txt (line 82)) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.8/dist-packages (from jupyter-events==0.6.3->-r requirements.txt (line 82)) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.8/dist-packages (from jupyter-events==0.6.3->-r requirements.txt (line 82)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-events==0.6.3->-r requirements.txt (line 82)) (0.1.1)\n",
      "Collecting keras-core (from keras-tuner==1.4.4->-r requirements.txt (line 88))\n",
      "  Obtaining dependency information for keras-core from https://files.pythonhosted.org/packages/33/18/5280fbfd70485fae8088a2f44647ea0d82c7dc9b80635ae2e7c578f785e0/keras_core-0.1.5-py3-none-any.whl.metadata\n",
      "  Downloading keras_core-0.1.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting Deprecated<2,>=1.2.7 (from kfp==1.8.22->-r requirements.txt (line 89))\n",
      "  Obtaining dependency information for Deprecated<2,>=1.2.7 from https://files.pythonhosted.org/packages/20/8d/778b7d51b981a96554f29136cd59ca7880bf58094338085bcf2a979a0e6a/Deprecated-1.2.14-py2.py3-none-any.whl.metadata\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kfp-server-api==1.8.5->-r requirements.txt (line 91)) (2023.7.22)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/lib/python3/dist-packages (from markdown==3.5.2->-r requirements.txt (line 95)) (4.6.4)\n",
      "Requirement already satisfied: pyfarmhash<0.4,>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-data-validation==1.14.0->-r requirements.txt (line 154)) (0.3.2)\n",
      "Collecting iniconfig (from pytest->-r requirements.txt (line 182))\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting pluggy<2.0,>=1.3.0 (from pytest->-r requirements.txt (line 182))\n",
      "  Obtaining dependency information for pluggy<2.0,>=1.3.0 from https://files.pythonhosted.org/packages/a5/5b/0cc789b59e8cc1bf288b38111d002d8c5917123194d45b29dcdac64723cc/pluggy-1.4.0-py3-none-any.whl.metadata\n",
      "  Downloading pluggy-1.4.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting tomli>=1.0.0 (from pytest->-r requirements.txt (line 182))\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "INFO: pip is looking at multiple versions of google-api-core[grpc] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 (from google-cloud-aiplatform==1.34.0->-r requirements.txt (line 43))\n",
      "  Obtaining dependency information for google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 from https://files.pythonhosted.org/packages/e7/b5/b93e0041c7132032fd3bd273aef334cf587c5b9c1056897728069b90968f/google_api_core-2.16.1-py3-none-any.whl.metadata\n",
      "  Downloading google_api_core-2.16.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Obtaining dependency information for google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 from https://files.pythonhosted.org/packages/60/69/5182c52db38323a6c9e64ffb9346879db1a0f003897b5e8710045943b38a/google_api_core-2.16.0-py3-none-any.whl.metadata\n",
      "  Downloading google_api_core-2.16.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "INFO: pip is looking at multiple versions of googleapis-common-protos[grpc] to determine which version is compatible with other requirements. This could take a while.\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema==4.17.3->-r requirements.txt (line 79)) (3.16.2)\n",
      "Downloading anyio-4.2.0-py3-none-any.whl (85 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.5/85.5 kB 12.6 MB/s eta 0:00:00\n",
      "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.4/66.4 kB 10.9 MB/s eta 0:00:00\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.9/147.9 kB 22.1 MB/s eta 0:00:00\n",
      "Downloading bleach-6.1.0-py3-none-any.whl (162 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 162.8/162.8 kB 20.1 MB/s eta 0:00:00\n",
      "Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Downloading cffi-1.16.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (444 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 444.7/444.7 kB 21.0 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.1/141.1 kB 18.3 MB/s eta 0:00:00\n",
      "Downloading comm-0.2.1-py3-none-any.whl (7.2 kB)\n",
      "Downloading debugpy-1.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.5/3.5 MB 30.1 MB/s eta 0:00:00\n",
      "Downloading dnspython-2.5.0-py3-none-any.whl (305 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 305.4/305.4 kB 33.9 MB/s eta 0:00:00\n",
      "Downloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading fastavro-1.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 47.1 MB/s eta 0:00:00\n",
      "Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Downloading fastjsonschema-2.19.1-py3-none-any.whl (23 kB)\n",
      "Downloading google_api_core-2.15.0-py3-none-any.whl (121 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.0/122.0 kB 15.9 MB/s eta 0:00:00\n",
      "Downloading google_auth-2.27.0-py2.py3-none-any.whl (186 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 186.8/186.8 kB 23.3 MB/s eta 0:00:00\n",
      "Downloading google_auth_httplib2-0.1.1-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading google_cloud_aiplatform-1.34.0-py2.py3-none-any.whl (3.1 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 54.1 MB/s eta 0:00:00\n",
      "Downloading google_cloud_bigquery_storage-2.24.0-py2.py3-none-any.whl (190 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 190.6/190.6 kB 18.4 MB/s eta 0:00:00\n",
      "Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_cloud_datastore-2.19.0-py2.py3-none-any.whl (176 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 176.4/176.4 kB 19.6 MB/s eta 0:00:00\n",
      "Downloading google_cloud_dlp-3.12.3-py2.py3-none-any.whl (143 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.4/143.4 kB 17.8 MB/s eta 0:00:00\n",
      "Downloading google_cloud_language-2.12.0-py2.py3-none-any.whl (137 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.8/137.8 kB 17.1 MB/s eta 0:00:00\n",
      "Downloading google_cloud_pubsub-2.18.4-py2.py3-none-any.whl (265 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 265.9/265.9 kB 25.2 MB/s eta 0:00:00\n",
      "Downloading google_cloud_recommendations_ai-0.10.5-py2.py3-none-any.whl (173 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.4/173.4 kB 22.2 MB/s eta 0:00:00\n",
      "Downloading google_cloud_resource_manager-1.10.4-py2.py3-none-any.whl (320 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 321.0/321.0 kB 27.4 MB/s eta 0:00:00\n",
      "Downloading google_cloud_storage-2.14.0-py2.py3-none-any.whl (121 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 kB 18.2 MB/s eta 0:00:00\n",
      "Downloading google_cloud_videointelligence-2.11.4-py2.py3-none-any.whl (229 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.5/229.5 kB 28.5 MB/s eta 0:00:00\n",
      "Downloading google_resumable_media-2.7.0-py2.py3-none-any.whl (80 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 kB 11.1 MB/s eta 0:00:00\n",
      "Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 228.7/228.7 kB 24.0 MB/s eta 0:00:00\n",
      "Downloading grpc_google_iam_v1-0.13.0-py2.py3-none-any.whl (25 kB)\n",
      "Downloading grpcio-1.60.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 43.8 MB/s eta 0:00:00\n",
      "Downloading h5py-3.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 44.3 MB/s eta 0:00:00\n",
      "Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.6/61.6 kB 10.0 MB/s eta 0:00:00\n",
      "Downloading ipykernel-6.29.0-py3-none-any.whl (116 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.1/116.1 kB 17.1 MB/s eta 0:00:00\n",
      "Downloading ipywidgets-7.8.1-py2.py3-none-any.whl (124 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 kB 17.5 MB/s eta 0:00:00\n",
      "Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 60.8 MB/s eta 0:00:00\n",
      "Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.2/133.2 kB 19.9 MB/s eta 0:00:00\n",
      "Downloading jupyter_core-5.7.1-py3-none-any.whl (28 kB)\n",
      "Downloading jupyter_server-2.10.0-py3-none-any.whl (377 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 377.9/377.9 kB 39.4 MB/s eta 0:00:00\n",
      "Downloading jupyter_server_terminals-0.5.2-py3-none-any.whl (13 kB)\n",
      "Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading jupyterlab_widgets-1.1.7-py3-none-any.whl (295 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.4/295.4 kB 33.2 MB/s eta 0:00:00\n",
      "Downloading keras_tuner-1.4.4-py3-none-any.whl (127 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.0/128.0 kB 7.8 MB/s eta 0:00:00\n",
      "Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.9/103.9 kB 14.1 MB/s eta 0:00:00\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.5/87.5 kB 14.8 MB/s eta 0:00:00\n",
      "Downloading mistune-3.0.2-py3-none-any.whl (47 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.0/48.0 kB 8.0 MB/s eta 0:00:00\n",
      "Downloading nbclient-0.9.0-py3-none-any.whl (24 kB)\n",
      "Downloading nbconvert-7.14.2-py3-none-any.whl (256 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 256.4/256.4 kB 33.9 MB/s eta 0:00:00\n",
      "Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Downloading notebook-6.5.6-py3-none-any.whl (529 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 529.8/529.8 kB 36.6 MB/s eta 0:00:00\n",
      "Downloading orjson-3.9.12-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.5/139.5 kB 20.4 MB/s eta 0:00:00\n",
      "Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.8/63.8 kB 11.0 MB/s eta 0:00:00\n",
      "Downloading pillow-10.2.0-cp38-cp38-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 62.3 MB/s eta 0:00:00\n",
      "Downloading platformdirs-4.1.0-py3-none-any.whl (17 kB)\n",
      "Downloading prometheus_client-0.19.0-py3-none-any.whl (54 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.2/54.2 kB 7.4 MB/s eta 0:00:00\n",
      "Downloading prompt_toolkit-3.0.43-py3-none-any.whl (386 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 386.1/386.1 kB 40.4 MB/s eta 0:00:00\n",
      "Downloading proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.8/48.8 kB 8.5 MB/s eta 0:00:00\n",
      "Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 288.2/288.2 kB 35.3 MB/s eta 0:00:00\n",
      "Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.9/84.9 kB 14.5 MB/s eta 0:00:00\n",
      "Downloading pydantic-1.10.14-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 60.8 MB/s eta 0:00:00\n",
      "Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 240.6/240.6 kB 31.7 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 52.0 MB/s eta 0:00:00\n",
      "Downloading tensorflow-2.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 479.6/479.6 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading tensorflow_serving_api-2.13.1-py2.py3-none-any.whl (26 kB)\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading terminado-0.18.0-py3-none-any.whl (14 kB)\n",
      "Downloading tornado-6.4-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 435.4/435.4 kB 39.5 MB/s eta 0:00:00\n",
      "Downloading traitlets-5.14.1-py3-none-any.whl (85 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.4/85.4 kB 13.4 MB/s eta 0:00:00\n",
      "Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.8/143.8 kB 20.1 MB/s eta 0:00:00\n",
      "Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Downloading websocket_client-1.7.0-py3-none-any.whl (58 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.5/58.5 kB 9.0 MB/s eta 0:00:00\n",
      "Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.7/226.7 kB 23.0 MB/s eta 0:00:00\n",
      "Downloading widgetsnbextension-3.6.6-py2.py3-none-any.whl (1.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 66.6 MB/s eta 0:00:00\n",
      "Downloading wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.4/83.4 kB 13.4 MB/s eta 0:00:00\n",
      "Downloading zstandard-0.22.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 62.0 MB/s eta 0:00:00\n",
      "Downloading pytest-8.0.0-py3-none-any.whl (334 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 334.0/334.0 kB 37.3 MB/s eta 0:00:00\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading pluggy-1.4.0-py3-none-any.whl (20 kB)\n",
      "Downloading types_python_dateutil-2.8.19.20240106-py3-none-any.whl (9.7 kB)\n",
      "Downloading keras_core-0.1.5-py3-none-any.whl (924 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 924.6/924.6 kB 54.8 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: fire, kfp, kfp-server-api, strip-hints\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116951 sha256=8f27e325d1b1aa13bb4d3a7d1a66914171e72b3b04fb77fe06faef11e097d633\n",
      "  Stored in directory: /root/.cache/pip/wheels/5b/eb/43/7295e71293b218ddfd627f935229bf54af9018add7fbb5aac6\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.8.22-py3-none-any.whl size=426988 sha256=a42614215a2d58cd09583b2ff32f4a23f42310cac087b227eb29100fb44d9a42\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/45/22/8a34fe9663f31e4b72f79bd206ee881e137c84c906986da491\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.5-py3-none-any.whl size=99716 sha256=ed9b5c4fe0e5137c7bc945d7f1b639dce841e329bfca89d5866914fa90a05b95\n",
      "  Stored in directory: /root/.cache/pip/wheels/93/b7/87/8884b574029455610a5b99752115d2ac857f8cfe8b846a1225\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=fc4877ae6237a5f03296d66239d8a390b8d8820cdc7a3cd592a1a30d88553621\n",
      "  Stored in directory: /root/.cache/pip/wheels/28/16/9b/7c21f4d08b98d02819658600b738d3453b2ffee3c9b757629e\n",
      "Successfully built fire kfp kfp-server-api strip-hints\n",
      "Installing collected packages: wcwidth, namex, fastjsonschema, zstandard, wrapt, werkzeug, websocket-client, urllib3, types-python-dateutil, typer, traitlets, tornado, tomli, termcolor, tensorboard-data-server, tabulate, strip-hints, pydantic, pyasn1, psutil, proto-plus, prompt-toolkit, prometheus-client, pluggy, platformdirs, pillow, pexpect, pandocfilters, orjson, oauthlib, nest-asyncio, mistune, mdurl, markdown, jupyterlab-widgets, jupyterlab-pygments, jinja2, jedi, iniconfig, idna, httplib2, h5py, grpcio, googleapis-common-protos, google-resumable-media, fasteners, fastavro, exceptiongroup, docstring-parser, dnspython, debugpy, charset-normalizer, cffi, cachetools, bleach, beautifulsoup4, terminado, pytest, markdown-it-py, kfp-server-api, jupyter-core, fire, Deprecated, comm, arrow, anyio, rich, requests-toolbelt, jupyter-server-terminals, grpc-google-iam-v1, google-auth, nbclient, keras-core, ipykernel, google-auth-httplib2, google-api-core, nbconvert, keras-tuner, google-cloud-core, tensorflow, jupyter-server, google-cloud-videointelligence, google-cloud-storage, google-cloud-resource-manager, google-cloud-recommendations-ai, google-cloud-pubsub, google-cloud-language, google-cloud-dlp, google-cloud-datastore, google-cloud-bigquery-storage, tensorflow-serving-api, kfp, google-cloud-aiplatform, notebook, widgetsnbextension, ipywidgets\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.6\n",
      "    Uninstalling wcwidth-0.2.6:\n",
      "      Successfully uninstalled wcwidth-0.2.6\n",
      "  Attempting uninstall: fastjsonschema\n",
      "    Found existing installation: fastjsonschema 2.18.0\n",
      "    Uninstalling fastjsonschema-2.18.0:\n",
      "      Successfully uninstalled fastjsonschema-2.18.0\n",
      "  Attempting uninstall: zstandard\n",
      "    Found existing installation: zstandard 0.21.0\n",
      "    Uninstalling zstandard-0.21.0:\n",
      "      Successfully uninstalled zstandard-0.21.0\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.15.0\n",
      "    Uninstalling wrapt-1.15.0:\n",
      "      Successfully uninstalled wrapt-1.15.0\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 2.3.7\n",
      "    Uninstalling Werkzeug-2.3.7:\n",
      "      Successfully uninstalled Werkzeug-2.3.7\n",
      "  Attempting uninstall: websocket-client\n",
      "    Found existing installation: websocket-client 1.6.2\n",
      "    Uninstalling websocket-client-1.6.2:\n",
      "      Successfully uninstalled websocket-client-1.6.2\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.16\n",
      "    Uninstalling urllib3-1.26.16:\n",
      "      Successfully uninstalled urllib3-1.26.16\n",
      "  Attempting uninstall: traitlets\n",
      "    Found existing installation: traitlets 5.9.0\n",
      "    Uninstalling traitlets-5.9.0:\n",
      "      Successfully uninstalled traitlets-5.9.0\n",
      "  Attempting uninstall: tornado\n",
      "    Found existing installation: tornado 6.3.3\n",
      "    Uninstalling tornado-6.3.3:\n",
      "      Successfully uninstalled tornado-6.3.3\n",
      "  Attempting uninstall: termcolor\n",
      "    Found existing installation: termcolor 2.3.0\n",
      "    Uninstalling termcolor-2.3.0:\n",
      "      Successfully uninstalled termcolor-2.3.0\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.1\n",
      "    Uninstalling tensorboard-data-server-0.7.1:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.1\n",
      "  Attempting uninstall: pyasn1\n",
      "    Found existing installation: pyasn1 0.5.0\n",
      "    Uninstalling pyasn1-0.5.0:\n",
      "      Successfully uninstalled pyasn1-0.5.0\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 5.9.5\n",
      "    Uninstalling psutil-5.9.5:\n",
      "      Successfully uninstalled psutil-5.9.5\n",
      "  Attempting uninstall: proto-plus\n",
      "    Found existing installation: proto-plus 1.22.3\n",
      "    Uninstalling proto-plus-1.22.3:\n",
      "      Successfully uninstalled proto-plus-1.22.3\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt-toolkit 3.0.39\n",
      "    Uninstalling prompt-toolkit-3.0.39:\n",
      "      Successfully uninstalled prompt-toolkit-3.0.39\n",
      "  Attempting uninstall: prometheus-client\n",
      "    Found existing installation: prometheus-client 0.17.1\n",
      "    Uninstalling prometheus-client-0.17.1:\n",
      "      Successfully uninstalled prometheus-client-0.17.1\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 3.10.0\n",
      "    Uninstalling platformdirs-3.10.0:\n",
      "      Successfully uninstalled platformdirs-3.10.0\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 10.0.0\n",
      "    Uninstalling Pillow-10.0.0:\n",
      "      Successfully uninstalled Pillow-10.0.0\n",
      "  Attempting uninstall: pexpect\n",
      "    Found existing installation: pexpect 4.8.0\n",
      "    Uninstalling pexpect-4.8.0:\n",
      "      Successfully uninstalled pexpect-4.8.0\n",
      "  Attempting uninstall: pandocfilters\n",
      "    Found existing installation: pandocfilters 1.5.0\n",
      "    Uninstalling pandocfilters-1.5.0:\n",
      "      Successfully uninstalled pandocfilters-1.5.0\n",
      "  Attempting uninstall: orjson\n",
      "    Found existing installation: orjson 3.9.5\n",
      "    Uninstalling orjson-3.9.5:\n",
      "      Successfully uninstalled orjson-3.9.5\n",
      "  Attempting uninstall: oauthlib\n",
      "    Found existing installation: oauthlib 3.2.0\n",
      "    Uninstalling oauthlib-3.2.0:\n",
      "      Successfully uninstalled oauthlib-3.2.0\n",
      "  Attempting uninstall: nest-asyncio\n",
      "    Found existing installation: nest-asyncio 1.5.7\n",
      "    Uninstalling nest-asyncio-1.5.7:\n",
      "      Successfully uninstalled nest-asyncio-1.5.7\n",
      "  Attempting uninstall: mistune\n",
      "    Found existing installation: mistune 3.0.1\n",
      "    Uninstalling mistune-3.0.1:\n",
      "      Successfully uninstalled mistune-3.0.1\n",
      "  Attempting uninstall: markdown\n",
      "    Found existing installation: Markdown 3.4.4\n",
      "    Uninstalling Markdown-3.4.4:\n",
      "      Successfully uninstalled Markdown-3.4.4\n",
      "  Attempting uninstall: jupyterlab-widgets\n",
      "    Found existing installation: jupyterlab-widgets 1.1.5\n",
      "    Uninstalling jupyterlab-widgets-1.1.5:\n",
      "      Successfully uninstalled jupyterlab-widgets-1.1.5\n",
      "  Attempting uninstall: jupyterlab-pygments\n",
      "    Found existing installation: jupyterlab-pygments 0.2.2\n",
      "    Uninstalling jupyterlab-pygments-0.2.2:\n",
      "      Successfully uninstalled jupyterlab-pygments-0.2.2\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.2\n",
      "    Uninstalling Jinja2-3.1.2:\n",
      "      Successfully uninstalled Jinja2-3.1.2\n",
      "  Attempting uninstall: jedi\n",
      "    Found existing installation: jedi 0.19.0\n",
      "    Uninstalling jedi-0.19.0:\n",
      "      Successfully uninstalled jedi-0.19.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: httplib2\n",
      "    Found existing installation: httplib2 0.20.2\n",
      "    Uninstalling httplib2-0.20.2:\n",
      "      Successfully uninstalled httplib2-0.20.2\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.9.0\n",
      "    Uninstalling h5py-3.9.0:\n",
      "      Successfully uninstalled h5py-3.9.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.57.0\n",
      "    Uninstalling grpcio-1.57.0:\n",
      "      Successfully uninstalled grpcio-1.57.0\n",
      "  Attempting uninstall: googleapis-common-protos\n",
      "    Found existing installation: googleapis-common-protos 1.60.0\n",
      "    Uninstalling googleapis-common-protos-1.60.0:\n",
      "      Successfully uninstalled googleapis-common-protos-1.60.0\n",
      "  Attempting uninstall: google-resumable-media\n",
      "    Found existing installation: google-resumable-media 2.6.0\n",
      "    Uninstalling google-resumable-media-2.6.0:\n",
      "      Successfully uninstalled google-resumable-media-2.6.0\n",
      "  Attempting uninstall: fasteners\n",
      "    Found existing installation: fasteners 0.18\n",
      "    Uninstalling fasteners-0.18:\n",
      "      Successfully uninstalled fasteners-0.18\n",
      "  Attempting uninstall: fastavro\n",
      "    Found existing installation: fastavro 1.8.2\n",
      "    Uninstalling fastavro-1.8.2:\n",
      "      Successfully uninstalled fastavro-1.8.2\n",
      "  Attempting uninstall: exceptiongroup\n",
      "    Found existing installation: exceptiongroup 1.1.3\n",
      "    Uninstalling exceptiongroup-1.1.3:\n",
      "      Successfully uninstalled exceptiongroup-1.1.3\n",
      "  Attempting uninstall: dnspython\n",
      "    Found existing installation: dnspython 2.4.2\n",
      "    Uninstalling dnspython-2.4.2:\n",
      "      Successfully uninstalled dnspython-2.4.2\n",
      "  Attempting uninstall: debugpy\n",
      "    Found existing installation: debugpy 1.6.7.post1\n",
      "    Uninstalling debugpy-1.6.7.post1:\n",
      "      Successfully uninstalled debugpy-1.6.7.post1\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.2.0\n",
      "    Uninstalling charset-normalizer-3.2.0:\n",
      "      Successfully uninstalled charset-normalizer-3.2.0\n",
      "  Attempting uninstall: cffi\n",
      "    Found existing installation: cffi 1.15.1\n",
      "    Uninstalling cffi-1.15.1:\n",
      "      Successfully uninstalled cffi-1.15.1\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.3.1\n",
      "    Uninstalling cachetools-5.3.1:\n",
      "      Successfully uninstalled cachetools-5.3.1\n",
      "  Attempting uninstall: bleach\n",
      "    Found existing installation: bleach 6.0.0\n",
      "    Uninstalling bleach-6.0.0:\n",
      "      Successfully uninstalled bleach-6.0.0\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.12.2\n",
      "    Uninstalling beautifulsoup4-4.12.2:\n",
      "      Successfully uninstalled beautifulsoup4-4.12.2\n",
      "  Attempting uninstall: terminado\n",
      "    Found existing installation: terminado 0.17.1\n",
      "    Uninstalling terminado-0.17.1:\n",
      "      Successfully uninstalled terminado-0.17.1\n",
      "  Attempting uninstall: jupyter-core\n",
      "    Found existing installation: jupyter_core 5.3.1\n",
      "    Uninstalling jupyter_core-5.3.1:\n",
      "      Successfully uninstalled jupyter_core-5.3.1\n",
      "  Attempting uninstall: comm\n",
      "    Found existing installation: comm 0.1.4\n",
      "    Uninstalling comm-0.1.4:\n",
      "      Successfully uninstalled comm-0.1.4\n",
      "  Attempting uninstall: arrow\n",
      "    Found existing installation: arrow 1.2.3\n",
      "    Uninstalling arrow-1.2.3:\n",
      "      Successfully uninstalled arrow-1.2.3\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.0.0\n",
      "    Uninstalling anyio-4.0.0:\n",
      "      Successfully uninstalled anyio-4.0.0\n",
      "  Attempting uninstall: jupyter-server-terminals\n",
      "    Found existing installation: jupyter_server_terminals 0.4.4\n",
      "    Uninstalling jupyter_server_terminals-0.4.4:\n",
      "      Successfully uninstalled jupyter_server_terminals-0.4.4\n",
      "  Attempting uninstall: grpc-google-iam-v1\n",
      "    Found existing installation: grpc-google-iam-v1 0.12.6\n",
      "    Uninstalling grpc-google-iam-v1-0.12.6:\n",
      "      Successfully uninstalled grpc-google-iam-v1-0.12.6\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.22.0\n",
      "    Uninstalling google-auth-2.22.0:\n",
      "      Successfully uninstalled google-auth-2.22.0\n",
      "  Attempting uninstall: nbclient\n",
      "    Found existing installation: nbclient 0.8.0\n",
      "    Uninstalling nbclient-0.8.0:\n",
      "      Successfully uninstalled nbclient-0.8.0\n",
      "  Attempting uninstall: ipykernel\n",
      "    Found existing installation: ipykernel 6.25.2\n",
      "    Uninstalling ipykernel-6.25.2:\n",
      "      Successfully uninstalled ipykernel-6.25.2\n",
      "  Attempting uninstall: google-auth-httplib2\n",
      "    Found existing installation: google-auth-httplib2 0.1.0\n",
      "    Uninstalling google-auth-httplib2-0.1.0:\n",
      "      Successfully uninstalled google-auth-httplib2-0.1.0\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 2.11.1\n",
      "    Uninstalling google-api-core-2.11.1:\n",
      "      Successfully uninstalled google-api-core-2.11.1\n",
      "  Attempting uninstall: nbconvert\n",
      "    Found existing installation: nbconvert 7.8.0\n",
      "    Uninstalling nbconvert-7.8.0:\n",
      "      Successfully uninstalled nbconvert-7.8.0\n",
      "  Attempting uninstall: keras-tuner\n",
      "    Found existing installation: keras-tuner 1.3.5\n",
      "    Uninstalling keras-tuner-1.3.5:\n",
      "      Successfully uninstalled keras-tuner-1.3.5\n",
      "  Attempting uninstall: google-cloud-core\n",
      "    Found existing installation: google-cloud-core 2.3.3\n",
      "    Uninstalling google-cloud-core-2.3.3:\n",
      "      Successfully uninstalled google-cloud-core-2.3.3\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.13.0\n",
      "    Uninstalling tensorflow-2.13.0:\n",
      "      Successfully uninstalled tensorflow-2.13.0\n",
      "  Attempting uninstall: jupyter-server\n",
      "    Found existing installation: jupyter_server 2.7.3\n",
      "    Uninstalling jupyter_server-2.7.3:\n",
      "      Successfully uninstalled jupyter_server-2.7.3\n",
      "  Attempting uninstall: google-cloud-videointelligence\n",
      "    Found existing installation: google-cloud-videointelligence 2.11.3\n",
      "    Uninstalling google-cloud-videointelligence-2.11.3:\n",
      "      Successfully uninstalled google-cloud-videointelligence-2.11.3\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.10.0\n",
      "    Uninstalling google-cloud-storage-2.10.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.10.0\n",
      "  Attempting uninstall: google-cloud-resource-manager\n",
      "    Found existing installation: google-cloud-resource-manager 1.10.3\n",
      "    Uninstalling google-cloud-resource-manager-1.10.3:\n",
      "      Successfully uninstalled google-cloud-resource-manager-1.10.3\n",
      "  Attempting uninstall: google-cloud-recommendations-ai\n",
      "    Found existing installation: google-cloud-recommendations-ai 0.10.4\n",
      "    Uninstalling google-cloud-recommendations-ai-0.10.4:\n",
      "      Successfully uninstalled google-cloud-recommendations-ai-0.10.4\n",
      "  Attempting uninstall: google-cloud-pubsub\n",
      "    Found existing installation: google-cloud-pubsub 2.18.3\n",
      "    Uninstalling google-cloud-pubsub-2.18.3:\n",
      "      Successfully uninstalled google-cloud-pubsub-2.18.3\n",
      "  Attempting uninstall: google-cloud-language\n",
      "    Found existing installation: google-cloud-language 2.11.0\n",
      "    Uninstalling google-cloud-language-2.11.0:\n",
      "      Successfully uninstalled google-cloud-language-2.11.0\n",
      "  Attempting uninstall: google-cloud-dlp\n",
      "    Found existing installation: google-cloud-dlp 3.12.2\n",
      "    Uninstalling google-cloud-dlp-3.12.2:\n",
      "      Successfully uninstalled google-cloud-dlp-3.12.2\n",
      "  Attempting uninstall: google-cloud-datastore\n",
      "    Found existing installation: google-cloud-datastore 2.18.0\n",
      "    Uninstalling google-cloud-datastore-2.18.0:\n",
      "      Successfully uninstalled google-cloud-datastore-2.18.0\n",
      "  Attempting uninstall: google-cloud-bigquery-storage\n",
      "    Found existing installation: google-cloud-bigquery-storage 2.22.0\n",
      "    Uninstalling google-cloud-bigquery-storage-2.22.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-storage-2.22.0\n",
      "  Attempting uninstall: tensorflow-serving-api\n",
      "    Found existing installation: tensorflow-serving-api 2.13.0\n",
      "    Uninstalling tensorflow-serving-api-2.13.0:\n",
      "      Successfully uninstalled tensorflow-serving-api-2.13.0\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.32.0\n",
      "    Uninstalling google-cloud-aiplatform-1.32.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.32.0\n",
      "  Attempting uninstall: notebook\n",
      "    Found existing installation: notebook 6.5.5\n",
      "    Uninstalling notebook-6.5.5:\n",
      "      Successfully uninstalled notebook-6.5.5\n",
      "  Attempting uninstall: widgetsnbextension\n",
      "    Found existing installation: widgetsnbextension 3.6.5\n",
      "    Uninstalling widgetsnbextension-3.6.5:\n",
      "      Successfully uninstalled widgetsnbextension-3.6.5\n",
      "  Attempting uninstall: ipywidgets\n",
      "    Found existing installation: ipywidgets 7.8.0\n",
      "    Uninstalling ipywidgets-7.8.0:\n",
      "      Successfully uninstalled ipywidgets-7.8.0\n",
      "Successfully installed Deprecated-1.2.14 anyio-4.2.0 arrow-1.3.0 beautifulsoup4-4.12.3 bleach-6.1.0 cachetools-5.3.2 cffi-1.16.0 charset-normalizer-3.3.2 comm-0.2.1 debugpy-1.8.0 dnspython-2.5.0 docstring-parser-0.15 exceptiongroup-1.2.0 fastavro-1.9.3 fasteners-0.19 fastjsonschema-2.19.1 fire-0.5.0 google-api-core-2.15.0 google-auth-2.27.0 google-auth-httplib2-0.1.1 google-cloud-aiplatform-1.34.0 google-cloud-bigquery-storage-2.24.0 google-cloud-core-2.4.1 google-cloud-datastore-2.19.0 google-cloud-dlp-3.12.3 google-cloud-language-2.12.0 google-cloud-pubsub-2.18.4 google-cloud-recommendations-ai-0.10.5 google-cloud-resource-manager-1.10.4 google-cloud-storage-2.14.0 google-cloud-videointelligence-2.11.4 google-resumable-media-2.7.0 googleapis-common-protos-1.62.0 grpc-google-iam-v1-0.13.0 grpcio-1.60.0 h5py-3.10.0 httplib2-0.22.0 idna-3.6 iniconfig-2.0.0 ipykernel-6.29.0 ipywidgets-7.8.1 jedi-0.19.1 jinja2-3.1.3 jupyter-core-5.7.1 jupyter-server-2.10.0 jupyter-server-terminals-0.5.2 jupyterlab-pygments-0.3.0 jupyterlab-widgets-1.1.7 keras-core-0.1.5 keras-tuner-1.4.4 kfp-1.8.22 kfp-server-api-1.8.5 markdown-3.5.2 markdown-it-py-3.0.0 mdurl-0.1.2 mistune-3.0.2 namex-0.0.7 nbclient-0.9.0 nbconvert-7.14.2 nest-asyncio-1.6.0 notebook-6.5.6 oauthlib-3.2.2 orjson-3.9.12 pandocfilters-1.5.1 pexpect-4.9.0 pillow-10.2.0 platformdirs-4.1.0 pluggy-1.4.0 prometheus-client-0.19.0 prompt-toolkit-3.0.43 proto-plus-1.23.0 psutil-5.9.8 pyasn1-0.5.1 pydantic-1.10.14 pytest-8.0.0 requests-toolbelt-0.10.1 rich-13.7.0 strip-hints-0.1.10 tabulate-0.9.0 tensorboard-data-server-0.7.2 tensorflow-2.13.1 tensorflow-serving-api-2.13.1 termcolor-2.4.0 terminado-0.18.0 tomli-2.0.1 tornado-6.4 traitlets-5.14.1 typer-0.9.0 types-python-dateutil-2.8.19.20240106 urllib3-1.26.18 wcwidth-0.2.13 websocket-client-1.7.0 werkzeug-3.0.1 widgetsnbextension-3.6.6 wrapt-1.16.0 zstandard-0.22.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 49f141428122\n",
      " ---> f11d56271bc0\n",
      "Successfully built f11d56271bc0\n",
      "Successfully tagged gcr.io/stellar-orb-408015/cicd:latest\n",
      "PUSH\n",
      "Pushing gcr.io/stellar-orb-408015/cicd:latest\n",
      "The push refers to repository [gcr.io/stellar-orb-408015/cicd]\n",
      "31a8413f5eb8: Preparing\n",
      "0b0bfd8b376d: Preparing\n",
      "35a98904d316: Preparing\n",
      "c82497e9151f: Preparing\n",
      "944f7ad7d5be: Preparing\n",
      "b3a75710e85e: Preparing\n",
      "775cb856bba4: Preparing\n",
      "78bb63e63ed0: Preparing\n",
      "38f846d7128b: Preparing\n",
      "6189eef4f001: Preparing\n",
      "eccfcf8683dc: Preparing\n",
      "b8a4020b158f: Preparing\n",
      "bc0d9c74ea9a: Preparing\n",
      "3259c8832a5d: Preparing\n",
      "4fc4812ae49f: Preparing\n",
      "df7e6d0d15e2: Preparing\n",
      "724d9e72f714: Preparing\n",
      "84eef6acfb89: Preparing\n",
      "fcaf6b281212: Preparing\n",
      "90b29c238a16: Preparing\n",
      "8fb66524d219: Preparing\n",
      "fe74336bfcaa: Preparing\n",
      "886cbf0cc5a6: Preparing\n",
      "a9f4b6b1b66a: Preparing\n",
      "9ab87db027f2: Preparing\n",
      "a9144587e03b: Preparing\n",
      "e27154b6913d: Preparing\n",
      "d56803d8f0fd: Preparing\n",
      "246e7cfb894b: Preparing\n",
      "d91491ee4880: Preparing\n",
      "436e99a10105: Preparing\n",
      "5542e763119e: Preparing\n",
      "da228921f21c: Preparing\n",
      "bd3521ef34cd: Preparing\n",
      "e8cddc117f9b: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "4ad7c08beb30: Preparing\n",
      "ae4c64fa0a1e: Preparing\n",
      "0235cf47cbae: Preparing\n",
      "2971cdbb4b45: Preparing\n",
      "8374b2bc65e7: Preparing\n",
      "3b93a6feba89: Preparing\n",
      "b15400eb0fa7: Preparing\n",
      "29ecaf0c2ae0: Preparing\n",
      "41e673079fce: Preparing\n",
      "cda9215846ee: Preparing\n",
      "c5eafb4bee8f: Preparing\n",
      "81182eb0608d: Preparing\n",
      "f2baf76d88ee: Preparing\n",
      "cdd7c7392317: Preparing\n",
      "a9144587e03b: Waiting\n",
      "e27154b6913d: Waiting\n",
      "d56803d8f0fd: Waiting\n",
      "246e7cfb894b: Waiting\n",
      "d91491ee4880: Waiting\n",
      "436e99a10105: Waiting\n",
      "5542e763119e: Waiting\n",
      "da228921f21c: Waiting\n",
      "bd3521ef34cd: Waiting\n",
      "b3a75710e85e: Waiting\n",
      "775cb856bba4: Waiting\n",
      "78bb63e63ed0: Waiting\n",
      "38f846d7128b: Waiting\n",
      "6189eef4f001: Waiting\n",
      "eccfcf8683dc: Waiting\n",
      "b8a4020b158f: Waiting\n",
      "bc0d9c74ea9a: Waiting\n",
      "3259c8832a5d: Waiting\n",
      "4fc4812ae49f: Waiting\n",
      "df7e6d0d15e2: Waiting\n",
      "724d9e72f714: Waiting\n",
      "84eef6acfb89: Waiting\n",
      "fcaf6b281212: Waiting\n",
      "90b29c238a16: Waiting\n",
      "8fb66524d219: Waiting\n",
      "fe74336bfcaa: Waiting\n",
      "886cbf0cc5a6: Waiting\n",
      "a9f4b6b1b66a: Waiting\n",
      "9ab87db027f2: Waiting\n",
      "e8cddc117f9b: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "4ad7c08beb30: Waiting\n",
      "ae4c64fa0a1e: Waiting\n",
      "0235cf47cbae: Waiting\n",
      "2971cdbb4b45: Waiting\n",
      "8374b2bc65e7: Waiting\n",
      "3b93a6feba89: Waiting\n",
      "b15400eb0fa7: Waiting\n",
      "29ecaf0c2ae0: Waiting\n",
      "41e673079fce: Waiting\n",
      "cda9215846ee: Waiting\n",
      "c5eafb4bee8f: Waiting\n",
      "81182eb0608d: Waiting\n",
      "f2baf76d88ee: Waiting\n",
      "cdd7c7392317: Waiting\n",
      "c82497e9151f: Layer already exists\n",
      "944f7ad7d5be: Layer already exists\n",
      "b3a75710e85e: Layer already exists\n",
      "775cb856bba4: Layer already exists\n",
      "78bb63e63ed0: Layer already exists\n",
      "38f846d7128b: Layer already exists\n",
      "6189eef4f001: Layer already exists\n",
      "eccfcf8683dc: Layer already exists\n",
      "b8a4020b158f: Layer already exists\n",
      "bc0d9c74ea9a: Layer already exists\n",
      "3259c8832a5d: Layer already exists\n",
      "4fc4812ae49f: Layer already exists\n",
      "df7e6d0d15e2: Layer already exists\n",
      "724d9e72f714: Layer already exists\n",
      "84eef6acfb89: Layer already exists\n",
      "fcaf6b281212: Layer already exists\n",
      "90b29c238a16: Layer already exists\n",
      "8fb66524d219: Layer already exists\n",
      "fe74336bfcaa: Layer already exists\n",
      "886cbf0cc5a6: Layer already exists\n",
      "a9f4b6b1b66a: Layer already exists\n",
      "9ab87db027f2: Layer already exists\n",
      "a9144587e03b: Layer already exists\n",
      "e27154b6913d: Layer already exists\n",
      "d56803d8f0fd: Layer already exists\n",
      "d91491ee4880: Layer already exists\n",
      "246e7cfb894b: Layer already exists\n",
      "436e99a10105: Layer already exists\n",
      "5542e763119e: Layer already exists\n",
      "bd3521ef34cd: Layer already exists\n",
      "da228921f21c: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "e8cddc117f9b: Layer already exists\n",
      "ae4c64fa0a1e: Layer already exists\n",
      "4ad7c08beb30: Layer already exists\n",
      "2971cdbb4b45: Layer already exists\n",
      "8374b2bc65e7: Layer already exists\n",
      "0235cf47cbae: Layer already exists\n",
      "3b93a6feba89: Layer already exists\n",
      "0b0bfd8b376d: Pushed\n",
      "b15400eb0fa7: Layer already exists\n",
      "41e673079fce: Layer already exists\n",
      "29ecaf0c2ae0: Layer already exists\n",
      "cda9215846ee: Layer already exists\n",
      "c5eafb4bee8f: Layer already exists\n",
      "81182eb0608d: Layer already exists\n",
      "f2baf76d88ee: Layer already exists\n",
      "35a98904d316: Pushed\n",
      "cdd7c7392317: Layer already exists\n",
      "31a8413f5eb8: Pushed\n",
      "latest: digest: sha256:3039e751639e5c225015b7225dc9c7624a1f6d51ff731e84c14ce287625f8181 size: 10823\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                            IMAGES                                    STATUS\n",
      "8ff3f9f2-3eae-4a7c-8258-c8099588d430  2024-02-01T11:26:44+00:00  12M21S    gs://stellar-orb-408015_cloudbuild/source/1706786804.361925-54d3a9151f3e4645bfa1c49c1781a36b.tgz  gcr.io/stellar-orb-408015/cicd (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $CICD_IMAGE_URI build/. --timeout=3h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc9b2af",
   "metadata": {},
   "source": [
    "### Run CI/CD from pipeline deployment using Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69b9b784-d2cd-4a8b-b090-de80160b54eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_REPO_URL=https://github.com/Saoussen-CH/mlops-with-vertex-ai-steps.git,_BRANCH=main,_CICD_IMAGE_URI=gcr.io/stellar-orb-408015/cicd:latest,_PROJECT=stellar-orb-408015,_REGION=us-central1,_GCS_LOCATION=gs://stellar-orb-408015/chicago-taxi-tips/,_TEST_GCS_LOCATION=gs://stellar-orb-408015/chicago-taxi-tips/e2e_tests,_BQ_LOCATION=US,_BQ_DATASET_NAME=playground_us,_BQ_TABLE_NAME=chicago_taxitrips_final,_DATASET_DISPLAY_NAME=chicago-taxi-tips,_MODEL_DISPLAY_NAME=chicago-taxi-tips-classifier-v01,_CI_TRAIN_LIMIT=10,_CI_TEST_LIMIT=2,_CI_ACCURACY_THRESHOLD=0.1,_BEAM_RUNNER=DataflowRunner,_TRAINING_RUNNER=vertex,_TFX_IMAGE_URI=gcr.io/stellar-orb-408015/chicago-taxi-tips:04,_PIPELINE_NAME=chicago-taxi-tips-classifier-v01-train-pipeline,_ENDPOINT_NAME_PROD=predict-explain-on-chicago-taxi-tips-classifier-prod,\n"
     ]
    }
   ],
   "source": [
    "REPO_URL = \"https://github.com/Saoussen-CH/mlops-with-vertex-ai-steps.git\" # Change to your github repo.\n",
    "BRANCH = \"main\"\n",
    "GCS_LOCATION = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/\"\n",
    "TEST_GCS_LOCATION = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/e2e_tests\"\n",
    "CI_TRAIN_LIMIT = 10\n",
    "CI_TEST_LIMIT = 2\n",
    "CI_ACCURACY_THRESHOLD = 0.1\n",
    "BEAM_RUNNER = \"DataflowRunner\"\n",
    "TRAINING_RUNNER = \"vertex\"\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-train-pipeline'\n",
    "MODEL_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}-classifier-{VERSION}'\n",
    "ENDPOINT_NAME_TEST = f'predict-explain-on-{DATASET_DISPLAY_NAME}-classifier_local-test'\n",
    "ENDPOINT_NAME_PROD = f'predict-explain-on-{DATASET_DISPLAY_NAME}-classifier-prod'\n",
    "BQ_LOCATION = 'US'\n",
    "BQ_DATASET_NAME = 'playground_us' # Change to your BQ dataset name.\n",
    "BQ_TABLE_NAME = 'chicago_taxitrips_final'\n",
    "VERSION = 'v01'\n",
    "DATASET_DISPLAY_NAME = 'chicago-taxi-tips'\n",
    "MODEL_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}-classifier-{VERSION}'\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-train-pipeline'\n",
    "CICD_IMAGE_NAME = 'cicd:latest'\n",
    "CICD_IMAGE_URI = f\"gcr.io/{PROJECT}/{CICD_IMAGE_NAME}\"\n",
    "TFX_IMAGE_URI = f\"gcr.io/{PROJECT}/{DATASET_DISPLAY_NAME}:04\"\n",
    "\n",
    "SUBSTITUTIONS=f\"\"\"\\\n",
    "_REPO_URL='{REPO_URL}',\\\n",
    "_BRANCH={BRANCH},\\\n",
    "_CICD_IMAGE_URI={CICD_IMAGE_URI},\\\n",
    "_PROJECT={PROJECT},\\\n",
    "_REGION={REGION},\\\n",
    "_GCS_LOCATION={GCS_LOCATION},\\\n",
    "_TEST_GCS_LOCATION={TEST_GCS_LOCATION},\\\n",
    "_BQ_LOCATION={BQ_LOCATION},\\\n",
    "_BQ_DATASET_NAME={BQ_DATASET_NAME},\\\n",
    "_BQ_TABLE_NAME={BQ_TABLE_NAME},\\\n",
    "_DATASET_DISPLAY_NAME={DATASET_DISPLAY_NAME},\\\n",
    "_MODEL_DISPLAY_NAME={MODEL_DISPLAY_NAME},\\\n",
    "_CI_TRAIN_LIMIT={CI_TRAIN_LIMIT},\\\n",
    "_CI_TEST_LIMIT={CI_TEST_LIMIT},\\\n",
    "_CI_ACCURACY_THRESHOLD={CI_ACCURACY_THRESHOLD},\\\n",
    "_BEAM_RUNNER={BEAM_RUNNER},\\\n",
    "_TRAINING_RUNNER={TRAINING_RUNNER},\\\n",
    "_TFX_IMAGE_URI={TFX_IMAGE_URI},\\\n",
    "_PIPELINE_NAME={PIPELINE_NAME},\\\n",
    "_ENDPOINT_NAME_PROD={ENDPOINT_NAME_PROD}\\,\n",
    "_ENDPOINT_NAME_TEST={ENDPOINT_NAME_TEST}\\\n",
    "\"\"\"\n",
    "\n",
    "!echo $SUBSTITUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bc589ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.builds.submit) parsing build/pipeline-deployment.yaml: while parsing a block mapping\n",
      "  in \"build/pipeline-deployment.yaml\", line 19, column 1\n",
      "expected <block end>, but found '-'\n",
      "  in \"build/pipeline-deployment.yaml\", line 109, column 1\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --no-source --timeout=7h --config build/pipeline-deployment.yaml --substitutions {SUBSTITUTIONS} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16028a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930d06d-5bce-42ce-8df6-28d8e1d1367c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
